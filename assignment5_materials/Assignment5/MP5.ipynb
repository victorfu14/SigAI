{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lifes(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 2.0   memory length: 220   epsilon: 1.0    steps: 220     evaluation reward: 2.0\n",
      "episode: 1   score: 1.0   memory length: 396   epsilon: 1.0    steps: 176     evaluation reward: 1.5\n",
      "episode: 2   score: 0.0   memory length: 528   epsilon: 1.0    steps: 132     evaluation reward: 1.0\n",
      "episode: 3   score: 2.0   memory length: 739   epsilon: 1.0    steps: 211     evaluation reward: 1.25\n",
      "episode: 4   score: 0.0   memory length: 875   epsilon: 1.0    steps: 136     evaluation reward: 1.0\n",
      "episode: 5   score: 2.0   memory length: 1067   epsilon: 1.0    steps: 192     evaluation reward: 1.1666666666666667\n",
      "episode: 6   score: 1.0   memory length: 1224   epsilon: 1.0    steps: 157     evaluation reward: 1.1428571428571428\n",
      "episode: 7   score: 2.0   memory length: 1409   epsilon: 1.0    steps: 185     evaluation reward: 1.25\n",
      "episode: 8   score: 0.0   memory length: 1534   epsilon: 1.0    steps: 125     evaluation reward: 1.1111111111111112\n",
      "episode: 9   score: 0.0   memory length: 1675   epsilon: 1.0    steps: 141     evaluation reward: 1.0\n",
      "episode: 10   score: 2.0   memory length: 1884   epsilon: 1.0    steps: 209     evaluation reward: 1.0909090909090908\n",
      "episode: 11   score: 2.0   memory length: 2107   epsilon: 1.0    steps: 223     evaluation reward: 1.1666666666666667\n",
      "episode: 12   score: 0.0   memory length: 2234   epsilon: 1.0    steps: 127     evaluation reward: 1.0769230769230769\n",
      "episode: 13   score: 0.0   memory length: 2367   epsilon: 1.0    steps: 133     evaluation reward: 1.0\n",
      "episode: 14   score: 1.0   memory length: 2560   epsilon: 1.0    steps: 193     evaluation reward: 1.0\n",
      "episode: 15   score: 2.0   memory length: 2761   epsilon: 1.0    steps: 201     evaluation reward: 1.0625\n",
      "episode: 16   score: 1.0   memory length: 2925   epsilon: 1.0    steps: 164     evaluation reward: 1.0588235294117647\n",
      "episode: 17   score: 0.0   memory length: 3051   epsilon: 1.0    steps: 126     evaluation reward: 1.0\n",
      "episode: 18   score: 1.0   memory length: 3225   epsilon: 1.0    steps: 174     evaluation reward: 1.0\n",
      "episode: 19   score: 4.0   memory length: 3531   epsilon: 1.0    steps: 306     evaluation reward: 1.15\n",
      "episode: 20   score: 4.0   memory length: 3774   epsilon: 1.0    steps: 243     evaluation reward: 1.2857142857142858\n",
      "episode: 21   score: 0.0   memory length: 3932   epsilon: 1.0    steps: 158     evaluation reward: 1.2272727272727273\n",
      "episode: 22   score: 1.0   memory length: 4105   epsilon: 1.0    steps: 173     evaluation reward: 1.2173913043478262\n",
      "episode: 23   score: 3.0   memory length: 4364   epsilon: 1.0    steps: 259     evaluation reward: 1.2916666666666667\n",
      "episode: 24   score: 1.0   memory length: 4551   epsilon: 1.0    steps: 187     evaluation reward: 1.28\n",
      "episode: 25   score: 1.0   memory length: 4703   epsilon: 1.0    steps: 152     evaluation reward: 1.2692307692307692\n",
      "episode: 26   score: 2.0   memory length: 4909   epsilon: 1.0    steps: 206     evaluation reward: 1.2962962962962963\n",
      "episode: 27   score: 0.0   memory length: 5037   epsilon: 1.0    steps: 128     evaluation reward: 1.25\n",
      "episode: 28   score: 2.0   memory length: 5230   epsilon: 1.0    steps: 193     evaluation reward: 1.2758620689655173\n",
      "episode: 29   score: 2.0   memory length: 5422   epsilon: 1.0    steps: 192     evaluation reward: 1.3\n",
      "episode: 30   score: 1.0   memory length: 5590   epsilon: 1.0    steps: 168     evaluation reward: 1.2903225806451613\n",
      "episode: 31   score: 4.0   memory length: 5839   epsilon: 1.0    steps: 249     evaluation reward: 1.375\n",
      "episode: 32   score: 1.0   memory length: 5998   epsilon: 1.0    steps: 159     evaluation reward: 1.3636363636363635\n",
      "episode: 33   score: 8.0   memory length: 6439   epsilon: 1.0    steps: 441     evaluation reward: 1.5588235294117647\n",
      "episode: 34   score: 0.0   memory length: 6566   epsilon: 1.0    steps: 127     evaluation reward: 1.5142857142857142\n",
      "episode: 35   score: 2.0   memory length: 6770   epsilon: 1.0    steps: 204     evaluation reward: 1.5277777777777777\n",
      "episode: 36   score: 0.0   memory length: 6899   epsilon: 1.0    steps: 129     evaluation reward: 1.4864864864864864\n",
      "episode: 37   score: 3.0   memory length: 7152   epsilon: 1.0    steps: 253     evaluation reward: 1.5263157894736843\n",
      "episode: 38   score: 2.0   memory length: 7348   epsilon: 1.0    steps: 196     evaluation reward: 1.5384615384615385\n",
      "episode: 39   score: 2.0   memory length: 7545   epsilon: 1.0    steps: 197     evaluation reward: 1.55\n",
      "episode: 40   score: 1.0   memory length: 7709   epsilon: 1.0    steps: 164     evaluation reward: 1.5365853658536586\n",
      "episode: 41   score: 2.0   memory length: 7933   epsilon: 1.0    steps: 224     evaluation reward: 1.5476190476190477\n",
      "episode: 42   score: 0.0   memory length: 8071   epsilon: 1.0    steps: 138     evaluation reward: 1.5116279069767442\n",
      "episode: 43   score: 0.0   memory length: 8199   epsilon: 1.0    steps: 128     evaluation reward: 1.4772727272727273\n",
      "episode: 44   score: 1.0   memory length: 8384   epsilon: 1.0    steps: 185     evaluation reward: 1.4666666666666666\n",
      "episode: 45   score: 0.0   memory length: 8518   epsilon: 1.0    steps: 134     evaluation reward: 1.434782608695652\n",
      "episode: 46   score: 2.0   memory length: 8710   epsilon: 1.0    steps: 192     evaluation reward: 1.446808510638298\n",
      "episode: 47   score: 0.0   memory length: 8836   epsilon: 1.0    steps: 126     evaluation reward: 1.4166666666666667\n",
      "episode: 48   score: 0.0   memory length: 8968   epsilon: 1.0    steps: 132     evaluation reward: 1.3877551020408163\n",
      "episode: 49   score: 2.0   memory length: 9175   epsilon: 1.0    steps: 207     evaluation reward: 1.4\n",
      "episode: 50   score: 5.0   memory length: 9487   epsilon: 1.0    steps: 312     evaluation reward: 1.4705882352941178\n",
      "episode: 51   score: 0.0   memory length: 9613   epsilon: 1.0    steps: 126     evaluation reward: 1.4423076923076923\n",
      "episode: 52   score: 2.0   memory length: 9844   epsilon: 1.0    steps: 231     evaluation reward: 1.4528301886792452\n",
      "episode: 53   score: 2.0   memory length: 10083   epsilon: 1.0    steps: 239     evaluation reward: 1.462962962962963\n",
      "episode: 54   score: 0.0   memory length: 10211   epsilon: 1.0    steps: 128     evaluation reward: 1.4363636363636363\n",
      "episode: 55   score: 1.0   memory length: 10393   epsilon: 1.0    steps: 182     evaluation reward: 1.4285714285714286\n",
      "episode: 56   score: 0.0   memory length: 10522   epsilon: 1.0    steps: 129     evaluation reward: 1.4035087719298245\n",
      "episode: 57   score: 1.0   memory length: 10682   epsilon: 1.0    steps: 160     evaluation reward: 1.396551724137931\n",
      "episode: 58   score: 1.0   memory length: 10851   epsilon: 1.0    steps: 169     evaluation reward: 1.3898305084745763\n",
      "episode: 59   score: 1.0   memory length: 11025   epsilon: 1.0    steps: 174     evaluation reward: 1.3833333333333333\n",
      "episode: 60   score: 0.0   memory length: 11162   epsilon: 1.0    steps: 137     evaluation reward: 1.360655737704918\n",
      "episode: 61   score: 3.0   memory length: 11402   epsilon: 1.0    steps: 240     evaluation reward: 1.3870967741935485\n",
      "episode: 62   score: 2.0   memory length: 11612   epsilon: 1.0    steps: 210     evaluation reward: 1.3968253968253967\n",
      "episode: 63   score: 1.0   memory length: 11790   epsilon: 1.0    steps: 178     evaluation reward: 1.390625\n",
      "episode: 64   score: 2.0   memory length: 11987   epsilon: 1.0    steps: 197     evaluation reward: 1.4\n",
      "episode: 65   score: 0.0   memory length: 12115   epsilon: 1.0    steps: 128     evaluation reward: 1.378787878787879\n",
      "episode: 66   score: 1.0   memory length: 12298   epsilon: 1.0    steps: 183     evaluation reward: 1.373134328358209\n",
      "episode: 67   score: 1.0   memory length: 12469   epsilon: 1.0    steps: 171     evaluation reward: 1.3676470588235294\n",
      "episode: 68   score: 3.0   memory length: 12741   epsilon: 1.0    steps: 272     evaluation reward: 1.391304347826087\n",
      "episode: 69   score: 2.0   memory length: 12941   epsilon: 1.0    steps: 200     evaluation reward: 1.4\n",
      "episode: 70   score: 3.0   memory length: 13191   epsilon: 1.0    steps: 250     evaluation reward: 1.4225352112676057\n",
      "episode: 71   score: 2.0   memory length: 13395   epsilon: 1.0    steps: 204     evaluation reward: 1.4305555555555556\n",
      "episode: 72   score: 1.0   memory length: 13570   epsilon: 1.0    steps: 175     evaluation reward: 1.4246575342465753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 73   score: 0.0   memory length: 13703   epsilon: 1.0    steps: 133     evaluation reward: 1.4054054054054055\n",
      "episode: 74   score: 1.0   memory length: 13877   epsilon: 1.0    steps: 174     evaluation reward: 1.4\n",
      "episode: 75   score: 0.0   memory length: 14008   epsilon: 1.0    steps: 131     evaluation reward: 1.381578947368421\n",
      "episode: 76   score: 0.0   memory length: 14133   epsilon: 1.0    steps: 125     evaluation reward: 1.3636363636363635\n",
      "episode: 77   score: 0.0   memory length: 14262   epsilon: 1.0    steps: 129     evaluation reward: 1.3461538461538463\n",
      "episode: 78   score: 0.0   memory length: 14392   epsilon: 1.0    steps: 130     evaluation reward: 1.3291139240506329\n",
      "episode: 79   score: 2.0   memory length: 14596   epsilon: 1.0    steps: 204     evaluation reward: 1.3375\n",
      "episode: 80   score: 0.0   memory length: 14732   epsilon: 1.0    steps: 136     evaluation reward: 1.3209876543209877\n",
      "episode: 81   score: 1.0   memory length: 14895   epsilon: 1.0    steps: 163     evaluation reward: 1.3170731707317074\n",
      "episode: 82   score: 2.0   memory length: 15114   epsilon: 1.0    steps: 219     evaluation reward: 1.3253012048192772\n",
      "episode: 83   score: 4.0   memory length: 15446   epsilon: 1.0    steps: 332     evaluation reward: 1.3571428571428572\n",
      "episode: 84   score: 3.0   memory length: 15706   epsilon: 1.0    steps: 260     evaluation reward: 1.3764705882352941\n",
      "episode: 85   score: 0.0   memory length: 15850   epsilon: 1.0    steps: 144     evaluation reward: 1.3604651162790697\n",
      "episode: 86   score: 0.0   memory length: 15989   epsilon: 1.0    steps: 139     evaluation reward: 1.3448275862068966\n",
      "episode: 87   score: 4.0   memory length: 16267   epsilon: 1.0    steps: 278     evaluation reward: 1.375\n",
      "episode: 88   score: 0.0   memory length: 16397   epsilon: 1.0    steps: 130     evaluation reward: 1.3595505617977528\n",
      "episode: 89   score: 0.0   memory length: 16522   epsilon: 1.0    steps: 125     evaluation reward: 1.3444444444444446\n",
      "episode: 90   score: 0.0   memory length: 16646   epsilon: 1.0    steps: 124     evaluation reward: 1.3296703296703296\n",
      "episode: 91   score: 2.0   memory length: 16840   epsilon: 1.0    steps: 194     evaluation reward: 1.3369565217391304\n",
      "episode: 92   score: 2.0   memory length: 17043   epsilon: 1.0    steps: 203     evaluation reward: 1.3440860215053763\n",
      "episode: 93   score: 3.0   memory length: 17268   epsilon: 1.0    steps: 225     evaluation reward: 1.3617021276595744\n",
      "episode: 94   score: 1.0   memory length: 17426   epsilon: 1.0    steps: 158     evaluation reward: 1.3578947368421053\n",
      "episode: 95   score: 0.0   memory length: 17558   epsilon: 1.0    steps: 132     evaluation reward: 1.34375\n",
      "episode: 96   score: 1.0   memory length: 17717   epsilon: 1.0    steps: 159     evaluation reward: 1.3402061855670102\n",
      "episode: 97   score: 1.0   memory length: 17895   epsilon: 1.0    steps: 178     evaluation reward: 1.336734693877551\n",
      "episode: 98   score: 1.0   memory length: 18057   epsilon: 1.0    steps: 162     evaluation reward: 1.3333333333333333\n",
      "episode: 99   score: 2.0   memory length: 18303   epsilon: 1.0    steps: 246     evaluation reward: 1.34\n",
      "episode: 100   score: 2.0   memory length: 18506   epsilon: 1.0    steps: 203     evaluation reward: 1.34\n",
      "episode: 101   score: 3.0   memory length: 18746   epsilon: 1.0    steps: 240     evaluation reward: 1.36\n",
      "episode: 102   score: 1.0   memory length: 18906   epsilon: 1.0    steps: 160     evaluation reward: 1.37\n",
      "episode: 103   score: 1.0   memory length: 19076   epsilon: 1.0    steps: 170     evaluation reward: 1.36\n",
      "episode: 104   score: 1.0   memory length: 19254   epsilon: 1.0    steps: 178     evaluation reward: 1.37\n",
      "episode: 105   score: 2.0   memory length: 19442   epsilon: 1.0    steps: 188     evaluation reward: 1.37\n",
      "episode: 106   score: 3.0   memory length: 19701   epsilon: 1.0    steps: 259     evaluation reward: 1.39\n",
      "episode: 107   score: 2.0   memory length: 19930   epsilon: 1.0    steps: 229     evaluation reward: 1.39\n",
      "episode: 108   score: 0.0   memory length: 20062   epsilon: 1.0    steps: 132     evaluation reward: 1.39\n",
      "episode: 109   score: 3.0   memory length: 20312   epsilon: 1.0    steps: 250     evaluation reward: 1.42\n",
      "episode: 110   score: 1.0   memory length: 20486   epsilon: 1.0    steps: 174     evaluation reward: 1.41\n",
      "episode: 111   score: 1.0   memory length: 20669   epsilon: 1.0    steps: 183     evaluation reward: 1.4\n",
      "episode: 112   score: 0.0   memory length: 20795   epsilon: 1.0    steps: 126     evaluation reward: 1.4\n",
      "episode: 113   score: 1.0   memory length: 20959   epsilon: 1.0    steps: 164     evaluation reward: 1.41\n",
      "episode: 114   score: 0.0   memory length: 21092   epsilon: 1.0    steps: 133     evaluation reward: 1.4\n",
      "episode: 115   score: 1.0   memory length: 21268   epsilon: 1.0    steps: 176     evaluation reward: 1.39\n",
      "episode: 116   score: 2.0   memory length: 21453   epsilon: 1.0    steps: 185     evaluation reward: 1.4\n",
      "episode: 117   score: 1.0   memory length: 21641   epsilon: 1.0    steps: 188     evaluation reward: 1.41\n",
      "episode: 118   score: 1.0   memory length: 21815   epsilon: 1.0    steps: 174     evaluation reward: 1.41\n",
      "episode: 119   score: 0.0   memory length: 21954   epsilon: 1.0    steps: 139     evaluation reward: 1.37\n",
      "episode: 120   score: 3.0   memory length: 22231   epsilon: 1.0    steps: 277     evaluation reward: 1.36\n",
      "episode: 121   score: 2.0   memory length: 22441   epsilon: 1.0    steps: 210     evaluation reward: 1.38\n",
      "episode: 122   score: 0.0   memory length: 22564   epsilon: 1.0    steps: 123     evaluation reward: 1.37\n",
      "episode: 123   score: 0.0   memory length: 22705   epsilon: 1.0    steps: 141     evaluation reward: 1.34\n",
      "episode: 124   score: 0.0   memory length: 22836   epsilon: 1.0    steps: 131     evaluation reward: 1.33\n",
      "episode: 125   score: 4.0   memory length: 23118   epsilon: 1.0    steps: 282     evaluation reward: 1.36\n",
      "episode: 126   score: 1.0   memory length: 23274   epsilon: 1.0    steps: 156     evaluation reward: 1.35\n",
      "episode: 127   score: 0.0   memory length: 23400   epsilon: 1.0    steps: 126     evaluation reward: 1.35\n",
      "episode: 128   score: 2.0   memory length: 23600   epsilon: 1.0    steps: 200     evaluation reward: 1.35\n",
      "episode: 129   score: 2.0   memory length: 23801   epsilon: 1.0    steps: 201     evaluation reward: 1.35\n",
      "episode: 130   score: 1.0   memory length: 23963   epsilon: 1.0    steps: 162     evaluation reward: 1.35\n",
      "episode: 131   score: 0.0   memory length: 24086   epsilon: 1.0    steps: 123     evaluation reward: 1.31\n",
      "episode: 132   score: 0.0   memory length: 24221   epsilon: 1.0    steps: 135     evaluation reward: 1.3\n",
      "episode: 133   score: 2.0   memory length: 24427   epsilon: 1.0    steps: 206     evaluation reward: 1.24\n",
      "episode: 134   score: 1.0   memory length: 24599   epsilon: 1.0    steps: 172     evaluation reward: 1.25\n",
      "episode: 135   score: 3.0   memory length: 24842   epsilon: 1.0    steps: 243     evaluation reward: 1.26\n",
      "episode: 136   score: 2.0   memory length: 25069   epsilon: 1.0    steps: 227     evaluation reward: 1.28\n",
      "episode: 137   score: 0.0   memory length: 25197   epsilon: 1.0    steps: 128     evaluation reward: 1.25\n",
      "episode: 138   score: 0.0   memory length: 25328   epsilon: 1.0    steps: 131     evaluation reward: 1.23\n",
      "episode: 139   score: 3.0   memory length: 25607   epsilon: 1.0    steps: 279     evaluation reward: 1.24\n",
      "episode: 140   score: 1.0   memory length: 25780   epsilon: 1.0    steps: 173     evaluation reward: 1.24\n",
      "episode: 141   score: 2.0   memory length: 26016   epsilon: 1.0    steps: 236     evaluation reward: 1.24\n",
      "episode: 142   score: 0.0   memory length: 26153   epsilon: 1.0    steps: 137     evaluation reward: 1.24\n",
      "episode: 143   score: 4.0   memory length: 26431   epsilon: 1.0    steps: 278     evaluation reward: 1.28\n",
      "episode: 144   score: 0.0   memory length: 26556   epsilon: 1.0    steps: 125     evaluation reward: 1.27\n",
      "episode: 145   score: 5.0   memory length: 26907   epsilon: 1.0    steps: 351     evaluation reward: 1.32\n",
      "episode: 146   score: 1.0   memory length: 27064   epsilon: 1.0    steps: 157     evaluation reward: 1.31\n",
      "episode: 147   score: 4.0   memory length: 27344   epsilon: 1.0    steps: 280     evaluation reward: 1.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 148   score: 0.0   memory length: 27472   epsilon: 1.0    steps: 128     evaluation reward: 1.35\n",
      "episode: 149   score: 0.0   memory length: 27612   epsilon: 1.0    steps: 140     evaluation reward: 1.33\n",
      "episode: 150   score: 3.0   memory length: 27850   epsilon: 1.0    steps: 238     evaluation reward: 1.31\n",
      "episode: 151   score: 0.0   memory length: 27977   epsilon: 1.0    steps: 127     evaluation reward: 1.31\n",
      "episode: 152   score: 1.0   memory length: 28136   epsilon: 1.0    steps: 159     evaluation reward: 1.3\n",
      "episode: 153   score: 0.0   memory length: 28266   epsilon: 1.0    steps: 130     evaluation reward: 1.28\n",
      "episode: 154   score: 0.0   memory length: 28395   epsilon: 1.0    steps: 129     evaluation reward: 1.28\n",
      "episode: 155   score: 2.0   memory length: 28587   epsilon: 1.0    steps: 192     evaluation reward: 1.29\n",
      "episode: 156   score: 2.0   memory length: 28805   epsilon: 1.0    steps: 218     evaluation reward: 1.31\n",
      "episode: 157   score: 2.0   memory length: 28993   epsilon: 1.0    steps: 188     evaluation reward: 1.32\n",
      "episode: 158   score: 4.0   memory length: 29312   epsilon: 1.0    steps: 319     evaluation reward: 1.35\n",
      "episode: 159   score: 2.0   memory length: 29513   epsilon: 1.0    steps: 201     evaluation reward: 1.36\n",
      "episode: 160   score: 0.0   memory length: 29643   epsilon: 1.0    steps: 130     evaluation reward: 1.36\n",
      "episode: 161   score: 1.0   memory length: 29824   epsilon: 1.0    steps: 181     evaluation reward: 1.34\n",
      "episode: 162   score: 1.0   memory length: 30017   epsilon: 1.0    steps: 193     evaluation reward: 1.33\n",
      "episode: 163   score: 1.0   memory length: 30192   epsilon: 1.0    steps: 175     evaluation reward: 1.33\n",
      "episode: 164   score: 2.0   memory length: 30399   epsilon: 1.0    steps: 207     evaluation reward: 1.33\n",
      "episode: 165   score: 0.0   memory length: 30528   epsilon: 1.0    steps: 129     evaluation reward: 1.33\n",
      "episode: 166   score: 1.0   memory length: 30708   epsilon: 1.0    steps: 180     evaluation reward: 1.33\n",
      "episode: 167   score: 1.0   memory length: 30886   epsilon: 1.0    steps: 178     evaluation reward: 1.33\n",
      "episode: 168   score: 0.0   memory length: 31010   epsilon: 1.0    steps: 124     evaluation reward: 1.3\n",
      "episode: 169   score: 0.0   memory length: 31148   epsilon: 1.0    steps: 138     evaluation reward: 1.28\n",
      "episode: 170   score: 0.0   memory length: 31276   epsilon: 1.0    steps: 128     evaluation reward: 1.25\n",
      "episode: 171   score: 2.0   memory length: 31467   epsilon: 1.0    steps: 191     evaluation reward: 1.25\n",
      "episode: 172   score: 0.0   memory length: 31593   epsilon: 1.0    steps: 126     evaluation reward: 1.24\n",
      "episode: 173   score: 0.0   memory length: 31722   epsilon: 1.0    steps: 129     evaluation reward: 1.24\n",
      "episode: 174   score: 2.0   memory length: 31947   epsilon: 1.0    steps: 225     evaluation reward: 1.25\n",
      "episode: 175   score: 2.0   memory length: 32150   epsilon: 1.0    steps: 203     evaluation reward: 1.27\n",
      "episode: 176   score: 0.0   memory length: 32276   epsilon: 1.0    steps: 126     evaluation reward: 1.27\n",
      "episode: 177   score: 2.0   memory length: 32493   epsilon: 1.0    steps: 217     evaluation reward: 1.29\n",
      "episode: 178   score: 0.0   memory length: 32632   epsilon: 1.0    steps: 139     evaluation reward: 1.29\n",
      "episode: 179   score: 1.0   memory length: 32817   epsilon: 1.0    steps: 185     evaluation reward: 1.28\n",
      "episode: 180   score: 3.0   memory length: 33077   epsilon: 1.0    steps: 260     evaluation reward: 1.31\n",
      "episode: 181   score: 1.0   memory length: 33253   epsilon: 1.0    steps: 176     evaluation reward: 1.31\n",
      "episode: 182   score: 1.0   memory length: 33405   epsilon: 1.0    steps: 152     evaluation reward: 1.3\n",
      "episode: 183   score: 0.0   memory length: 33539   epsilon: 1.0    steps: 134     evaluation reward: 1.26\n",
      "episode: 184   score: 0.0   memory length: 33666   epsilon: 1.0    steps: 127     evaluation reward: 1.23\n",
      "episode: 185   score: 1.0   memory length: 33844   epsilon: 1.0    steps: 178     evaluation reward: 1.24\n",
      "episode: 186   score: 1.0   memory length: 34022   epsilon: 1.0    steps: 178     evaluation reward: 1.25\n",
      "episode: 187   score: 2.0   memory length: 34217   epsilon: 1.0    steps: 195     evaluation reward: 1.23\n",
      "episode: 188   score: 0.0   memory length: 34349   epsilon: 1.0    steps: 132     evaluation reward: 1.23\n",
      "episode: 189   score: 1.0   memory length: 34531   epsilon: 1.0    steps: 182     evaluation reward: 1.24\n",
      "episode: 190   score: 0.0   memory length: 34663   epsilon: 1.0    steps: 132     evaluation reward: 1.24\n",
      "episode: 191   score: 1.0   memory length: 34834   epsilon: 1.0    steps: 171     evaluation reward: 1.23\n",
      "episode: 192   score: 0.0   memory length: 34958   epsilon: 1.0    steps: 124     evaluation reward: 1.21\n",
      "episode: 193   score: 1.0   memory length: 35110   epsilon: 1.0    steps: 152     evaluation reward: 1.19\n",
      "episode: 194   score: 0.0   memory length: 35237   epsilon: 1.0    steps: 127     evaluation reward: 1.18\n",
      "episode: 195   score: 1.0   memory length: 35410   epsilon: 1.0    steps: 173     evaluation reward: 1.19\n",
      "episode: 196   score: 2.0   memory length: 35613   epsilon: 1.0    steps: 203     evaluation reward: 1.2\n",
      "episode: 197   score: 0.0   memory length: 35742   epsilon: 1.0    steps: 129     evaluation reward: 1.19\n",
      "episode: 198   score: 1.0   memory length: 35917   epsilon: 1.0    steps: 175     evaluation reward: 1.19\n",
      "episode: 199   score: 2.0   memory length: 36127   epsilon: 1.0    steps: 210     evaluation reward: 1.19\n",
      "episode: 200   score: 1.0   memory length: 36306   epsilon: 1.0    steps: 179     evaluation reward: 1.18\n",
      "episode: 201   score: 0.0   memory length: 36438   epsilon: 1.0    steps: 132     evaluation reward: 1.15\n",
      "episode: 202   score: 1.0   memory length: 36614   epsilon: 1.0    steps: 176     evaluation reward: 1.15\n",
      "episode: 203   score: 2.0   memory length: 36839   epsilon: 1.0    steps: 225     evaluation reward: 1.16\n",
      "episode: 204   score: 0.0   memory length: 36969   epsilon: 1.0    steps: 130     evaluation reward: 1.15\n",
      "episode: 205   score: 3.0   memory length: 37215   epsilon: 1.0    steps: 246     evaluation reward: 1.16\n",
      "episode: 206   score: 1.0   memory length: 37388   epsilon: 1.0    steps: 173     evaluation reward: 1.14\n",
      "episode: 207   score: 1.0   memory length: 37545   epsilon: 1.0    steps: 157     evaluation reward: 1.13\n",
      "episode: 208   score: 0.0   memory length: 37675   epsilon: 1.0    steps: 130     evaluation reward: 1.13\n",
      "episode: 209   score: 0.0   memory length: 37806   epsilon: 1.0    steps: 131     evaluation reward: 1.1\n",
      "episode: 210   score: 2.0   memory length: 38019   epsilon: 1.0    steps: 213     evaluation reward: 1.11\n",
      "episode: 211   score: 1.0   memory length: 38174   epsilon: 1.0    steps: 155     evaluation reward: 1.11\n",
      "episode: 212   score: 2.0   memory length: 38361   epsilon: 1.0    steps: 187     evaluation reward: 1.13\n",
      "episode: 213   score: 0.0   memory length: 38488   epsilon: 1.0    steps: 127     evaluation reward: 1.12\n",
      "episode: 214   score: 0.0   memory length: 38624   epsilon: 1.0    steps: 136     evaluation reward: 1.12\n",
      "episode: 215   score: 0.0   memory length: 38756   epsilon: 1.0    steps: 132     evaluation reward: 1.11\n",
      "episode: 216   score: 2.0   memory length: 38960   epsilon: 1.0    steps: 204     evaluation reward: 1.11\n",
      "episode: 217   score: 1.0   memory length: 39117   epsilon: 1.0    steps: 157     evaluation reward: 1.11\n",
      "episode: 218   score: 1.0   memory length: 39274   epsilon: 1.0    steps: 157     evaluation reward: 1.11\n",
      "episode: 219   score: 0.0   memory length: 39406   epsilon: 1.0    steps: 132     evaluation reward: 1.11\n",
      "episode: 220   score: 0.0   memory length: 39549   epsilon: 1.0    steps: 143     evaluation reward: 1.08\n",
      "episode: 221   score: 0.0   memory length: 39679   epsilon: 1.0    steps: 130     evaluation reward: 1.06\n",
      "episode: 222   score: 1.0   memory length: 39857   epsilon: 1.0    steps: 178     evaluation reward: 1.07\n",
      "episode: 223   score: 0.0   memory length: 39990   epsilon: 1.0    steps: 133     evaluation reward: 1.07\n",
      "episode: 224   score: 0.0   memory length: 40119   epsilon: 1.0    steps: 129     evaluation reward: 1.07\n",
      "episode: 225   score: 1.0   memory length: 40295   epsilon: 1.0    steps: 176     evaluation reward: 1.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 226   score: 1.0   memory length: 40453   epsilon: 1.0    steps: 158     evaluation reward: 1.04\n",
      "episode: 227   score: 2.0   memory length: 40675   epsilon: 1.0    steps: 222     evaluation reward: 1.06\n",
      "episode: 228   score: 2.0   memory length: 40889   epsilon: 1.0    steps: 214     evaluation reward: 1.06\n",
      "episode: 229   score: 1.0   memory length: 41049   epsilon: 1.0    steps: 160     evaluation reward: 1.05\n",
      "episode: 230   score: 2.0   memory length: 41253   epsilon: 1.0    steps: 204     evaluation reward: 1.06\n",
      "episode: 231   score: 2.0   memory length: 41460   epsilon: 1.0    steps: 207     evaluation reward: 1.08\n",
      "episode: 232   score: 1.0   memory length: 41633   epsilon: 1.0    steps: 173     evaluation reward: 1.09\n",
      "episode: 233   score: 2.0   memory length: 41833   epsilon: 1.0    steps: 200     evaluation reward: 1.09\n",
      "episode: 234   score: 0.0   memory length: 41969   epsilon: 1.0    steps: 136     evaluation reward: 1.08\n",
      "episode: 235   score: 1.0   memory length: 42142   epsilon: 1.0    steps: 173     evaluation reward: 1.06\n",
      "episode: 236   score: 0.0   memory length: 42269   epsilon: 1.0    steps: 127     evaluation reward: 1.04\n",
      "episode: 237   score: 0.0   memory length: 42394   epsilon: 1.0    steps: 125     evaluation reward: 1.04\n",
      "episode: 238   score: 0.0   memory length: 42525   epsilon: 1.0    steps: 131     evaluation reward: 1.04\n",
      "episode: 239   score: 3.0   memory length: 42756   epsilon: 1.0    steps: 231     evaluation reward: 1.04\n",
      "episode: 240   score: 0.0   memory length: 42883   epsilon: 1.0    steps: 127     evaluation reward: 1.03\n",
      "episode: 241   score: 3.0   memory length: 43169   epsilon: 1.0    steps: 286     evaluation reward: 1.04\n",
      "episode: 242   score: 3.0   memory length: 43398   epsilon: 1.0    steps: 229     evaluation reward: 1.07\n",
      "episode: 243   score: 0.0   memory length: 43526   epsilon: 1.0    steps: 128     evaluation reward: 1.03\n",
      "episode: 244   score: 3.0   memory length: 43798   epsilon: 1.0    steps: 272     evaluation reward: 1.06\n",
      "episode: 245   score: 2.0   memory length: 44009   epsilon: 1.0    steps: 211     evaluation reward: 1.03\n",
      "episode: 246   score: 1.0   memory length: 44164   epsilon: 1.0    steps: 155     evaluation reward: 1.03\n",
      "episode: 247   score: 2.0   memory length: 44371   epsilon: 1.0    steps: 207     evaluation reward: 1.01\n",
      "episode: 248   score: 3.0   memory length: 44622   epsilon: 1.0    steps: 251     evaluation reward: 1.04\n",
      "episode: 249   score: 2.0   memory length: 44829   epsilon: 1.0    steps: 207     evaluation reward: 1.06\n",
      "episode: 250   score: 0.0   memory length: 44963   epsilon: 1.0    steps: 134     evaluation reward: 1.03\n",
      "episode: 251   score: 1.0   memory length: 45122   epsilon: 1.0    steps: 159     evaluation reward: 1.04\n",
      "episode: 252   score: 0.0   memory length: 45257   epsilon: 1.0    steps: 135     evaluation reward: 1.03\n",
      "episode: 253   score: 2.0   memory length: 45467   epsilon: 1.0    steps: 210     evaluation reward: 1.05\n",
      "episode: 254   score: 0.0   memory length: 45595   epsilon: 1.0    steps: 128     evaluation reward: 1.05\n",
      "episode: 255   score: 1.0   memory length: 45766   epsilon: 1.0    steps: 171     evaluation reward: 1.04\n",
      "episode: 256   score: 1.0   memory length: 45922   epsilon: 1.0    steps: 156     evaluation reward: 1.03\n",
      "episode: 257   score: 3.0   memory length: 46201   epsilon: 1.0    steps: 279     evaluation reward: 1.04\n",
      "episode: 258   score: 0.0   memory length: 46338   epsilon: 1.0    steps: 137     evaluation reward: 1.0\n",
      "episode: 259   score: 4.0   memory length: 46635   epsilon: 1.0    steps: 297     evaluation reward: 1.02\n",
      "episode: 260   score: 3.0   memory length: 46894   epsilon: 1.0    steps: 259     evaluation reward: 1.05\n",
      "episode: 261   score: 3.0   memory length: 47144   epsilon: 1.0    steps: 250     evaluation reward: 1.07\n",
      "episode: 262   score: 0.0   memory length: 47283   epsilon: 1.0    steps: 139     evaluation reward: 1.06\n",
      "episode: 263   score: 0.0   memory length: 47419   epsilon: 1.0    steps: 136     evaluation reward: 1.05\n",
      "episode: 264   score: 2.0   memory length: 47631   epsilon: 1.0    steps: 212     evaluation reward: 1.05\n",
      "episode: 265   score: 2.0   memory length: 47834   epsilon: 1.0    steps: 203     evaluation reward: 1.07\n",
      "episode: 266   score: 0.0   memory length: 47966   epsilon: 1.0    steps: 132     evaluation reward: 1.06\n",
      "episode: 267   score: 3.0   memory length: 48221   epsilon: 1.0    steps: 255     evaluation reward: 1.08\n",
      "episode: 268   score: 0.0   memory length: 48348   epsilon: 1.0    steps: 127     evaluation reward: 1.08\n",
      "episode: 269   score: 1.0   memory length: 48515   epsilon: 1.0    steps: 167     evaluation reward: 1.09\n",
      "episode: 270   score: 2.0   memory length: 48704   epsilon: 1.0    steps: 189     evaluation reward: 1.11\n",
      "episode: 271   score: 0.0   memory length: 48831   epsilon: 1.0    steps: 127     evaluation reward: 1.09\n",
      "episode: 272   score: 0.0   memory length: 48958   epsilon: 1.0    steps: 127     evaluation reward: 1.09\n",
      "episode: 273   score: 0.0   memory length: 49081   epsilon: 1.0    steps: 123     evaluation reward: 1.09\n",
      "episode: 274   score: 2.0   memory length: 49268   epsilon: 1.0    steps: 187     evaluation reward: 1.09\n",
      "episode: 275   score: 0.0   memory length: 49395   epsilon: 1.0    steps: 127     evaluation reward: 1.07\n",
      "episode: 276   score: 0.0   memory length: 49527   epsilon: 1.0    steps: 132     evaluation reward: 1.07\n",
      "episode: 277   score: 0.0   memory length: 49671   epsilon: 1.0    steps: 144     evaluation reward: 1.05\n",
      "episode: 278   score: 0.0   memory length: 49800   epsilon: 1.0    steps: 129     evaluation reward: 1.05\n",
      "episode: 279   score: 0.0   memory length: 49932   epsilon: 1.0    steps: 132     evaluation reward: 1.04\n",
      "now time :  2019-09-25 16:14:12.162261\n",
      "episode: 280   score: 2.0   memory length: 50140   epsilon: 0.9997208200000061    steps: 208     evaluation reward: 1.03\n",
      "episode: 281   score: 0.0   memory length: 50269   epsilon: 0.9994654000000116    steps: 129     evaluation reward: 1.02\n",
      "episode: 282   score: 0.0   memory length: 50399   epsilon: 0.9992080000000172    steps: 130     evaluation reward: 1.01\n",
      "episode: 283   score: 0.0   memory length: 50530   epsilon: 0.9989486200000228    steps: 131     evaluation reward: 1.01\n",
      "episode: 284   score: 1.0   memory length: 50693   epsilon: 0.9986258800000298    steps: 163     evaluation reward: 1.02\n",
      "episode: 285   score: 1.0   memory length: 50871   epsilon: 0.9982734400000375    steps: 178     evaluation reward: 1.02\n",
      "episode: 286   score: 0.0   memory length: 51007   epsilon: 0.9980041600000433    steps: 136     evaluation reward: 1.01\n",
      "episode: 287   score: 1.0   memory length: 51171   epsilon: 0.9976794400000504    steps: 164     evaluation reward: 1.0\n",
      "episode: 288   score: 2.0   memory length: 51354   epsilon: 0.9973171000000582    steps: 183     evaluation reward: 1.02\n",
      "episode: 289   score: 2.0   memory length: 51546   epsilon: 0.9969369400000665    steps: 192     evaluation reward: 1.03\n",
      "episode: 290   score: 0.0   memory length: 51675   epsilon: 0.996681520000072    steps: 129     evaluation reward: 1.03\n",
      "episode: 291   score: 2.0   memory length: 51895   epsilon: 0.9962459200000815    steps: 220     evaluation reward: 1.04\n",
      "episode: 292   score: 3.0   memory length: 52141   epsilon: 0.9957588400000921    steps: 246     evaluation reward: 1.07\n",
      "episode: 293   score: 0.0   memory length: 52275   epsilon: 0.9954935200000978    steps: 134     evaluation reward: 1.06\n",
      "episode: 294   score: 1.0   memory length: 52436   epsilon: 0.9951747400001048    steps: 161     evaluation reward: 1.07\n",
      "episode: 295   score: 0.0   memory length: 52563   epsilon: 0.9949232800001102    steps: 127     evaluation reward: 1.06\n",
      "episode: 296   score: 1.0   memory length: 52740   epsilon: 0.9945728200001178    steps: 177     evaluation reward: 1.05\n",
      "episode: 297   score: 3.0   memory length: 53013   epsilon: 0.9940322800001296    steps: 273     evaluation reward: 1.08\n",
      "episode: 298   score: 0.0   memory length: 53145   epsilon: 0.9937709200001352    steps: 132     evaluation reward: 1.07\n",
      "episode: 299   score: 0.0   memory length: 53288   epsilon: 0.9934877800001414    steps: 143     evaluation reward: 1.05\n",
      "episode: 300   score: 1.0   memory length: 53462   epsilon: 0.9931432600001489    steps: 174     evaluation reward: 1.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 301   score: 3.0   memory length: 53716   epsilon: 0.9926403400001598    steps: 254     evaluation reward: 1.08\n",
      "episode: 302   score: 1.0   memory length: 53873   epsilon: 0.9923294800001665    steps: 157     evaluation reward: 1.08\n",
      "episode: 303   score: 0.0   memory length: 54013   epsilon: 0.9920522800001725    steps: 140     evaluation reward: 1.06\n",
      "episode: 304   score: 0.0   memory length: 54145   epsilon: 0.9917909200001782    steps: 132     evaluation reward: 1.06\n",
      "episode: 305   score: 0.0   memory length: 54277   epsilon: 0.9915295600001839    steps: 132     evaluation reward: 1.03\n",
      "episode: 306   score: 1.0   memory length: 54456   epsilon: 0.9911751400001916    steps: 179     evaluation reward: 1.03\n",
      "episode: 307   score: 0.0   memory length: 54584   epsilon: 0.9909217000001971    steps: 128     evaluation reward: 1.02\n",
      "episode: 308   score: 2.0   memory length: 54824   epsilon: 0.9904465000002074    steps: 240     evaluation reward: 1.04\n",
      "episode: 309   score: 0.0   memory length: 54947   epsilon: 0.9902029600002127    steps: 123     evaluation reward: 1.04\n",
      "episode: 310   score: 0.0   memory length: 55075   epsilon: 0.9899495200002182    steps: 128     evaluation reward: 1.02\n",
      "episode: 311   score: 0.0   memory length: 55220   epsilon: 0.9896624200002244    steps: 145     evaluation reward: 1.01\n",
      "episode: 312   score: 3.0   memory length: 55460   epsilon: 0.9891872200002347    steps: 240     evaluation reward: 1.02\n",
      "episode: 313   score: 6.0   memory length: 55822   epsilon: 0.9884704600002503    steps: 362     evaluation reward: 1.08\n",
      "episode: 314   score: 0.0   memory length: 55960   epsilon: 0.9881972200002562    steps: 138     evaluation reward: 1.08\n",
      "episode: 315   score: 4.0   memory length: 56261   epsilon: 0.9876012400002692    steps: 301     evaluation reward: 1.12\n",
      "episode: 316   score: 1.0   memory length: 56421   epsilon: 0.987284440000276    steps: 160     evaluation reward: 1.11\n",
      "episode: 317   score: 3.0   memory length: 56676   epsilon: 0.986779540000287    steps: 255     evaluation reward: 1.13\n",
      "episode: 318   score: 1.0   memory length: 56854   epsilon: 0.9864271000002947    steps: 178     evaluation reward: 1.13\n",
      "episode: 319   score: 1.0   memory length: 57008   epsilon: 0.9861221800003013    steps: 154     evaluation reward: 1.14\n",
      "episode: 320   score: 1.0   memory length: 57164   epsilon: 0.985813300000308    steps: 156     evaluation reward: 1.15\n",
      "episode: 321   score: 1.0   memory length: 57323   epsilon: 0.9854984800003148    steps: 159     evaluation reward: 1.16\n",
      "episode: 322   score: 2.0   memory length: 57544   epsilon: 0.9850609000003243    steps: 221     evaluation reward: 1.17\n",
      "episode: 323   score: 0.0   memory length: 57674   epsilon: 0.9848035000003299    steps: 130     evaluation reward: 1.17\n",
      "episode: 324   score: 1.0   memory length: 57846   epsilon: 0.9844629400003373    steps: 172     evaluation reward: 1.18\n",
      "episode: 325   score: 0.0   memory length: 57972   epsilon: 0.9842134600003427    steps: 126     evaluation reward: 1.17\n",
      "episode: 326   score: 2.0   memory length: 58193   epsilon: 0.9837758800003522    steps: 221     evaluation reward: 1.18\n",
      "episode: 327   score: 0.0   memory length: 58338   epsilon: 0.9834887800003584    steps: 145     evaluation reward: 1.16\n",
      "episode: 328   score: 1.0   memory length: 58509   epsilon: 0.9831502000003658    steps: 171     evaluation reward: 1.15\n",
      "episode: 329   score: 0.0   memory length: 58648   epsilon: 0.9828749800003718    steps: 139     evaluation reward: 1.14\n",
      "episode: 330   score: 0.0   memory length: 58778   epsilon: 0.9826175800003774    steps: 130     evaluation reward: 1.12\n",
      "episode: 331   score: 7.0   memory length: 59170   epsilon: 0.9818414200003942    steps: 392     evaluation reward: 1.17\n",
      "episode: 332   score: 3.0   memory length: 59402   epsilon: 0.9813820600004042    steps: 232     evaluation reward: 1.19\n",
      "episode: 333   score: 0.0   memory length: 59534   epsilon: 0.9811207000004099    steps: 132     evaluation reward: 1.17\n",
      "episode: 334   score: 4.0   memory length: 59828   epsilon: 0.9805385800004225    steps: 294     evaluation reward: 1.21\n",
      "episode: 335   score: 2.0   memory length: 60029   epsilon: 0.9801406000004311    steps: 201     evaluation reward: 1.22\n",
      "episode: 336   score: 3.0   memory length: 60247   epsilon: 0.9797089600004405    steps: 218     evaluation reward: 1.25\n",
      "episode: 337   score: 1.0   memory length: 60404   epsilon: 0.9793981000004472    steps: 157     evaluation reward: 1.26\n",
      "episode: 338   score: 1.0   memory length: 60583   epsilon: 0.9790436800004549    steps: 179     evaluation reward: 1.27\n",
      "episode: 339   score: 3.0   memory length: 60835   epsilon: 0.9785447200004658    steps: 252     evaluation reward: 1.27\n",
      "episode: 340   score: 5.0   memory length: 61187   epsilon: 0.9778477600004809    steps: 352     evaluation reward: 1.32\n",
      "episode: 341   score: 2.0   memory length: 61415   epsilon: 0.9773963200004907    steps: 228     evaluation reward: 1.31\n",
      "episode: 342   score: 0.0   memory length: 61546   epsilon: 0.9771369400004963    steps: 131     evaluation reward: 1.28\n",
      "episode: 343   score: 2.0   memory length: 61763   epsilon: 0.9767072800005057    steps: 217     evaluation reward: 1.3\n",
      "episode: 344   score: 2.0   memory length: 61979   epsilon: 0.976279600000515    steps: 216     evaluation reward: 1.29\n",
      "episode: 345   score: 3.0   memory length: 62217   epsilon: 0.9758083600005252    steps: 238     evaluation reward: 1.3\n",
      "episode: 346   score: 2.0   memory length: 62415   epsilon: 0.9754163200005337    steps: 198     evaluation reward: 1.31\n",
      "episode: 347   score: 0.0   memory length: 62547   epsilon: 0.9751549600005394    steps: 132     evaluation reward: 1.29\n",
      "episode: 348   score: 3.0   memory length: 62797   epsilon: 0.9746599600005501    steps: 250     evaluation reward: 1.29\n",
      "episode: 349   score: 1.0   memory length: 62987   epsilon: 0.9742837600005583    steps: 190     evaluation reward: 1.28\n",
      "episode: 350   score: 1.0   memory length: 63145   epsilon: 0.9739709200005651    steps: 158     evaluation reward: 1.29\n",
      "episode: 351   score: 2.0   memory length: 63369   epsilon: 0.9735274000005747    steps: 224     evaluation reward: 1.3\n",
      "episode: 352   score: 1.0   memory length: 63536   epsilon: 0.9731967400005819    steps: 167     evaluation reward: 1.31\n",
      "episode: 353   score: 3.0   memory length: 63778   epsilon: 0.9727175800005923    steps: 242     evaluation reward: 1.32\n",
      "episode: 354   score: 1.0   memory length: 63950   epsilon: 0.9723770200005997    steps: 172     evaluation reward: 1.33\n",
      "episode: 355   score: 2.0   memory length: 64152   epsilon: 0.9719770600006084    steps: 202     evaluation reward: 1.34\n",
      "episode: 356   score: 1.0   memory length: 64335   epsilon: 0.9716147200006162    steps: 183     evaluation reward: 1.34\n",
      "episode: 357   score: 1.0   memory length: 64516   epsilon: 0.971256340000624    steps: 181     evaluation reward: 1.32\n",
      "episode: 358   score: 1.0   memory length: 64689   epsilon: 0.9709138000006314    steps: 173     evaluation reward: 1.33\n",
      "episode: 359   score: 2.0   memory length: 64883   epsilon: 0.9705296800006398    steps: 194     evaluation reward: 1.31\n",
      "episode: 360   score: 1.0   memory length: 65061   epsilon: 0.9701772400006474    steps: 178     evaluation reward: 1.29\n",
      "episode: 361   score: 0.0   memory length: 65189   epsilon: 0.9699238000006529    steps: 128     evaluation reward: 1.26\n",
      "episode: 362   score: 2.0   memory length: 65389   epsilon: 0.9695278000006615    steps: 200     evaluation reward: 1.28\n",
      "episode: 363   score: 1.0   memory length: 65567   epsilon: 0.9691753600006692    steps: 178     evaluation reward: 1.29\n",
      "episode: 364   score: 2.0   memory length: 65751   epsilon: 0.9688110400006771    steps: 184     evaluation reward: 1.29\n",
      "episode: 365   score: 1.0   memory length: 65909   epsilon: 0.9684982000006839    steps: 158     evaluation reward: 1.28\n",
      "episode: 366   score: 0.0   memory length: 66042   epsilon: 0.9682348600006896    steps: 133     evaluation reward: 1.28\n",
      "episode: 367   score: 1.0   memory length: 66200   epsilon: 0.9679220200006964    steps: 158     evaluation reward: 1.26\n",
      "episode: 368   score: 0.0   memory length: 66351   epsilon: 0.9676230400007029    steps: 151     evaluation reward: 1.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 369   score: 0.0   memory length: 66489   epsilon: 0.9673498000007088    steps: 138     evaluation reward: 1.25\n",
      "episode: 370   score: 3.0   memory length: 66741   epsilon: 0.9668508400007196    steps: 252     evaluation reward: 1.26\n",
      "episode: 371   score: 1.0   memory length: 66894   epsilon: 0.9665479000007262    steps: 153     evaluation reward: 1.27\n",
      "episode: 372   score: 0.0   memory length: 67023   epsilon: 0.9662924800007318    steps: 129     evaluation reward: 1.27\n",
      "episode: 373   score: 1.0   memory length: 67187   epsilon: 0.9659677600007388    steps: 164     evaluation reward: 1.28\n",
      "episode: 374   score: 1.0   memory length: 67353   epsilon: 0.9656390800007459    steps: 166     evaluation reward: 1.27\n",
      "episode: 375   score: 1.0   memory length: 67504   epsilon: 0.9653401000007524    steps: 151     evaluation reward: 1.28\n",
      "episode: 376   score: 0.0   memory length: 67629   epsilon: 0.9650926000007578    steps: 125     evaluation reward: 1.28\n",
      "episode: 377   score: 0.0   memory length: 67759   epsilon: 0.9648352000007634    steps: 130     evaluation reward: 1.28\n",
      "episode: 378   score: 3.0   memory length: 68009   epsilon: 0.9643402000007741    steps: 250     evaluation reward: 1.31\n",
      "episode: 379   score: 2.0   memory length: 68230   epsilon: 0.9639026200007836    steps: 221     evaluation reward: 1.33\n",
      "episode: 380   score: 1.0   memory length: 68405   epsilon: 0.9635561200007912    steps: 175     evaluation reward: 1.32\n",
      "episode: 381   score: 3.0   memory length: 68644   epsilon: 0.9630829000008014    steps: 239     evaluation reward: 1.35\n",
      "episode: 382   score: 3.0   memory length: 68909   epsilon: 0.9625582000008128    steps: 265     evaluation reward: 1.38\n",
      "episode: 383   score: 3.0   memory length: 69151   epsilon: 0.9620790400008232    steps: 242     evaluation reward: 1.41\n",
      "episode: 384   score: 0.0   memory length: 69282   epsilon: 0.9618196600008289    steps: 131     evaluation reward: 1.4\n",
      "episode: 385   score: 0.0   memory length: 69411   epsilon: 0.9615642400008344    steps: 129     evaluation reward: 1.39\n",
      "episode: 386   score: 0.0   memory length: 69544   epsilon: 0.9613009000008401    steps: 133     evaluation reward: 1.39\n",
      "episode: 387   score: 1.0   memory length: 69717   epsilon: 0.9609583600008476    steps: 173     evaluation reward: 1.39\n",
      "episode: 388   score: 1.0   memory length: 69887   epsilon: 0.9606217600008549    steps: 170     evaluation reward: 1.38\n",
      "episode: 389   score: 0.0   memory length: 70020   epsilon: 0.9603584200008606    steps: 133     evaluation reward: 1.36\n",
      "episode: 390   score: 0.0   memory length: 70152   epsilon: 0.9600970600008663    steps: 132     evaluation reward: 1.36\n",
      "episode: 391   score: 3.0   memory length: 70433   epsilon: 0.9595406800008783    steps: 281     evaluation reward: 1.37\n",
      "episode: 392   score: 3.0   memory length: 70685   epsilon: 0.9590417200008892    steps: 252     evaluation reward: 1.37\n",
      "episode: 393   score: 0.0   memory length: 70818   epsilon: 0.9587783800008949    steps: 133     evaluation reward: 1.37\n",
      "episode: 394   score: 3.0   memory length: 71041   epsilon: 0.9583368400009045    steps: 223     evaluation reward: 1.39\n",
      "episode: 395   score: 4.0   memory length: 71348   epsilon: 0.9577289800009177    steps: 307     evaluation reward: 1.43\n",
      "episode: 396   score: 1.0   memory length: 71520   epsilon: 0.9573884200009251    steps: 172     evaluation reward: 1.43\n",
      "episode: 397   score: 1.0   memory length: 71698   epsilon: 0.9570359800009327    steps: 178     evaluation reward: 1.41\n",
      "episode: 398   score: 3.0   memory length: 71954   epsilon: 0.9565291000009437    steps: 256     evaluation reward: 1.44\n",
      "episode: 399   score: 2.0   memory length: 72138   epsilon: 0.9561647800009516    steps: 184     evaluation reward: 1.46\n",
      "episode: 400   score: 1.0   memory length: 72299   epsilon: 0.9558460000009585    steps: 161     evaluation reward: 1.46\n",
      "episode: 401   score: 2.0   memory length: 72504   epsilon: 0.9554401000009674    steps: 205     evaluation reward: 1.45\n",
      "episode: 402   score: 0.0   memory length: 72638   epsilon: 0.9551747800009731    steps: 134     evaluation reward: 1.44\n",
      "episode: 403   score: 1.0   memory length: 72798   epsilon: 0.95485798000098    steps: 160     evaluation reward: 1.45\n",
      "episode: 404   score: 1.0   memory length: 72953   epsilon: 0.9545510800009867    steps: 155     evaluation reward: 1.46\n",
      "episode: 405   score: 2.0   memory length: 73164   epsilon: 0.9541333000009957    steps: 211     evaluation reward: 1.48\n",
      "episode: 406   score: 0.0   memory length: 73290   epsilon: 0.9538838200010011    steps: 126     evaluation reward: 1.47\n",
      "episode: 407   score: 0.0   memory length: 73420   epsilon: 0.9536264200010067    steps: 130     evaluation reward: 1.47\n",
      "episode: 408   score: 0.0   memory length: 73554   epsilon: 0.9533611000010125    steps: 134     evaluation reward: 1.45\n",
      "episode: 409   score: 0.0   memory length: 73690   epsilon: 0.9530918200010183    steps: 136     evaluation reward: 1.45\n",
      "episode: 410   score: 1.0   memory length: 73859   epsilon: 0.9527572000010256    steps: 169     evaluation reward: 1.46\n",
      "episode: 411   score: 1.0   memory length: 74033   epsilon: 0.9524126800010331    steps: 174     evaluation reward: 1.47\n",
      "episode: 412   score: 3.0   memory length: 74286   epsilon: 0.951911740001044    steps: 253     evaluation reward: 1.47\n",
      "episode: 413   score: 3.0   memory length: 74527   epsilon: 0.9514345600010543    steps: 241     evaluation reward: 1.44\n",
      "episode: 414   score: 1.0   memory length: 74700   epsilon: 0.9510920200010617    steps: 173     evaluation reward: 1.45\n",
      "episode: 415   score: 0.0   memory length: 74829   epsilon: 0.9508366000010673    steps: 129     evaluation reward: 1.41\n",
      "episode: 416   score: 0.0   memory length: 74956   epsilon: 0.9505851400010727    steps: 127     evaluation reward: 1.4\n",
      "episode: 417   score: 3.0   memory length: 75232   epsilon: 0.9500386600010846    steps: 276     evaluation reward: 1.4\n",
      "episode: 418   score: 0.0   memory length: 75354   epsilon: 0.9497971000010899    steps: 122     evaluation reward: 1.39\n",
      "episode: 419   score: 0.0   memory length: 75476   epsilon: 0.9495555400010951    steps: 122     evaluation reward: 1.38\n",
      "episode: 420   score: 2.0   memory length: 75674   epsilon: 0.9491635000011036    steps: 198     evaluation reward: 1.39\n",
      "episode: 421   score: 2.0   memory length: 75896   epsilon: 0.9487239400011132    steps: 222     evaluation reward: 1.4\n",
      "episode: 422   score: 0.0   memory length: 76021   epsilon: 0.9484764400011185    steps: 125     evaluation reward: 1.38\n",
      "episode: 423   score: 1.0   memory length: 76194   epsilon: 0.948133900001126    steps: 173     evaluation reward: 1.39\n",
      "episode: 424   score: 2.0   memory length: 76415   epsilon: 0.9476963200011355    steps: 221     evaluation reward: 1.4\n",
      "episode: 425   score: 1.0   memory length: 76590   epsilon: 0.947349820001143    steps: 175     evaluation reward: 1.41\n",
      "episode: 426   score: 2.0   memory length: 76778   epsilon: 0.9469775800011511    steps: 188     evaluation reward: 1.41\n",
      "episode: 427   score: 3.0   memory length: 77014   epsilon: 0.9465103000011612    steps: 236     evaluation reward: 1.44\n",
      "episode: 428   score: 0.0   memory length: 77140   epsilon: 0.9462608200011666    steps: 126     evaluation reward: 1.43\n",
      "episode: 429   score: 2.0   memory length: 77343   epsilon: 0.9458588800011754    steps: 203     evaluation reward: 1.45\n",
      "episode: 430   score: 2.0   memory length: 77543   epsilon: 0.945462880001184    steps: 200     evaluation reward: 1.47\n",
      "episode: 431   score: 0.0   memory length: 77682   epsilon: 0.9451876600011899    steps: 139     evaluation reward: 1.4\n",
      "episode: 432   score: 0.0   memory length: 77829   epsilon: 0.9448966000011962    steps: 147     evaluation reward: 1.37\n",
      "episode: 433   score: 2.0   memory length: 78034   epsilon: 0.944490700001205    steps: 205     evaluation reward: 1.39\n",
      "episode: 434   score: 1.0   memory length: 78189   epsilon: 0.9441838000012117    steps: 155     evaluation reward: 1.36\n",
      "episode: 435   score: 1.0   memory length: 78346   epsilon: 0.9438729400012185    steps: 157     evaluation reward: 1.35\n",
      "episode: 436   score: 2.0   memory length: 78573   epsilon: 0.9434234800012282    steps: 227     evaluation reward: 1.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 437   score: 0.0   memory length: 78699   epsilon: 0.9431740000012336    steps: 126     evaluation reward: 1.33\n",
      "episode: 438   score: 1.0   memory length: 78860   epsilon: 0.9428552200012406    steps: 161     evaluation reward: 1.33\n",
      "episode: 439   score: 1.0   memory length: 79029   epsilon: 0.9425206000012478    steps: 169     evaluation reward: 1.31\n",
      "episode: 440   score: 1.0   memory length: 79216   epsilon: 0.9421503400012559    steps: 187     evaluation reward: 1.27\n",
      "episode: 441   score: 0.0   memory length: 79345   epsilon: 0.9418949200012614    steps: 129     evaluation reward: 1.25\n",
      "episode: 442   score: 1.0   memory length: 79523   epsilon: 0.941542480001269    steps: 178     evaluation reward: 1.26\n",
      "episode: 443   score: 4.0   memory length: 79803   epsilon: 0.9409880800012811    steps: 280     evaluation reward: 1.28\n",
      "episode: 444   score: 1.0   memory length: 79962   epsilon: 0.9406732600012879    steps: 159     evaluation reward: 1.27\n",
      "episode: 445   score: 0.0   memory length: 80099   epsilon: 0.9404020000012938    steps: 137     evaluation reward: 1.24\n",
      "episode: 446   score: 0.0   memory length: 80230   epsilon: 0.9401426200012994    steps: 131     evaluation reward: 1.22\n",
      "episode: 447   score: 0.0   memory length: 80362   epsilon: 0.9398812600013051    steps: 132     evaluation reward: 1.22\n",
      "episode: 448   score: 1.0   memory length: 80524   epsilon: 0.9395605000013121    steps: 162     evaluation reward: 1.2\n",
      "episode: 449   score: 0.0   memory length: 80660   epsilon: 0.9392912200013179    steps: 136     evaluation reward: 1.19\n",
      "episode: 450   score: 2.0   memory length: 80887   epsilon: 0.9388417600013277    steps: 227     evaluation reward: 1.2\n",
      "episode: 451   score: 2.0   memory length: 81098   epsilon: 0.9384239800013368    steps: 211     evaluation reward: 1.2\n",
      "episode: 452   score: 0.0   memory length: 81233   epsilon: 0.9381566800013426    steps: 135     evaluation reward: 1.19\n",
      "episode: 453   score: 2.0   memory length: 81468   epsilon: 0.9376913800013527    steps: 235     evaluation reward: 1.18\n",
      "episode: 454   score: 0.0   memory length: 81601   epsilon: 0.9374280400013584    steps: 133     evaluation reward: 1.17\n",
      "episode: 455   score: 1.0   memory length: 81761   epsilon: 0.9371112400013653    steps: 160     evaluation reward: 1.16\n",
      "episode: 456   score: 0.0   memory length: 81891   epsilon: 0.9368538400013708    steps: 130     evaluation reward: 1.15\n",
      "episode: 457   score: 2.0   memory length: 82075   epsilon: 0.9364895200013788    steps: 184     evaluation reward: 1.16\n",
      "episode: 458   score: 0.0   memory length: 82205   epsilon: 0.9362321200013843    steps: 130     evaluation reward: 1.15\n",
      "episode: 459   score: 1.0   memory length: 82392   epsilon: 0.9358618600013924    steps: 187     evaluation reward: 1.14\n",
      "episode: 460   score: 1.0   memory length: 82573   epsilon: 0.9355034800014002    steps: 181     evaluation reward: 1.14\n",
      "episode: 461   score: 3.0   memory length: 82824   epsilon: 0.935006500001411    steps: 251     evaluation reward: 1.17\n",
      "episode: 462   score: 1.0   memory length: 82985   epsilon: 0.9346877200014179    steps: 161     evaluation reward: 1.16\n",
      "episode: 463   score: 1.0   memory length: 83163   epsilon: 0.9343352800014255    steps: 178     evaluation reward: 1.16\n",
      "episode: 464   score: 1.0   memory length: 83330   epsilon: 0.9340046200014327    steps: 167     evaluation reward: 1.15\n",
      "episode: 465   score: 3.0   memory length: 83587   epsilon: 0.9334957600014437    steps: 257     evaluation reward: 1.17\n",
      "episode: 466   score: 4.0   memory length: 83874   epsilon: 0.9329275000014561    steps: 287     evaluation reward: 1.21\n",
      "episode: 467   score: 3.0   memory length: 84107   epsilon: 0.9324661600014661    steps: 233     evaluation reward: 1.23\n",
      "episode: 468   score: 5.0   memory length: 84463   epsilon: 0.9317612800014814    steps: 356     evaluation reward: 1.28\n",
      "episode: 469   score: 1.0   memory length: 84622   epsilon: 0.9314464600014882    steps: 159     evaluation reward: 1.29\n",
      "episode: 470   score: 1.0   memory length: 84798   epsilon: 0.9310979800014958    steps: 176     evaluation reward: 1.27\n",
      "episode: 471   score: 2.0   memory length: 85026   epsilon: 0.9306465400015056    steps: 228     evaluation reward: 1.28\n",
      "episode: 472   score: 2.0   memory length: 85222   epsilon: 0.930258460001514    steps: 196     evaluation reward: 1.3\n",
      "episode: 473   score: 3.0   memory length: 85449   epsilon: 0.9298090000015238    steps: 227     evaluation reward: 1.32\n",
      "episode: 474   score: 0.0   memory length: 85583   epsilon: 0.9295436800015295    steps: 134     evaluation reward: 1.31\n",
      "episode: 475   score: 1.0   memory length: 85737   epsilon: 0.9292387600015362    steps: 154     evaluation reward: 1.31\n",
      "episode: 476   score: 1.0   memory length: 85892   epsilon: 0.9289318600015428    steps: 155     evaluation reward: 1.32\n",
      "episode: 477   score: 4.0   memory length: 86171   epsilon: 0.9283794400015548    steps: 279     evaluation reward: 1.36\n",
      "episode: 478   score: 2.0   memory length: 86371   epsilon: 0.9279834400015634    steps: 200     evaluation reward: 1.35\n",
      "episode: 479   score: 0.0   memory length: 86500   epsilon: 0.927728020001569    steps: 129     evaluation reward: 1.33\n",
      "episode: 480   score: 1.0   memory length: 86673   epsilon: 0.9273854800015764    steps: 173     evaluation reward: 1.33\n",
      "episode: 481   score: 3.0   memory length: 86919   epsilon: 0.926898400001587    steps: 246     evaluation reward: 1.33\n",
      "episode: 482   score: 1.0   memory length: 87074   epsilon: 0.9265915000015936    steps: 155     evaluation reward: 1.31\n",
      "episode: 483   score: 0.0   memory length: 87208   epsilon: 0.9263261800015994    steps: 134     evaluation reward: 1.28\n",
      "episode: 484   score: 2.0   memory length: 87412   epsilon: 0.9259222600016082    steps: 204     evaluation reward: 1.3\n",
      "episode: 485   score: 3.0   memory length: 87681   epsilon: 0.9253896400016197    steps: 269     evaluation reward: 1.33\n",
      "episode: 486   score: 1.0   memory length: 87855   epsilon: 0.9250451200016272    steps: 174     evaluation reward: 1.34\n",
      "episode: 487   score: 0.0   memory length: 87997   epsilon: 0.9247639600016333    steps: 142     evaluation reward: 1.33\n",
      "episode: 488   score: 1.0   memory length: 88149   epsilon: 0.9244630000016398    steps: 152     evaluation reward: 1.33\n",
      "episode: 489   score: 1.0   memory length: 88304   epsilon: 0.9241561000016465    steps: 155     evaluation reward: 1.34\n",
      "episode: 490   score: 1.0   memory length: 88478   epsilon: 0.923811580001654    steps: 174     evaluation reward: 1.35\n",
      "episode: 491   score: 0.0   memory length: 88603   epsilon: 0.9235640800016593    steps: 125     evaluation reward: 1.32\n",
      "episode: 492   score: 0.0   memory length: 88731   epsilon: 0.9233106400016649    steps: 128     evaluation reward: 1.29\n",
      "episode: 493   score: 3.0   memory length: 88947   epsilon: 0.9228829600016741    steps: 216     evaluation reward: 1.32\n",
      "episode: 494   score: 2.0   memory length: 89161   epsilon: 0.9224592400016833    steps: 214     evaluation reward: 1.31\n",
      "episode: 495   score: 1.0   memory length: 89352   epsilon: 0.9220810600016915    steps: 191     evaluation reward: 1.28\n",
      "episode: 496   score: 4.0   memory length: 89652   epsilon: 0.9214870600017044    steps: 300     evaluation reward: 1.31\n",
      "episode: 497   score: 0.0   memory length: 89777   epsilon: 0.9212395600017098    steps: 125     evaluation reward: 1.3\n",
      "episode: 498   score: 1.0   memory length: 89936   epsilon: 0.9209247400017166    steps: 159     evaluation reward: 1.28\n",
      "episode: 499   score: 1.0   memory length: 90087   epsilon: 0.9206257600017231    steps: 151     evaluation reward: 1.27\n",
      "episode: 500   score: 0.0   memory length: 90223   epsilon: 0.920356480001729    steps: 136     evaluation reward: 1.26\n",
      "episode: 501   score: 3.0   memory length: 90493   epsilon: 0.9198218800017406    steps: 270     evaluation reward: 1.27\n",
      "episode: 502   score: 1.0   memory length: 90658   epsilon: 0.9194951800017477    steps: 165     evaluation reward: 1.28\n",
      "episode: 503   score: 1.0   memory length: 90830   epsilon: 0.9191546200017551    steps: 172     evaluation reward: 1.28\n",
      "episode: 504   score: 2.0   memory length: 91028   epsilon: 0.9187625800017636    steps: 198     evaluation reward: 1.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 505   score: 4.0   memory length: 91302   epsilon: 0.9182200600017754    steps: 274     evaluation reward: 1.31\n",
      "episode: 506   score: 5.0   memory length: 91619   epsilon: 0.917592400001789    steps: 317     evaluation reward: 1.36\n",
      "episode: 507   score: 1.0   memory length: 91782   epsilon: 0.917269660001796    steps: 163     evaluation reward: 1.37\n",
      "episode: 508   score: 2.0   memory length: 92002   epsilon: 0.9168340600018055    steps: 220     evaluation reward: 1.39\n",
      "episode: 509   score: 1.0   memory length: 92180   epsilon: 0.9164816200018131    steps: 178     evaluation reward: 1.4\n",
      "episode: 510   score: 3.0   memory length: 92431   epsilon: 0.9159846400018239    steps: 251     evaluation reward: 1.42\n",
      "episode: 511   score: 3.0   memory length: 92680   epsilon: 0.9154916200018346    steps: 249     evaluation reward: 1.44\n",
      "episode: 512   score: 2.0   memory length: 92864   epsilon: 0.9151273000018425    steps: 184     evaluation reward: 1.43\n",
      "episode: 513   score: 1.0   memory length: 93022   epsilon: 0.9148144600018493    steps: 158     evaluation reward: 1.41\n",
      "episode: 514   score: 1.0   memory length: 93181   epsilon: 0.9144996400018561    steps: 159     evaluation reward: 1.41\n",
      "episode: 515   score: 2.0   memory length: 93392   epsilon: 0.9140818600018652    steps: 211     evaluation reward: 1.43\n",
      "episode: 516   score: 3.0   memory length: 93641   epsilon: 0.9135888400018759    steps: 249     evaluation reward: 1.46\n",
      "episode: 517   score: 2.0   memory length: 93827   epsilon: 0.9132205600018839    steps: 186     evaluation reward: 1.45\n",
      "episode: 518   score: 2.0   memory length: 94047   epsilon: 0.9127849600018934    steps: 220     evaluation reward: 1.47\n",
      "episode: 519   score: 0.0   memory length: 94174   epsilon: 0.9125335000018988    steps: 127     evaluation reward: 1.47\n",
      "episode: 520   score: 5.0   memory length: 94513   epsilon: 0.9118622800019134    steps: 339     evaluation reward: 1.5\n",
      "episode: 521   score: 1.0   memory length: 94688   epsilon: 0.9115157800019209    steps: 175     evaluation reward: 1.49\n",
      "episode: 522   score: 4.0   memory length: 94960   epsilon: 0.9109772200019326    steps: 272     evaluation reward: 1.53\n",
      "episode: 523   score: 1.0   memory length: 95122   epsilon: 0.9106564600019396    steps: 162     evaluation reward: 1.53\n",
      "episode: 524   score: 2.0   memory length: 95312   epsilon: 0.9102802600019477    steps: 190     evaluation reward: 1.53\n",
      "episode: 525   score: 0.0   memory length: 95440   epsilon: 0.9100268200019532    steps: 128     evaluation reward: 1.52\n",
      "episode: 526   score: 2.0   memory length: 95647   epsilon: 0.9096169600019621    steps: 207     evaluation reward: 1.52\n",
      "episode: 527   score: 0.0   memory length: 95779   epsilon: 0.9093556000019678    steps: 132     evaluation reward: 1.49\n",
      "episode: 528   score: 2.0   memory length: 96004   epsilon: 0.9089101000019775    steps: 225     evaluation reward: 1.51\n",
      "episode: 529   score: 3.0   memory length: 96252   epsilon: 0.9084190600019881    steps: 248     evaluation reward: 1.52\n",
      "episode: 530   score: 1.0   memory length: 96433   epsilon: 0.9080606800019959    steps: 181     evaluation reward: 1.51\n",
      "episode: 531   score: 1.0   memory length: 96605   epsilon: 0.9077201200020033    steps: 172     evaluation reward: 1.52\n",
      "episode: 532   score: 3.0   memory length: 96838   epsilon: 0.9072587800020133    steps: 233     evaluation reward: 1.55\n",
      "episode: 533   score: 5.0   memory length: 97133   epsilon: 0.906674680002026    steps: 295     evaluation reward: 1.58\n",
      "episode: 534   score: 4.0   memory length: 97396   epsilon: 0.9061539400020373    steps: 263     evaluation reward: 1.61\n",
      "episode: 535   score: 3.0   memory length: 97631   epsilon: 0.9056886400020474    steps: 235     evaluation reward: 1.63\n",
      "episode: 536   score: 3.0   memory length: 97846   epsilon: 0.9052629400020566    steps: 215     evaluation reward: 1.64\n",
      "episode: 537   score: 2.0   memory length: 98036   epsilon: 0.9048867400020648    steps: 190     evaluation reward: 1.66\n",
      "episode: 538   score: 2.0   memory length: 98256   epsilon: 0.9044511400020743    steps: 220     evaluation reward: 1.67\n",
      "episode: 539   score: 2.0   memory length: 98484   epsilon: 0.9039997000020841    steps: 228     evaluation reward: 1.68\n",
      "episode: 540   score: 2.0   memory length: 98697   epsilon: 0.9035779600020932    steps: 213     evaluation reward: 1.69\n",
      "episode: 541   score: 4.0   memory length: 98977   epsilon: 0.9030235600021053    steps: 280     evaluation reward: 1.73\n",
      "episode: 542   score: 2.0   memory length: 99167   epsilon: 0.9026473600021134    steps: 190     evaluation reward: 1.74\n",
      "episode: 543   score: 2.0   memory length: 99360   epsilon: 0.9022652200021217    steps: 193     evaluation reward: 1.72\n",
      "episode: 544   score: 1.0   memory length: 99517   epsilon: 0.9019543600021285    steps: 157     evaluation reward: 1.72\n",
      "episode: 545   score: 1.0   memory length: 99681   epsilon: 0.9016296400021355    steps: 164     evaluation reward: 1.73\n",
      "episode: 546   score: 1.0   memory length: 99838   epsilon: 0.9013187800021423    steps: 157     evaluation reward: 1.74\n",
      "now time :  2019-09-25 16:25:09.109769\n",
      "episode: 547   score: 2.0   memory length: 100027   epsilon: 0.9009445600021504    steps: 189     evaluation reward: 1.76\n",
      "episode: 548   score: 1.0   memory length: 100203   epsilon: 0.900596080002158    steps: 176     evaluation reward: 1.76\n",
      "episode: 549   score: 0.0   memory length: 100341   epsilon: 0.9003228400021639    steps: 138     evaluation reward: 1.76\n",
      "episode: 550   score: 3.0   memory length: 100573   epsilon: 0.8998634800021739    steps: 232     evaluation reward: 1.77\n",
      "episode: 551   score: 3.0   memory length: 100823   epsilon: 0.8993684800021846    steps: 250     evaluation reward: 1.78\n",
      "episode: 552   score: 2.0   memory length: 101047   epsilon: 0.8989249600021942    steps: 224     evaluation reward: 1.8\n",
      "episode: 553   score: 3.0   memory length: 101301   epsilon: 0.8984220400022052    steps: 254     evaluation reward: 1.81\n",
      "episode: 554   score: 2.0   memory length: 101483   epsilon: 0.898061680002213    steps: 182     evaluation reward: 1.83\n",
      "episode: 555   score: 3.0   memory length: 101712   epsilon: 0.8976082600022228    steps: 229     evaluation reward: 1.85\n",
      "episode: 556   score: 2.0   memory length: 101902   epsilon: 0.897232060002231    steps: 190     evaluation reward: 1.87\n",
      "episode: 557   score: 3.0   memory length: 102160   epsilon: 0.8967212200022421    steps: 258     evaluation reward: 1.88\n",
      "episode: 558   score: 1.0   memory length: 102335   epsilon: 0.8963747200022496    steps: 175     evaluation reward: 1.89\n",
      "episode: 559   score: 4.0   memory length: 102619   epsilon: 0.8958124000022618    steps: 284     evaluation reward: 1.92\n",
      "episode: 560   score: 1.0   memory length: 102790   epsilon: 0.8954738200022692    steps: 171     evaluation reward: 1.92\n",
      "episode: 561   score: 2.0   memory length: 103006   epsilon: 0.8950461400022784    steps: 216     evaluation reward: 1.91\n",
      "episode: 562   score: 1.0   memory length: 103164   epsilon: 0.8947333000022852    steps: 158     evaluation reward: 1.91\n",
      "episode: 563   score: 3.0   memory length: 103451   epsilon: 0.8941650400022976    steps: 287     evaluation reward: 1.93\n",
      "episode: 564   score: 3.0   memory length: 103670   epsilon: 0.893731420002307    steps: 219     evaluation reward: 1.95\n",
      "episode: 565   score: 1.0   memory length: 103824   epsilon: 0.8934265000023136    steps: 154     evaluation reward: 1.93\n",
      "episode: 566   score: 4.0   memory length: 104089   epsilon: 0.892901800002325    steps: 265     evaluation reward: 1.93\n",
      "episode: 567   score: 1.0   memory length: 104253   epsilon: 0.892577080002332    steps: 164     evaluation reward: 1.91\n",
      "episode: 568   score: 1.0   memory length: 104414   epsilon: 0.892258300002339    steps: 161     evaluation reward: 1.87\n",
      "episode: 569   score: 1.0   memory length: 104578   epsilon: 0.891933580002346    steps: 164     evaluation reward: 1.87\n",
      "episode: 570   score: 1.0   memory length: 104739   epsilon: 0.8916148000023529    steps: 161     evaluation reward: 1.87\n",
      "episode: 571   score: 0.0   memory length: 104876   epsilon: 0.8913435400023588    steps: 137     evaluation reward: 1.85\n",
      "episode: 572   score: 2.0   memory length: 105094   epsilon: 0.8909119000023682    steps: 218     evaluation reward: 1.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 573   score: 4.0   memory length: 105414   epsilon: 0.890278300002382    steps: 320     evaluation reward: 1.86\n",
      "episode: 574   score: 3.0   memory length: 105673   epsilon: 0.8897654800023931    steps: 259     evaluation reward: 1.89\n",
      "episode: 575   score: 1.0   memory length: 105841   epsilon: 0.8894328400024003    steps: 168     evaluation reward: 1.89\n",
      "episode: 576   score: 3.0   memory length: 106062   epsilon: 0.8889952600024098    steps: 221     evaluation reward: 1.91\n",
      "episode: 577   score: 1.0   memory length: 106222   epsilon: 0.8886784600024167    steps: 160     evaluation reward: 1.88\n",
      "episode: 578   score: 1.0   memory length: 106402   epsilon: 0.8883220600024244    steps: 180     evaluation reward: 1.87\n",
      "episode: 579   score: 0.0   memory length: 106539   epsilon: 0.8880508000024303    steps: 137     evaluation reward: 1.87\n",
      "episode: 580   score: 2.0   memory length: 106728   epsilon: 0.8876765800024384    steps: 189     evaluation reward: 1.88\n",
      "episode: 581   score: 0.0   memory length: 106853   epsilon: 0.8874290800024438    steps: 125     evaluation reward: 1.85\n",
      "episode: 582   score: 2.0   memory length: 107061   epsilon: 0.8870172400024527    steps: 208     evaluation reward: 1.86\n",
      "episode: 583   score: 0.0   memory length: 107193   epsilon: 0.8867558800024584    steps: 132     evaluation reward: 1.86\n",
      "episode: 584   score: 4.0   memory length: 107463   epsilon: 0.88622128000247    steps: 270     evaluation reward: 1.88\n",
      "episode: 585   score: 1.0   memory length: 107648   epsilon: 0.885854980002478    steps: 185     evaluation reward: 1.86\n",
      "episode: 586   score: 0.0   memory length: 107783   epsilon: 0.8855876800024838    steps: 135     evaluation reward: 1.85\n",
      "episode: 587   score: 4.0   memory length: 108065   epsilon: 0.8850293200024959    steps: 282     evaluation reward: 1.89\n",
      "episode: 588   score: 2.0   memory length: 108256   epsilon: 0.8846511400025041    steps: 191     evaluation reward: 1.9\n",
      "episode: 589   score: 5.0   memory length: 108544   epsilon: 0.8840809000025165    steps: 288     evaluation reward: 1.94\n",
      "episode: 590   score: 3.0   memory length: 108772   epsilon: 0.8836294600025263    steps: 228     evaluation reward: 1.96\n",
      "episode: 591   score: 1.0   memory length: 108947   epsilon: 0.8832829600025338    steps: 175     evaluation reward: 1.97\n",
      "episode: 592   score: 2.0   memory length: 109152   epsilon: 0.8828770600025426    steps: 205     evaluation reward: 1.99\n",
      "episode: 593   score: 3.0   memory length: 109407   epsilon: 0.8823721600025536    steps: 255     evaluation reward: 1.99\n",
      "episode: 594   score: 2.0   memory length: 109597   epsilon: 0.8819959600025618    steps: 190     evaluation reward: 1.99\n",
      "episode: 595   score: 3.0   memory length: 109841   epsilon: 0.8815128400025722    steps: 244     evaluation reward: 2.01\n",
      "episode: 596   score: 2.0   memory length: 110053   epsilon: 0.8810930800025814    steps: 212     evaluation reward: 1.99\n",
      "episode: 597   score: 2.0   memory length: 110268   epsilon: 0.8806673800025906    steps: 215     evaluation reward: 2.01\n",
      "episode: 598   score: 2.0   memory length: 110471   epsilon: 0.8802654400025993    steps: 203     evaluation reward: 2.02\n",
      "episode: 599   score: 0.0   memory length: 110598   epsilon: 0.8800139800026048    steps: 127     evaluation reward: 2.01\n",
      "episode: 600   score: 3.0   memory length: 110837   epsilon: 0.879540760002615    steps: 239     evaluation reward: 2.04\n",
      "episode: 601   score: 1.0   memory length: 110995   epsilon: 0.8792279200026218    steps: 158     evaluation reward: 2.02\n",
      "episode: 602   score: 1.0   memory length: 111154   epsilon: 0.8789131000026287    steps: 159     evaluation reward: 2.02\n",
      "episode: 603   score: 2.0   memory length: 111354   epsilon: 0.8785171000026373    steps: 200     evaluation reward: 2.03\n",
      "episode: 604   score: 4.0   memory length: 111620   epsilon: 0.8779904200026487    steps: 266     evaluation reward: 2.05\n",
      "episode: 605   score: 1.0   memory length: 111774   epsilon: 0.8776855000026553    steps: 154     evaluation reward: 2.02\n",
      "episode: 606   score: 1.0   memory length: 111957   epsilon: 0.8773231600026632    steps: 183     evaluation reward: 1.98\n",
      "episode: 607   score: 3.0   memory length: 112210   epsilon: 0.8768222200026741    steps: 253     evaluation reward: 2.0\n",
      "episode: 608   score: 4.0   memory length: 112487   epsilon: 0.876273760002686    steps: 277     evaluation reward: 2.02\n",
      "episode: 609   score: 1.0   memory length: 112670   epsilon: 0.8759114200026938    steps: 183     evaluation reward: 2.02\n",
      "episode: 610   score: 1.0   memory length: 112824   epsilon: 0.8756065000027005    steps: 154     evaluation reward: 2.0\n",
      "episode: 611   score: 1.0   memory length: 112979   epsilon: 0.8752996000027071    steps: 155     evaluation reward: 1.98\n",
      "episode: 612   score: 3.0   memory length: 113226   epsilon: 0.8748105400027177    steps: 247     evaluation reward: 1.99\n",
      "episode: 613   score: 4.0   memory length: 113484   epsilon: 0.8742997000027288    steps: 258     evaluation reward: 2.02\n",
      "episode: 614   score: 4.0   memory length: 113767   epsilon: 0.873739360002741    steps: 283     evaluation reward: 2.05\n",
      "episode: 615   score: 1.0   memory length: 113922   epsilon: 0.8734324600027477    steps: 155     evaluation reward: 2.04\n",
      "episode: 616   score: 1.0   memory length: 114084   epsilon: 0.8731117000027546    steps: 162     evaluation reward: 2.02\n",
      "episode: 617   score: 5.0   memory length: 114362   epsilon: 0.8725612600027666    steps: 278     evaluation reward: 2.05\n",
      "episode: 618   score: 2.0   memory length: 114568   epsilon: 0.8721533800027754    steps: 206     evaluation reward: 2.05\n",
      "episode: 619   score: 2.0   memory length: 114751   epsilon: 0.8717910400027833    steps: 183     evaluation reward: 2.07\n",
      "episode: 620   score: 6.0   memory length: 115078   epsilon: 0.8711435800027973    steps: 327     evaluation reward: 2.08\n",
      "episode: 621   score: 2.0   memory length: 115270   epsilon: 0.8707634200028056    steps: 192     evaluation reward: 2.09\n",
      "episode: 622   score: 5.0   memory length: 115562   epsilon: 0.8701852600028182    steps: 292     evaluation reward: 2.1\n",
      "episode: 623   score: 1.0   memory length: 115736   epsilon: 0.8698407400028256    steps: 174     evaluation reward: 2.1\n",
      "episode: 624   score: 1.0   memory length: 115919   epsilon: 0.8694784000028335    steps: 183     evaluation reward: 2.09\n",
      "episode: 625   score: 1.0   memory length: 116088   epsilon: 0.8691437800028408    steps: 169     evaluation reward: 2.1\n",
      "episode: 626   score: 4.0   memory length: 116385   epsilon: 0.8685557200028535    steps: 297     evaluation reward: 2.12\n",
      "episode: 627   score: 2.0   memory length: 116597   epsilon: 0.8681359600028626    steps: 212     evaluation reward: 2.14\n",
      "episode: 628   score: 3.0   memory length: 116851   epsilon: 0.8676330400028736    steps: 254     evaluation reward: 2.15\n",
      "episode: 629   score: 5.0   memory length: 117191   epsilon: 0.8669598400028882    steps: 340     evaluation reward: 2.17\n",
      "episode: 630   score: 2.0   memory length: 117397   epsilon: 0.866551960002897    steps: 206     evaluation reward: 2.18\n",
      "episode: 631   score: 0.0   memory length: 117547   epsilon: 0.8662549600029035    steps: 150     evaluation reward: 2.17\n",
      "episode: 632   score: 2.0   memory length: 117749   epsilon: 0.8658550000029122    steps: 202     evaluation reward: 2.16\n",
      "episode: 633   score: 0.0   memory length: 117880   epsilon: 0.8655956200029178    steps: 131     evaluation reward: 2.11\n",
      "episode: 634   score: 2.0   memory length: 118094   epsilon: 0.865171900002927    steps: 214     evaluation reward: 2.09\n",
      "episode: 635   score: 3.0   memory length: 118358   epsilon: 0.8646491800029383    steps: 264     evaluation reward: 2.09\n",
      "episode: 636   score: 2.0   memory length: 118567   epsilon: 0.8642353600029473    steps: 209     evaluation reward: 2.08\n",
      "episode: 637   score: 4.0   memory length: 118858   epsilon: 0.8636591800029598    steps: 291     evaluation reward: 2.1\n",
      "episode: 638   score: 4.0   memory length: 119115   epsilon: 0.8631503200029709    steps: 257     evaluation reward: 2.12\n",
      "episode: 639   score: 0.0   memory length: 119246   epsilon: 0.8628909400029765    steps: 131     evaluation reward: 2.1\n",
      "episode: 640   score: 8.0   memory length: 119717   epsilon: 0.8619583600029967    steps: 471     evaluation reward: 2.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 641   score: 0.0   memory length: 119846   epsilon: 0.8617029400030023    steps: 129     evaluation reward: 2.12\n",
      "episode: 642   score: 0.0   memory length: 119989   epsilon: 0.8614198000030084    steps: 143     evaluation reward: 2.1\n",
      "episode: 643   score: 2.0   memory length: 120197   epsilon: 0.8610079600030174    steps: 208     evaluation reward: 2.1\n",
      "episode: 644   score: 4.0   memory length: 120500   epsilon: 0.8604080200030304    steps: 303     evaluation reward: 2.13\n",
      "episode: 645   score: 3.0   memory length: 120747   epsilon: 0.859918960003041    steps: 247     evaluation reward: 2.15\n",
      "episode: 646   score: 3.0   memory length: 120980   epsilon: 0.859457620003051    steps: 233     evaluation reward: 2.17\n",
      "episode: 647   score: 2.0   memory length: 121165   epsilon: 0.859091320003059    steps: 185     evaluation reward: 2.17\n",
      "episode: 648   score: 1.0   memory length: 121328   epsilon: 0.858768580003066    steps: 163     evaluation reward: 2.17\n",
      "episode: 649   score: 4.0   memory length: 121611   epsilon: 0.8582082400030782    steps: 283     evaluation reward: 2.21\n",
      "episode: 650   score: 1.0   memory length: 121807   epsilon: 0.8578201600030866    steps: 196     evaluation reward: 2.19\n",
      "episode: 651   score: 2.0   memory length: 122016   epsilon: 0.8574063400030956    steps: 209     evaluation reward: 2.18\n",
      "episode: 652   score: 0.0   memory length: 122151   epsilon: 0.8571390400031014    steps: 135     evaluation reward: 2.16\n",
      "episode: 653   score: 4.0   memory length: 122412   epsilon: 0.8566222600031126    steps: 261     evaluation reward: 2.17\n",
      "episode: 654   score: 4.0   memory length: 122670   epsilon: 0.8561114200031237    steps: 258     evaluation reward: 2.19\n",
      "episode: 655   score: 5.0   memory length: 123027   epsilon: 0.855404560003139    steps: 357     evaluation reward: 2.21\n",
      "episode: 656   score: 3.0   memory length: 123266   epsilon: 0.8549313400031493    steps: 239     evaluation reward: 2.22\n",
      "episode: 657   score: 2.0   memory length: 123457   epsilon: 0.8545531600031575    steps: 191     evaluation reward: 2.21\n",
      "episode: 658   score: 6.0   memory length: 123809   epsilon: 0.8538562000031726    steps: 352     evaluation reward: 2.26\n",
      "episode: 659   score: 2.0   memory length: 124007   epsilon: 0.8534641600031811    steps: 198     evaluation reward: 2.24\n",
      "episode: 660   score: 1.0   memory length: 124177   epsilon: 0.8531275600031885    steps: 170     evaluation reward: 2.24\n",
      "episode: 661   score: 3.0   memory length: 124446   epsilon: 0.8525949400032    steps: 269     evaluation reward: 2.25\n",
      "episode: 662   score: 2.0   memory length: 124661   epsilon: 0.8521692400032093    steps: 215     evaluation reward: 2.26\n",
      "episode: 663   score: 3.0   memory length: 124913   epsilon: 0.8516702800032201    steps: 252     evaluation reward: 2.26\n",
      "episode: 664   score: 4.0   memory length: 125216   epsilon: 0.8510703400032331    steps: 303     evaluation reward: 2.27\n",
      "episode: 665   score: 5.0   memory length: 125541   epsilon: 0.8504268400032471    steps: 325     evaluation reward: 2.31\n",
      "episode: 666   score: 3.0   memory length: 125794   epsilon: 0.849925900003258    steps: 253     evaluation reward: 2.3\n",
      "episode: 667   score: 4.0   memory length: 126054   epsilon: 0.8494111000032691    steps: 260     evaluation reward: 2.33\n",
      "episode: 668   score: 1.0   memory length: 126212   epsilon: 0.8490982600032759    steps: 158     evaluation reward: 2.33\n",
      "episode: 669   score: 9.0   memory length: 126662   epsilon: 0.8482072600032953    steps: 450     evaluation reward: 2.41\n",
      "episode: 670   score: 4.0   memory length: 126938   epsilon: 0.8476607800033071    steps: 276     evaluation reward: 2.44\n",
      "episode: 671   score: 2.0   memory length: 127125   epsilon: 0.8472905200033152    steps: 187     evaluation reward: 2.46\n",
      "episode: 672   score: 1.0   memory length: 127276   epsilon: 0.8469915400033217    steps: 151     evaluation reward: 2.45\n",
      "episode: 673   score: 4.0   memory length: 127544   epsilon: 0.8464609000033332    steps: 268     evaluation reward: 2.45\n",
      "episode: 674   score: 1.0   memory length: 127736   epsilon: 0.8460807400033414    steps: 192     evaluation reward: 2.43\n",
      "episode: 675   score: 2.0   memory length: 127934   epsilon: 0.84568870000335    steps: 198     evaluation reward: 2.44\n",
      "episode: 676   score: 5.0   memory length: 128269   epsilon: 0.8450254000033643    steps: 335     evaluation reward: 2.46\n",
      "episode: 677   score: 1.0   memory length: 128462   epsilon: 0.8446432600033726    steps: 193     evaluation reward: 2.46\n",
      "episode: 678   score: 7.0   memory length: 128871   epsilon: 0.8438334400033902    steps: 409     evaluation reward: 2.52\n",
      "episode: 679   score: 1.0   memory length: 129052   epsilon: 0.843475060003398    steps: 181     evaluation reward: 2.53\n",
      "episode: 680   score: 3.0   memory length: 129279   epsilon: 0.8430256000034078    steps: 227     evaluation reward: 2.54\n",
      "episode: 681   score: 5.0   memory length: 129627   epsilon: 0.8423365600034227    steps: 348     evaluation reward: 2.59\n",
      "episode: 682   score: 3.0   memory length: 129890   epsilon: 0.841815820003434    steps: 263     evaluation reward: 2.6\n",
      "episode: 683   score: 3.0   memory length: 130129   epsilon: 0.8413426000034443    steps: 239     evaluation reward: 2.63\n",
      "episode: 684   score: 4.0   memory length: 130395   epsilon: 0.8408159200034557    steps: 266     evaluation reward: 2.63\n",
      "episode: 685   score: 3.0   memory length: 130651   epsilon: 0.8403090400034667    steps: 256     evaluation reward: 2.65\n",
      "episode: 686   score: 4.0   memory length: 130952   epsilon: 0.8397130600034797    steps: 301     evaluation reward: 2.69\n",
      "episode: 687   score: 5.0   memory length: 131257   epsilon: 0.8391091600034928    steps: 305     evaluation reward: 2.7\n",
      "episode: 688   score: 2.0   memory length: 131454   epsilon: 0.8387191000035013    steps: 197     evaluation reward: 2.7\n",
      "episode: 689   score: 2.0   memory length: 131660   epsilon: 0.8383112200035101    steps: 206     evaluation reward: 2.67\n",
      "episode: 690   score: 0.0   memory length: 131800   epsilon: 0.8380340200035161    steps: 140     evaluation reward: 2.64\n",
      "episode: 691   score: 1.0   memory length: 131957   epsilon: 0.8377231600035229    steps: 157     evaluation reward: 2.64\n",
      "episode: 692   score: 1.0   memory length: 132139   epsilon: 0.8373628000035307    steps: 182     evaluation reward: 2.63\n",
      "episode: 693   score: 1.0   memory length: 132302   epsilon: 0.8370400600035377    steps: 163     evaluation reward: 2.61\n",
      "episode: 694   score: 2.0   memory length: 132495   epsilon: 0.836657920003546    steps: 193     evaluation reward: 2.61\n",
      "episode: 695   score: 2.0   memory length: 132696   epsilon: 0.8362599400035546    steps: 201     evaluation reward: 2.6\n",
      "episode: 696   score: 3.0   memory length: 132980   epsilon: 0.8356976200035668    steps: 284     evaluation reward: 2.61\n",
      "episode: 697   score: 2.0   memory length: 133171   epsilon: 0.835319440003575    steps: 191     evaluation reward: 2.61\n",
      "episode: 698   score: 1.0   memory length: 133333   epsilon: 0.834998680003582    steps: 162     evaluation reward: 2.6\n",
      "episode: 699   score: 1.0   memory length: 133509   epsilon: 0.8346502000035896    steps: 176     evaluation reward: 2.61\n",
      "episode: 700   score: 2.0   memory length: 133699   epsilon: 0.8342740000035977    steps: 190     evaluation reward: 2.6\n",
      "episode: 701   score: 1.0   memory length: 133858   epsilon: 0.8339591800036046    steps: 159     evaluation reward: 2.6\n",
      "episode: 702   score: 5.0   memory length: 134197   epsilon: 0.8332879600036192    steps: 339     evaluation reward: 2.64\n",
      "episode: 703   score: 2.0   memory length: 134418   epsilon: 0.8328503800036287    steps: 221     evaluation reward: 2.64\n",
      "episode: 704   score: 6.0   memory length: 134813   epsilon: 0.8320682800036456    steps: 395     evaluation reward: 2.66\n",
      "episode: 705   score: 2.0   memory length: 135001   epsilon: 0.8316960400036537    steps: 188     evaluation reward: 2.67\n",
      "episode: 706   score: 1.0   memory length: 135175   epsilon: 0.8313515200036612    steps: 174     evaluation reward: 2.67\n",
      "episode: 707   score: 5.0   memory length: 135491   epsilon: 0.8307258400036748    steps: 316     evaluation reward: 2.69\n",
      "episode: 708   score: 2.0   memory length: 135691   epsilon: 0.8303298400036834    steps: 200     evaluation reward: 2.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 709   score: 0.0   memory length: 135831   epsilon: 0.8300526400036894    steps: 140     evaluation reward: 2.66\n",
      "episode: 710   score: 3.0   memory length: 136061   epsilon: 0.8295972400036993    steps: 230     evaluation reward: 2.68\n",
      "episode: 711   score: 2.0   memory length: 136262   epsilon: 0.8291992600037079    steps: 201     evaluation reward: 2.69\n",
      "episode: 712   score: 1.0   memory length: 136434   epsilon: 0.8288587000037153    steps: 172     evaluation reward: 2.67\n",
      "episode: 713   score: 3.0   memory length: 136665   epsilon: 0.8284013200037252    steps: 231     evaluation reward: 2.66\n",
      "episode: 714   score: 3.0   memory length: 136897   epsilon: 0.8279419600037352    steps: 232     evaluation reward: 2.65\n",
      "episode: 715   score: 2.0   memory length: 137106   epsilon: 0.8275281400037442    steps: 209     evaluation reward: 2.66\n",
      "episode: 716   score: 1.0   memory length: 137273   epsilon: 0.8271974800037514    steps: 167     evaluation reward: 2.66\n",
      "episode: 717   score: 6.0   memory length: 137641   epsilon: 0.8264688400037672    steps: 368     evaluation reward: 2.67\n",
      "episode: 718   score: 4.0   memory length: 137911   epsilon: 0.8259342400037788    steps: 270     evaluation reward: 2.69\n",
      "episode: 719   score: 7.0   memory length: 138312   epsilon: 0.825140260003796    steps: 401     evaluation reward: 2.74\n",
      "episode: 720   score: 2.0   memory length: 138517   epsilon: 0.8247343600038048    steps: 205     evaluation reward: 2.7\n",
      "episode: 721   score: 1.0   memory length: 138672   epsilon: 0.8244274600038115    steps: 155     evaluation reward: 2.69\n",
      "episode: 722   score: 0.0   memory length: 138818   epsilon: 0.8241383800038178    steps: 146     evaluation reward: 2.64\n",
      "episode: 723   score: 2.0   memory length: 138999   epsilon: 0.8237800000038256    steps: 181     evaluation reward: 2.65\n",
      "episode: 724   score: 2.0   memory length: 139219   epsilon: 0.823344400003835    steps: 220     evaluation reward: 2.66\n",
      "episode: 725   score: 2.0   memory length: 139420   epsilon: 0.8229464200038437    steps: 201     evaluation reward: 2.67\n",
      "episode: 726   score: 5.0   memory length: 139709   epsilon: 0.8223742000038561    steps: 289     evaluation reward: 2.68\n",
      "episode: 727   score: 3.0   memory length: 139937   epsilon: 0.8219227600038659    steps: 228     evaluation reward: 2.69\n",
      "episode: 728   score: 4.0   memory length: 140250   epsilon: 0.8213030200038793    steps: 313     evaluation reward: 2.7\n",
      "episode: 729   score: 3.0   memory length: 140466   epsilon: 0.8208753400038886    steps: 216     evaluation reward: 2.68\n",
      "episode: 730   score: 1.0   memory length: 140654   epsilon: 0.8205031000038967    steps: 188     evaluation reward: 2.67\n",
      "episode: 731   score: 7.0   memory length: 141062   epsilon: 0.8196952600039142    steps: 408     evaluation reward: 2.74\n",
      "episode: 732   score: 1.0   memory length: 141218   epsilon: 0.8193863800039209    steps: 156     evaluation reward: 2.73\n",
      "episode: 733   score: 4.0   memory length: 141526   epsilon: 0.8187765400039342    steps: 308     evaluation reward: 2.77\n",
      "episode: 734   score: 5.0   memory length: 141819   epsilon: 0.8181964000039468    steps: 293     evaluation reward: 2.8\n",
      "episode: 735   score: 5.0   memory length: 142137   epsilon: 0.8175667600039604    steps: 318     evaluation reward: 2.82\n",
      "episode: 736   score: 1.0   memory length: 142297   epsilon: 0.8172499600039673    steps: 160     evaluation reward: 2.81\n",
      "episode: 737   score: 1.0   memory length: 142452   epsilon: 0.816943060003974    steps: 155     evaluation reward: 2.78\n",
      "episode: 738   score: 4.0   memory length: 142721   epsilon: 0.8164104400039855    steps: 269     evaluation reward: 2.78\n",
      "episode: 739   score: 1.0   memory length: 142894   epsilon: 0.816067900003993    steps: 173     evaluation reward: 2.79\n",
      "episode: 740   score: 0.0   memory length: 143022   epsilon: 0.8158144600039985    steps: 128     evaluation reward: 2.71\n",
      "episode: 741   score: 5.0   memory length: 143348   epsilon: 0.8151689800040125    steps: 326     evaluation reward: 2.76\n",
      "episode: 742   score: 8.0   memory length: 143788   epsilon: 0.8142977800040314    steps: 440     evaluation reward: 2.84\n",
      "episode: 743   score: 2.0   memory length: 143985   epsilon: 0.8139077200040399    steps: 197     evaluation reward: 2.84\n",
      "episode: 744   score: 2.0   memory length: 144204   epsilon: 0.8134741000040493    steps: 219     evaluation reward: 2.82\n",
      "episode: 745   score: 5.0   memory length: 144551   epsilon: 0.8127870400040642    steps: 347     evaluation reward: 2.84\n",
      "episode: 746   score: 4.0   memory length: 144871   epsilon: 0.812153440004078    steps: 320     evaluation reward: 2.85\n",
      "episode: 747   score: 2.0   memory length: 145082   epsilon: 0.811735660004087    steps: 211     evaluation reward: 2.85\n",
      "episode: 748   score: 5.0   memory length: 145423   epsilon: 0.8110604800041017    steps: 341     evaluation reward: 2.89\n",
      "episode: 749   score: 7.0   memory length: 145851   epsilon: 0.8102130400041201    steps: 428     evaluation reward: 2.92\n",
      "episode: 750   score: 0.0   memory length: 145988   epsilon: 0.809941780004126    steps: 137     evaluation reward: 2.91\n",
      "episode: 751   score: 4.0   memory length: 146232   epsilon: 0.8094586600041365    steps: 244     evaluation reward: 2.93\n",
      "episode: 752   score: 3.0   memory length: 146484   epsilon: 0.8089597000041473    steps: 252     evaluation reward: 2.96\n",
      "episode: 753   score: 1.0   memory length: 146648   epsilon: 0.8086349800041543    steps: 164     evaluation reward: 2.93\n",
      "episode: 754   score: 2.0   memory length: 146838   epsilon: 0.8082587800041625    steps: 190     evaluation reward: 2.91\n",
      "episode: 755   score: 4.0   memory length: 147139   epsilon: 0.8076628000041755    steps: 301     evaluation reward: 2.9\n",
      "episode: 756   score: 3.0   memory length: 147370   epsilon: 0.8072054200041854    steps: 231     evaluation reward: 2.9\n",
      "episode: 757   score: 1.0   memory length: 147529   epsilon: 0.8068906000041922    steps: 159     evaluation reward: 2.89\n",
      "episode: 758   score: 6.0   memory length: 147884   epsilon: 0.8061877000042075    steps: 355     evaluation reward: 2.89\n",
      "episode: 759   score: 3.0   memory length: 148126   epsilon: 0.8057085400042179    steps: 242     evaluation reward: 2.9\n",
      "episode: 760   score: 4.0   memory length: 148409   epsilon: 0.80514820000423    steps: 283     evaluation reward: 2.93\n",
      "episode: 761   score: 2.0   memory length: 148615   epsilon: 0.8047403200042389    steps: 206     evaluation reward: 2.92\n",
      "episode: 762   score: 2.0   memory length: 148808   epsilon: 0.8043581800042472    steps: 193     evaluation reward: 2.92\n",
      "episode: 763   score: 0.0   memory length: 148935   epsilon: 0.8041067200042527    steps: 127     evaluation reward: 2.89\n",
      "episode: 764   score: 2.0   memory length: 149147   epsilon: 0.8036869600042618    steps: 212     evaluation reward: 2.87\n",
      "episode: 765   score: 2.0   memory length: 149336   epsilon: 0.8033127400042699    steps: 189     evaluation reward: 2.84\n",
      "episode: 766   score: 4.0   memory length: 149615   epsilon: 0.8027603200042819    steps: 279     evaluation reward: 2.85\n",
      "episode: 767   score: 0.0   memory length: 149765   epsilon: 0.8024633200042883    steps: 150     evaluation reward: 2.81\n",
      "episode: 768   score: 1.0   memory length: 149956   epsilon: 0.8020851400042965    steps: 191     evaluation reward: 2.81\n",
      "now time :  2019-09-25 16:36:07.565810\n",
      "episode: 769   score: 5.0   memory length: 150255   epsilon: 0.8014931200043094    steps: 299     evaluation reward: 2.77\n",
      "episode: 770   score: 2.0   memory length: 150456   epsilon: 0.801095140004318    steps: 201     evaluation reward: 2.75\n",
      "episode: 771   score: 1.0   memory length: 150611   epsilon: 0.8007882400043247    steps: 155     evaluation reward: 2.74\n",
      "episode: 772   score: 4.0   memory length: 150862   epsilon: 0.8002912600043355    steps: 251     evaluation reward: 2.77\n",
      "episode: 773   score: 7.0   memory length: 151278   epsilon: 0.7994675800043534    steps: 416     evaluation reward: 2.8\n",
      "episode: 774   score: 4.0   memory length: 151557   epsilon: 0.7989151600043654    steps: 279     evaluation reward: 2.83\n",
      "episode: 775   score: 2.0   memory length: 151768   epsilon: 0.7984973800043744    steps: 211     evaluation reward: 2.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 776   score: 6.0   memory length: 152155   epsilon: 0.7977311200043911    steps: 387     evaluation reward: 2.84\n",
      "episode: 777   score: 3.0   memory length: 152388   epsilon: 0.7972697800044011    steps: 233     evaluation reward: 2.86\n",
      "episode: 778   score: 5.0   memory length: 152716   epsilon: 0.7966203400044152    steps: 328     evaluation reward: 2.84\n",
      "episode: 779   score: 2.0   memory length: 152912   epsilon: 0.7962322600044236    steps: 196     evaluation reward: 2.85\n",
      "episode: 780   score: 5.0   memory length: 153222   epsilon: 0.7956184600044369    steps: 310     evaluation reward: 2.87\n",
      "episode: 781   score: 2.0   memory length: 153415   epsilon: 0.7952363200044452    steps: 193     evaluation reward: 2.84\n",
      "episode: 782   score: 4.0   memory length: 153709   epsilon: 0.7946542000044579    steps: 294     evaluation reward: 2.85\n",
      "episode: 783   score: 6.0   memory length: 154038   epsilon: 0.794002780004472    steps: 329     evaluation reward: 2.88\n",
      "episode: 784   score: 3.0   memory length: 154277   epsilon: 0.7935295600044823    steps: 239     evaluation reward: 2.87\n",
      "episode: 785   score: 4.0   memory length: 154567   epsilon: 0.7929553600044947    steps: 290     evaluation reward: 2.88\n",
      "episode: 786   score: 5.0   memory length: 154881   epsilon: 0.7923336400045082    steps: 314     evaluation reward: 2.89\n",
      "episode: 787   score: 5.0   memory length: 155213   epsilon: 0.7916762800045225    steps: 332     evaluation reward: 2.89\n",
      "episode: 788   score: 2.0   memory length: 155472   epsilon: 0.7911634600045336    steps: 259     evaluation reward: 2.89\n",
      "episode: 789   score: 1.0   memory length: 155625   epsilon: 0.7908605200045402    steps: 153     evaluation reward: 2.88\n",
      "episode: 790   score: 2.0   memory length: 155839   epsilon: 0.7904368000045494    steps: 214     evaluation reward: 2.9\n",
      "episode: 791   score: 3.0   memory length: 156057   epsilon: 0.7900051600045588    steps: 218     evaluation reward: 2.92\n",
      "episode: 792   score: 1.0   memory length: 156211   epsilon: 0.7897002400045654    steps: 154     evaluation reward: 2.92\n",
      "episode: 793   score: 2.0   memory length: 156416   epsilon: 0.7892943400045742    steps: 205     evaluation reward: 2.93\n",
      "episode: 794   score: 5.0   memory length: 156712   epsilon: 0.7887082600045869    steps: 296     evaluation reward: 2.96\n",
      "episode: 795   score: 5.0   memory length: 157058   epsilon: 0.7880231800046018    steps: 346     evaluation reward: 2.99\n",
      "episode: 796   score: 3.0   memory length: 157316   epsilon: 0.7875123400046129    steps: 258     evaluation reward: 2.99\n",
      "episode: 797   score: 7.0   memory length: 157685   epsilon: 0.7867817200046288    steps: 369     evaluation reward: 3.04\n",
      "episode: 798   score: 0.0   memory length: 157816   epsilon: 0.7865223400046344    steps: 131     evaluation reward: 3.03\n",
      "episode: 799   score: 3.0   memory length: 158060   epsilon: 0.7860392200046449    steps: 244     evaluation reward: 3.05\n",
      "episode: 800   score: 7.0   memory length: 158408   epsilon: 0.7853501800046598    steps: 348     evaluation reward: 3.1\n",
      "episode: 801   score: 6.0   memory length: 158772   epsilon: 0.7846294600046755    steps: 364     evaluation reward: 3.15\n",
      "episode: 802   score: 0.0   memory length: 158902   epsilon: 0.7843720600046811    steps: 130     evaluation reward: 3.1\n",
      "episode: 803   score: 2.0   memory length: 159089   epsilon: 0.7840018000046891    steps: 187     evaluation reward: 3.1\n",
      "episode: 804   score: 3.0   memory length: 159334   epsilon: 0.7835167000046996    steps: 245     evaluation reward: 3.07\n",
      "episode: 805   score: 5.0   memory length: 159705   epsilon: 0.7827821200047156    steps: 371     evaluation reward: 3.1\n",
      "episode: 806   score: 2.0   memory length: 159936   epsilon: 0.7823247400047255    steps: 231     evaluation reward: 3.11\n",
      "episode: 807   score: 2.0   memory length: 160150   epsilon: 0.7819010200047347    steps: 214     evaluation reward: 3.08\n",
      "episode: 808   score: 3.0   memory length: 160405   epsilon: 0.7813961200047457    steps: 255     evaluation reward: 3.09\n",
      "episode: 809   score: 1.0   memory length: 160584   epsilon: 0.7810417000047534    steps: 179     evaluation reward: 3.1\n",
      "episode: 810   score: 8.0   memory length: 161067   epsilon: 0.7800853600047741    steps: 483     evaluation reward: 3.15\n",
      "episode: 811   score: 3.0   memory length: 161313   epsilon: 0.7795982800047847    steps: 246     evaluation reward: 3.16\n",
      "episode: 812   score: 4.0   memory length: 161599   epsilon: 0.779032000004797    steps: 286     evaluation reward: 3.19\n",
      "episode: 813   score: 4.0   memory length: 161873   epsilon: 0.7784894800048088    steps: 274     evaluation reward: 3.2\n",
      "episode: 814   score: 5.0   memory length: 162223   epsilon: 0.7777964800048238    steps: 350     evaluation reward: 3.22\n",
      "episode: 815   score: 4.0   memory length: 162566   epsilon: 0.7771173400048386    steps: 343     evaluation reward: 3.24\n",
      "episode: 816   score: 5.0   memory length: 162913   epsilon: 0.7764302800048535    steps: 347     evaluation reward: 3.28\n",
      "episode: 817   score: 4.0   memory length: 163243   epsilon: 0.7757768800048677    steps: 330     evaluation reward: 3.26\n",
      "episode: 818   score: 4.0   memory length: 163521   epsilon: 0.7752264400048796    steps: 278     evaluation reward: 3.26\n",
      "episode: 819   score: 2.0   memory length: 163727   epsilon: 0.7748185600048885    steps: 206     evaluation reward: 3.21\n",
      "episode: 820   score: 3.0   memory length: 163942   epsilon: 0.7743928600048977    steps: 215     evaluation reward: 3.22\n",
      "episode: 821   score: 3.0   memory length: 164185   epsilon: 0.7739117200049082    steps: 243     evaluation reward: 3.24\n",
      "episode: 822   score: 2.0   memory length: 164410   epsilon: 0.7734662200049178    steps: 225     evaluation reward: 3.26\n",
      "episode: 823   score: 2.0   memory length: 164619   epsilon: 0.7730524000049268    steps: 209     evaluation reward: 3.26\n",
      "episode: 824   score: 1.0   memory length: 164784   epsilon: 0.7727257000049339    steps: 165     evaluation reward: 3.25\n",
      "episode: 825   score: 3.0   memory length: 165037   epsilon: 0.7722247600049448    steps: 253     evaluation reward: 3.26\n",
      "episode: 826   score: 5.0   memory length: 165373   epsilon: 0.7715594800049592    steps: 336     evaluation reward: 3.26\n",
      "episode: 827   score: 4.0   memory length: 165649   epsilon: 0.7710130000049711    steps: 276     evaluation reward: 3.27\n",
      "episode: 828   score: 3.0   memory length: 165898   epsilon: 0.7705199800049818    steps: 249     evaluation reward: 3.26\n",
      "episode: 829   score: 2.0   memory length: 166127   epsilon: 0.7700665600049916    steps: 229     evaluation reward: 3.25\n",
      "episode: 830   score: 13.0   memory length: 166595   epsilon: 0.7691399200050117    steps: 468     evaluation reward: 3.37\n",
      "episode: 831   score: 5.0   memory length: 166905   epsilon: 0.7685261200050251    steps: 310     evaluation reward: 3.35\n",
      "episode: 832   score: 3.0   memory length: 167127   epsilon: 0.7680865600050346    steps: 222     evaluation reward: 3.37\n",
      "episode: 833   score: 4.0   memory length: 167397   epsilon: 0.7675519600050462    steps: 270     evaluation reward: 3.37\n",
      "episode: 834   score: 4.0   memory length: 167662   epsilon: 0.7670272600050576    steps: 265     evaluation reward: 3.36\n",
      "episode: 835   score: 6.0   memory length: 168036   epsilon: 0.7662867400050737    steps: 374     evaluation reward: 3.37\n",
      "episode: 836   score: 3.0   memory length: 168248   epsilon: 0.7658669800050828    steps: 212     evaluation reward: 3.39\n",
      "episode: 837   score: 4.0   memory length: 168498   epsilon: 0.7653719800050935    steps: 250     evaluation reward: 3.42\n",
      "episode: 838   score: 2.0   memory length: 168703   epsilon: 0.7649660800051024    steps: 205     evaluation reward: 3.4\n",
      "episode: 839   score: 1.0   memory length: 168865   epsilon: 0.7646453200051093    steps: 162     evaluation reward: 3.4\n",
      "episode: 840   score: 6.0   memory length: 169195   epsilon: 0.7639919200051235    steps: 330     evaluation reward: 3.46\n",
      "episode: 841   score: 1.0   memory length: 169348   epsilon: 0.7636889800051301    steps: 153     evaluation reward: 3.42\n",
      "episode: 842   score: 4.0   memory length: 169638   epsilon: 0.7631147800051425    steps: 290     evaluation reward: 3.38\n",
      "episode: 843   score: 4.0   memory length: 169903   epsilon: 0.7625900800051539    steps: 265     evaluation reward: 3.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 844   score: 5.0   memory length: 170245   epsilon: 0.7619129200051686    steps: 342     evaluation reward: 3.43\n",
      "episode: 845   score: 6.0   memory length: 170646   epsilon: 0.7611189400051859    steps: 401     evaluation reward: 3.44\n",
      "episode: 846   score: 6.0   memory length: 170972   epsilon: 0.7604734600051999    steps: 326     evaluation reward: 3.46\n",
      "episode: 847   score: 3.0   memory length: 171257   epsilon: 0.7599091600052121    steps: 285     evaluation reward: 3.47\n",
      "episode: 848   score: 4.0   memory length: 171552   epsilon: 0.7593250600052248    steps: 295     evaluation reward: 3.46\n",
      "episode: 849   score: 2.0   memory length: 171756   epsilon: 0.7589211400052336    steps: 204     evaluation reward: 3.41\n",
      "episode: 850   score: 6.0   memory length: 172102   epsilon: 0.7582360600052485    steps: 346     evaluation reward: 3.47\n",
      "episode: 851   score: 0.0   memory length: 172233   epsilon: 0.7579766800052541    steps: 131     evaluation reward: 3.43\n",
      "episode: 852   score: 1.0   memory length: 172393   epsilon: 0.757659880005261    steps: 160     evaluation reward: 3.41\n",
      "episode: 853   score: 4.0   memory length: 172694   epsilon: 0.7570639000052739    steps: 301     evaluation reward: 3.44\n",
      "episode: 854   score: 5.0   memory length: 173018   epsilon: 0.7564223800052878    steps: 324     evaluation reward: 3.47\n",
      "episode: 855   score: 0.0   memory length: 173158   epsilon: 0.7561451800052938    steps: 140     evaluation reward: 3.43\n",
      "episode: 856   score: 3.0   memory length: 173421   epsilon: 0.7556244400053052    steps: 263     evaluation reward: 3.43\n",
      "episode: 857   score: 4.0   memory length: 173700   epsilon: 0.7550720200053171    steps: 279     evaluation reward: 3.46\n",
      "episode: 858   score: 4.0   memory length: 174001   epsilon: 0.7544760400053301    steps: 301     evaluation reward: 3.44\n",
      "episode: 859   score: 2.0   memory length: 174193   epsilon: 0.7540958800053383    steps: 192     evaluation reward: 3.43\n",
      "episode: 860   score: 0.0   memory length: 174317   epsilon: 0.7538503600053437    steps: 124     evaluation reward: 3.39\n",
      "episode: 861   score: 1.0   memory length: 174474   epsilon: 0.7535395000053504    steps: 157     evaluation reward: 3.38\n",
      "episode: 862   score: 1.0   memory length: 174646   epsilon: 0.7531989400053578    steps: 172     evaluation reward: 3.37\n",
      "episode: 863   score: 0.0   memory length: 174774   epsilon: 0.7529455000053633    steps: 128     evaluation reward: 3.37\n",
      "episode: 864   score: 0.0   memory length: 174897   epsilon: 0.7527019600053686    steps: 123     evaluation reward: 3.35\n",
      "episode: 865   score: 0.0   memory length: 175024   epsilon: 0.7524505000053741    steps: 127     evaluation reward: 3.33\n",
      "episode: 866   score: 1.0   memory length: 175190   epsilon: 0.7521218200053812    steps: 166     evaluation reward: 3.3\n",
      "episode: 867   score: 1.0   memory length: 175362   epsilon: 0.7517812600053886    steps: 172     evaluation reward: 3.31\n",
      "episode: 868   score: 1.0   memory length: 175514   epsilon: 0.7514803000053951    steps: 152     evaluation reward: 3.31\n",
      "episode: 869   score: 1.0   memory length: 175675   epsilon: 0.751161520005402    steps: 161     evaluation reward: 3.27\n",
      "episode: 870   score: 0.0   memory length: 175798   epsilon: 0.7509179800054073    steps: 123     evaluation reward: 3.25\n",
      "episode: 871   score: 4.0   memory length: 176078   epsilon: 0.7503635800054194    steps: 280     evaluation reward: 3.28\n",
      "episode: 872   score: 4.0   memory length: 176377   epsilon: 0.7497715600054322    steps: 299     evaluation reward: 3.28\n",
      "episode: 873   score: 5.0   memory length: 176724   epsilon: 0.7490845000054471    steps: 347     evaluation reward: 3.26\n",
      "episode: 874   score: 3.0   memory length: 176991   epsilon: 0.7485558400054586    steps: 267     evaluation reward: 3.25\n",
      "episode: 875   score: 1.0   memory length: 177161   epsilon: 0.7482192400054659    steps: 170     evaluation reward: 3.24\n",
      "episode: 876   score: 0.0   memory length: 177292   epsilon: 0.7479598600054715    steps: 131     evaluation reward: 3.18\n",
      "episode: 877   score: 2.0   memory length: 177476   epsilon: 0.7475955400054795    steps: 184     evaluation reward: 3.17\n",
      "episode: 878   score: 3.0   memory length: 177729   epsilon: 0.7470946000054903    steps: 253     evaluation reward: 3.15\n",
      "episode: 879   score: 2.0   memory length: 177928   epsilon: 0.7467005800054989    steps: 199     evaluation reward: 3.15\n",
      "episode: 880   score: 1.0   memory length: 178084   epsilon: 0.7463917000055056    steps: 156     evaluation reward: 3.11\n",
      "episode: 881   score: 1.0   memory length: 178236   epsilon: 0.7460907400055121    steps: 152     evaluation reward: 3.1\n",
      "episode: 882   score: 2.0   memory length: 178434   epsilon: 0.7456987000055206    steps: 198     evaluation reward: 3.08\n",
      "episode: 883   score: 1.0   memory length: 178614   epsilon: 0.7453423000055284    steps: 180     evaluation reward: 3.03\n",
      "episode: 884   score: 4.0   memory length: 178910   epsilon: 0.7447562200055411    steps: 296     evaluation reward: 3.04\n",
      "episode: 885   score: 0.0   memory length: 179039   epsilon: 0.7445008000055466    steps: 129     evaluation reward: 3.0\n",
      "episode: 886   score: 0.0   memory length: 179165   epsilon: 0.744251320005552    steps: 126     evaluation reward: 2.95\n",
      "episode: 887   score: 1.0   memory length: 179319   epsilon: 0.7439464000055587    steps: 154     evaluation reward: 2.91\n",
      "episode: 888   score: 1.0   memory length: 179492   epsilon: 0.7436038600055661    steps: 173     evaluation reward: 2.9\n",
      "episode: 889   score: 1.0   memory length: 179648   epsilon: 0.7432949800055728    steps: 156     evaluation reward: 2.9\n",
      "episode: 890   score: 2.0   memory length: 179846   epsilon: 0.7429029400055813    steps: 198     evaluation reward: 2.9\n",
      "episode: 891   score: 2.0   memory length: 180055   epsilon: 0.7424891200055903    steps: 209     evaluation reward: 2.89\n",
      "episode: 892   score: 1.0   memory length: 180231   epsilon: 0.7421406400055979    steps: 176     evaluation reward: 2.89\n",
      "episode: 893   score: 1.0   memory length: 180382   epsilon: 0.7418416600056044    steps: 151     evaluation reward: 2.88\n",
      "episode: 894   score: 0.0   memory length: 180511   epsilon: 0.7415862400056099    steps: 129     evaluation reward: 2.83\n",
      "episode: 895   score: 1.0   memory length: 180679   epsilon: 0.7412536000056171    steps: 168     evaluation reward: 2.79\n",
      "episode: 896   score: 1.0   memory length: 180829   epsilon: 0.7409566000056236    steps: 150     evaluation reward: 2.77\n",
      "episode: 897   score: 1.0   memory length: 180991   epsilon: 0.7406358400056305    steps: 162     evaluation reward: 2.71\n",
      "episode: 898   score: 0.0   memory length: 181115   epsilon: 0.7403903200056359    steps: 124     evaluation reward: 2.71\n",
      "episode: 899   score: 3.0   memory length: 181363   epsilon: 0.7398992800056465    steps: 248     evaluation reward: 2.71\n",
      "episode: 900   score: 0.0   memory length: 181492   epsilon: 0.7396438600056521    steps: 129     evaluation reward: 2.64\n",
      "episode: 901   score: 0.0   memory length: 181616   epsilon: 0.7393983400056574    steps: 124     evaluation reward: 2.58\n",
      "episode: 902   score: 1.0   memory length: 181785   epsilon: 0.7390637200056647    steps: 169     evaluation reward: 2.59\n",
      "episode: 903   score: 2.0   memory length: 182004   epsilon: 0.7386301000056741    steps: 219     evaluation reward: 2.59\n",
      "episode: 904   score: 0.0   memory length: 182130   epsilon: 0.7383806200056795    steps: 126     evaluation reward: 2.56\n",
      "episode: 905   score: 2.0   memory length: 182331   epsilon: 0.7379826400056881    steps: 201     evaluation reward: 2.53\n",
      "episode: 906   score: 2.0   memory length: 182529   epsilon: 0.7375906000056967    steps: 198     evaluation reward: 2.53\n",
      "episode: 907   score: 1.0   memory length: 182699   epsilon: 0.737254000005704    steps: 170     evaluation reward: 2.52\n",
      "episode: 908   score: 0.0   memory length: 182823   epsilon: 0.7370084800057093    steps: 124     evaluation reward: 2.49\n",
      "episode: 909   score: 0.0   memory length: 182949   epsilon: 0.7367590000057147    steps: 126     evaluation reward: 2.48\n",
      "episode: 910   score: 2.0   memory length: 183160   epsilon: 0.7363412200057238    steps: 211     evaluation reward: 2.42\n",
      "episode: 911   score: 3.0   memory length: 183406   epsilon: 0.7358541400057343    steps: 246     evaluation reward: 2.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 912   score: 0.0   memory length: 183530   epsilon: 0.7356086200057397    steps: 124     evaluation reward: 2.38\n",
      "episode: 913   score: 0.0   memory length: 183655   epsilon: 0.735361120005745    steps: 125     evaluation reward: 2.34\n",
      "episode: 914   score: 3.0   memory length: 183873   epsilon: 0.7349294800057544    steps: 218     evaluation reward: 2.32\n",
      "episode: 915   score: 0.0   memory length: 183997   epsilon: 0.7346839600057598    steps: 124     evaluation reward: 2.28\n",
      "episode: 916   score: 0.0   memory length: 184126   epsilon: 0.7344285400057653    steps: 129     evaluation reward: 2.23\n",
      "episode: 917   score: 3.0   memory length: 184382   epsilon: 0.7339216600057763    steps: 256     evaluation reward: 2.22\n",
      "episode: 918   score: 3.0   memory length: 184631   epsilon: 0.733428640005787    steps: 249     evaluation reward: 2.21\n",
      "episode: 919   score: 0.0   memory length: 184755   epsilon: 0.7331831200057923    steps: 124     evaluation reward: 2.19\n",
      "episode: 920   score: 1.0   memory length: 184907   epsilon: 0.7328821600057989    steps: 152     evaluation reward: 2.17\n",
      "episode: 921   score: 0.0   memory length: 185035   epsilon: 0.7326287200058044    steps: 128     evaluation reward: 2.14\n",
      "episode: 922   score: 3.0   memory length: 185262   epsilon: 0.7321792600058141    steps: 227     evaluation reward: 2.15\n",
      "episode: 923   score: 0.0   memory length: 185384   epsilon: 0.7319377000058194    steps: 122     evaluation reward: 2.13\n",
      "episode: 924   score: 2.0   memory length: 185573   epsilon: 0.7315634800058275    steps: 189     evaluation reward: 2.14\n",
      "episode: 925   score: 4.0   memory length: 185873   epsilon: 0.7309694800058404    steps: 300     evaluation reward: 2.15\n",
      "episode: 926   score: 1.0   memory length: 186025   epsilon: 0.7306685200058469    steps: 152     evaluation reward: 2.11\n",
      "episode: 927   score: 2.0   memory length: 186243   epsilon: 0.7302368800058563    steps: 218     evaluation reward: 2.09\n",
      "episode: 928   score: 1.0   memory length: 186416   epsilon: 0.7298943400058637    steps: 173     evaluation reward: 2.07\n",
      "episode: 929   score: 0.0   memory length: 186543   epsilon: 0.7296428800058692    steps: 127     evaluation reward: 2.05\n",
      "episode: 930   score: 0.0   memory length: 186668   epsilon: 0.7293953800058746    steps: 125     evaluation reward: 1.92\n",
      "episode: 931   score: 1.0   memory length: 186821   epsilon: 0.7290924400058811    steps: 153     evaluation reward: 1.88\n",
      "episode: 932   score: 1.0   memory length: 186977   epsilon: 0.7287835600058878    steps: 156     evaluation reward: 1.86\n",
      "episode: 933   score: 2.0   memory length: 187197   epsilon: 0.7283479600058973    steps: 220     evaluation reward: 1.84\n",
      "episode: 934   score: 1.0   memory length: 187372   epsilon: 0.7280014600059048    steps: 175     evaluation reward: 1.81\n",
      "episode: 935   score: 2.0   memory length: 187560   epsilon: 0.7276292200059129    steps: 188     evaluation reward: 1.77\n",
      "episode: 936   score: 6.0   memory length: 187935   epsilon: 0.726886720005929    steps: 375     evaluation reward: 1.8\n",
      "episode: 937   score: 2.0   memory length: 188135   epsilon: 0.7264907200059376    steps: 200     evaluation reward: 1.78\n",
      "episode: 938   score: 4.0   memory length: 188399   epsilon: 0.725968000005949    steps: 264     evaluation reward: 1.8\n",
      "episode: 939   score: 0.0   memory length: 188525   epsilon: 0.7257185200059544    steps: 126     evaluation reward: 1.79\n",
      "episode: 940   score: 3.0   memory length: 188759   epsilon: 0.7252552000059644    steps: 234     evaluation reward: 1.76\n",
      "episode: 941   score: 1.0   memory length: 188917   epsilon: 0.7249423600059712    steps: 158     evaluation reward: 1.76\n",
      "episode: 942   score: 0.0   memory length: 189045   epsilon: 0.7246889200059767    steps: 128     evaluation reward: 1.72\n",
      "episode: 943   score: 1.0   memory length: 189214   epsilon: 0.724354300005984    steps: 169     evaluation reward: 1.69\n",
      "episode: 944   score: 0.0   memory length: 189343   epsilon: 0.7240988800059895    steps: 129     evaluation reward: 1.64\n",
      "episode: 945   score: 1.0   memory length: 189516   epsilon: 0.723756340005997    steps: 173     evaluation reward: 1.59\n",
      "episode: 946   score: 1.0   memory length: 189667   epsilon: 0.7234573600060035    steps: 151     evaluation reward: 1.54\n",
      "episode: 947   score: 3.0   memory length: 189903   epsilon: 0.7229900800060136    steps: 236     evaluation reward: 1.54\n",
      "episode: 948   score: 0.0   memory length: 190027   epsilon: 0.7227445600060189    steps: 124     evaluation reward: 1.5\n",
      "episode: 949   score: 2.0   memory length: 190230   epsilon: 0.7223426200060277    steps: 203     evaluation reward: 1.5\n",
      "episode: 950   score: 1.0   memory length: 190403   epsilon: 0.7220000800060351    steps: 173     evaluation reward: 1.45\n",
      "episode: 951   score: 0.0   memory length: 190525   epsilon: 0.7217585200060403    steps: 122     evaluation reward: 1.45\n",
      "episode: 952   score: 2.0   memory length: 190709   epsilon: 0.7213942000060483    steps: 184     evaluation reward: 1.46\n",
      "episode: 953   score: 0.0   memory length: 190834   epsilon: 0.7211467000060536    steps: 125     evaluation reward: 1.42\n",
      "episode: 954   score: 3.0   memory length: 191082   epsilon: 0.7206556600060643    steps: 248     evaluation reward: 1.4\n",
      "episode: 955   score: 0.0   memory length: 191209   epsilon: 0.7204042000060698    steps: 127     evaluation reward: 1.4\n",
      "episode: 956   score: 1.0   memory length: 191380   epsilon: 0.7200656200060771    steps: 171     evaluation reward: 1.38\n",
      "episode: 957   score: 3.0   memory length: 191632   epsilon: 0.7195666600060879    steps: 252     evaluation reward: 1.37\n",
      "episode: 958   score: 3.0   memory length: 191861   epsilon: 0.7191132400060978    steps: 229     evaluation reward: 1.36\n",
      "episode: 959   score: 2.0   memory length: 192082   epsilon: 0.7186756600061073    steps: 221     evaluation reward: 1.36\n",
      "episode: 960   score: 0.0   memory length: 192211   epsilon: 0.7184202400061128    steps: 129     evaluation reward: 1.36\n",
      "episode: 961   score: 1.0   memory length: 192364   epsilon: 0.7181173000061194    steps: 153     evaluation reward: 1.36\n",
      "episode: 962   score: 0.0   memory length: 192488   epsilon: 0.7178717800061247    steps: 124     evaluation reward: 1.35\n",
      "episode: 963   score: 0.0   memory length: 192611   epsilon: 0.71762824000613    steps: 123     evaluation reward: 1.35\n",
      "episode: 964   score: 1.0   memory length: 192761   epsilon: 0.7173312400061365    steps: 150     evaluation reward: 1.36\n",
      "episode: 965   score: 0.0   memory length: 192893   epsilon: 0.7170698800061421    steps: 132     evaluation reward: 1.36\n",
      "episode: 966   score: 6.0   memory length: 193271   epsilon: 0.7163214400061584    steps: 378     evaluation reward: 1.41\n",
      "episode: 967   score: 1.0   memory length: 193425   epsilon: 0.716016520006165    steps: 154     evaluation reward: 1.41\n",
      "episode: 968   score: 1.0   memory length: 193597   epsilon: 0.7156759600061724    steps: 172     evaluation reward: 1.41\n",
      "episode: 969   score: 2.0   memory length: 193820   epsilon: 0.715234420006182    steps: 223     evaluation reward: 1.42\n",
      "episode: 970   score: 0.0   memory length: 193943   epsilon: 0.7149908800061873    steps: 123     evaluation reward: 1.42\n",
      "episode: 971   score: 1.0   memory length: 194102   epsilon: 0.7146760600061941    steps: 159     evaluation reward: 1.39\n",
      "episode: 972   score: 3.0   memory length: 194337   epsilon: 0.7142107600062042    steps: 235     evaluation reward: 1.38\n",
      "episode: 973   score: 0.0   memory length: 194465   epsilon: 0.7139573200062097    steps: 128     evaluation reward: 1.33\n",
      "episode: 974   score: 2.0   memory length: 194684   epsilon: 0.7135237000062191    steps: 219     evaluation reward: 1.32\n",
      "episode: 975   score: 1.0   memory length: 194865   epsilon: 0.7131653200062269    steps: 181     evaluation reward: 1.32\n",
      "episode: 976   score: 0.0   memory length: 194994   epsilon: 0.7129099000062324    steps: 129     evaluation reward: 1.32\n",
      "episode: 977   score: 2.0   memory length: 195194   epsilon: 0.712513900006241    steps: 200     evaluation reward: 1.32\n",
      "episode: 978   score: 2.0   memory length: 195396   epsilon: 0.7121139400062497    steps: 202     evaluation reward: 1.31\n",
      "episode: 979   score: 3.0   memory length: 195648   epsilon: 0.7116149800062606    steps: 252     evaluation reward: 1.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 980   score: 2.0   memory length: 195847   epsilon: 0.7112209600062691    steps: 199     evaluation reward: 1.33\n",
      "episode: 981   score: 0.0   memory length: 195973   epsilon: 0.7109714800062745    steps: 126     evaluation reward: 1.32\n",
      "episode: 982   score: 2.0   memory length: 196171   epsilon: 0.710579440006283    steps: 198     evaluation reward: 1.32\n",
      "episode: 983   score: 2.0   memory length: 196373   epsilon: 0.7101794800062917    steps: 202     evaluation reward: 1.33\n",
      "episode: 984   score: 3.0   memory length: 196584   epsilon: 0.7097617000063008    steps: 211     evaluation reward: 1.32\n",
      "episode: 985   score: 1.0   memory length: 196737   epsilon: 0.7094587600063074    steps: 153     evaluation reward: 1.33\n",
      "episode: 986   score: 0.0   memory length: 196862   epsilon: 0.7092112600063127    steps: 125     evaluation reward: 1.33\n",
      "episode: 987   score: 1.0   memory length: 197033   epsilon: 0.7088726800063201    steps: 171     evaluation reward: 1.33\n",
      "episode: 988   score: 1.0   memory length: 197203   epsilon: 0.7085360800063274    steps: 170     evaluation reward: 1.33\n",
      "episode: 989   score: 0.0   memory length: 197332   epsilon: 0.7082806600063329    steps: 129     evaluation reward: 1.32\n",
      "episode: 990   score: 4.0   memory length: 197608   epsilon: 0.7077341800063448    steps: 276     evaluation reward: 1.34\n",
      "episode: 991   score: 0.0   memory length: 197734   epsilon: 0.7074847000063502    steps: 126     evaluation reward: 1.32\n",
      "episode: 992   score: 1.0   memory length: 197908   epsilon: 0.7071401800063577    steps: 174     evaluation reward: 1.32\n",
      "episode: 993   score: 1.0   memory length: 198078   epsilon: 0.706803580006365    steps: 170     evaluation reward: 1.32\n",
      "episode: 994   score: 1.0   memory length: 198232   epsilon: 0.7064986600063716    steps: 154     evaluation reward: 1.33\n",
      "episode: 995   score: 0.0   memory length: 198354   epsilon: 0.7062571000063769    steps: 122     evaluation reward: 1.32\n",
      "episode: 996   score: 4.0   memory length: 198629   epsilon: 0.7057126000063887    steps: 275     evaluation reward: 1.35\n",
      "episode: 997   score: 2.0   memory length: 198828   epsilon: 0.7053185800063972    steps: 199     evaluation reward: 1.36\n",
      "episode: 998   score: 3.0   memory length: 199071   epsilon: 0.7048374400064077    steps: 243     evaluation reward: 1.39\n",
      "episode: 999   score: 4.0   memory length: 199392   epsilon: 0.7042018600064215    steps: 321     evaluation reward: 1.4\n",
      "episode: 1000   score: 0.0   memory length: 199520   epsilon: 0.703948420006427    steps: 128     evaluation reward: 1.4\n",
      "episode: 1001   score: 0.0   memory length: 199646   epsilon: 0.7036989400064324    steps: 126     evaluation reward: 1.4\n",
      "episode: 1002   score: 0.0   memory length: 199770   epsilon: 0.7034534200064377    steps: 124     evaluation reward: 1.39\n",
      "episode: 1003   score: 0.0   memory length: 199896   epsilon: 0.7032039400064432    steps: 126     evaluation reward: 1.37\n",
      "now time :  2019-09-25 16:47:38.684654\n",
      "episode: 1004   score: 10.0   memory length: 200292   epsilon: 0.7024198600064602    steps: 396     evaluation reward: 1.47\n",
      "episode: 1005   score: 2.0   memory length: 200491   epsilon: 0.7020258400064687    steps: 199     evaluation reward: 1.47\n",
      "episode: 1006   score: 0.0   memory length: 200616   epsilon: 0.7017783400064741    steps: 125     evaluation reward: 1.45\n",
      "episode: 1007   score: 0.0   memory length: 200745   epsilon: 0.7015229200064796    steps: 129     evaluation reward: 1.44\n",
      "episode: 1008   score: 3.0   memory length: 200978   epsilon: 0.7010615800064897    steps: 233     evaluation reward: 1.47\n",
      "episode: 1009   score: 0.0   memory length: 201103   epsilon: 0.700814080006495    steps: 125     evaluation reward: 1.47\n",
      "episode: 1010   score: 0.0   memory length: 201229   epsilon: 0.7005646000065004    steps: 126     evaluation reward: 1.45\n",
      "episode: 1011   score: 0.0   memory length: 201354   epsilon: 0.7003171000065058    steps: 125     evaluation reward: 1.42\n",
      "episode: 1012   score: 1.0   memory length: 201528   epsilon: 0.6999725800065133    steps: 174     evaluation reward: 1.43\n",
      "episode: 1013   score: 1.0   memory length: 201699   epsilon: 0.6996340000065207    steps: 171     evaluation reward: 1.44\n",
      "episode: 1014   score: 1.0   memory length: 201871   epsilon: 0.699293440006528    steps: 172     evaluation reward: 1.42\n",
      "episode: 1015   score: 1.0   memory length: 202025   epsilon: 0.6989885200065347    steps: 154     evaluation reward: 1.43\n",
      "episode: 1016   score: 0.0   memory length: 202150   epsilon: 0.69874102000654    steps: 125     evaluation reward: 1.43\n",
      "episode: 1017   score: 1.0   memory length: 202305   epsilon: 0.6984341200065467    steps: 155     evaluation reward: 1.41\n",
      "episode: 1018   score: 2.0   memory length: 202486   epsilon: 0.6980757400065545    steps: 181     evaluation reward: 1.4\n",
      "episode: 1019   score: 3.0   memory length: 202733   epsilon: 0.6975866800065651    steps: 247     evaluation reward: 1.43\n",
      "episode: 1020   score: 0.0   memory length: 202860   epsilon: 0.6973352200065706    steps: 127     evaluation reward: 1.42\n",
      "episode: 1021   score: 3.0   memory length: 203112   epsilon: 0.6968362600065814    steps: 252     evaluation reward: 1.45\n",
      "episode: 1022   score: 3.0   memory length: 203363   epsilon: 0.6963392800065922    steps: 251     evaluation reward: 1.45\n",
      "episode: 1023   score: 2.0   memory length: 203561   epsilon: 0.6959472400066007    steps: 198     evaluation reward: 1.47\n",
      "episode: 1024   score: 0.0   memory length: 203686   epsilon: 0.6956997400066061    steps: 125     evaluation reward: 1.45\n",
      "episode: 1025   score: 2.0   memory length: 203884   epsilon: 0.6953077000066146    steps: 198     evaluation reward: 1.43\n",
      "episode: 1026   score: 2.0   memory length: 204081   epsilon: 0.694917640006623    steps: 197     evaluation reward: 1.44\n",
      "episode: 1027   score: 2.0   memory length: 204304   epsilon: 0.6944761000066326    steps: 223     evaluation reward: 1.44\n",
      "episode: 1028   score: 2.0   memory length: 204504   epsilon: 0.6940801000066412    steps: 200     evaluation reward: 1.45\n",
      "episode: 1029   score: 1.0   memory length: 204674   epsilon: 0.6937435000066485    steps: 170     evaluation reward: 1.46\n",
      "episode: 1030   score: 0.0   memory length: 204802   epsilon: 0.693490060006654    steps: 128     evaluation reward: 1.46\n",
      "episode: 1031   score: 2.0   memory length: 205021   epsilon: 0.6930564400066634    steps: 219     evaluation reward: 1.47\n",
      "episode: 1032   score: 1.0   memory length: 205173   epsilon: 0.69275548000667    steps: 152     evaluation reward: 1.47\n",
      "episode: 1033   score: 1.0   memory length: 205331   epsilon: 0.6924426400066768    steps: 158     evaluation reward: 1.46\n",
      "episode: 1034   score: 2.0   memory length: 205532   epsilon: 0.6920446600066854    steps: 201     evaluation reward: 1.47\n",
      "episode: 1035   score: 0.0   memory length: 205660   epsilon: 0.6917912200066909    steps: 128     evaluation reward: 1.45\n",
      "episode: 1036   score: 0.0   memory length: 205784   epsilon: 0.6915457000066962    steps: 124     evaluation reward: 1.39\n",
      "episode: 1037   score: 2.0   memory length: 205967   epsilon: 0.6911833600067041    steps: 183     evaluation reward: 1.39\n",
      "episode: 1038   score: 0.0   memory length: 206094   epsilon: 0.6909319000067096    steps: 127     evaluation reward: 1.35\n",
      "episode: 1039   score: 0.0   memory length: 206216   epsilon: 0.6906903400067148    steps: 122     evaluation reward: 1.35\n",
      "episode: 1040   score: 0.0   memory length: 206339   epsilon: 0.6904468000067201    steps: 123     evaluation reward: 1.32\n",
      "episode: 1041   score: 1.0   memory length: 206512   epsilon: 0.6901042600067275    steps: 173     evaluation reward: 1.32\n",
      "episode: 1042   score: 1.0   memory length: 206683   epsilon: 0.6897656800067349    steps: 171     evaluation reward: 1.33\n",
      "episode: 1043   score: 2.0   memory length: 206901   epsilon: 0.6893340400067443    steps: 218     evaluation reward: 1.34\n",
      "episode: 1044   score: 0.0   memory length: 207024   epsilon: 0.6890905000067495    steps: 123     evaluation reward: 1.34\n",
      "episode: 1045   score: 0.0   memory length: 207149   epsilon: 0.6888430000067549    steps: 125     evaluation reward: 1.33\n",
      "episode: 1046   score: 3.0   memory length: 207401   epsilon: 0.6883440400067657    steps: 252     evaluation reward: 1.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1047   score: 4.0   memory length: 207719   epsilon: 0.6877144000067794    steps: 318     evaluation reward: 1.36\n",
      "episode: 1048   score: 2.0   memory length: 207923   epsilon: 0.6873104800067882    steps: 204     evaluation reward: 1.38\n",
      "episode: 1049   score: 1.0   memory length: 208079   epsilon: 0.6870016000067949    steps: 156     evaluation reward: 1.37\n",
      "episode: 1050   score: 1.0   memory length: 208252   epsilon: 0.6866590600068023    steps: 173     evaluation reward: 1.37\n",
      "episode: 1051   score: 0.0   memory length: 208380   epsilon: 0.6864056200068078    steps: 128     evaluation reward: 1.37\n",
      "episode: 1052   score: 2.0   memory length: 208579   epsilon: 0.6860116000068164    steps: 199     evaluation reward: 1.37\n",
      "episode: 1053   score: 0.0   memory length: 208708   epsilon: 0.6857561800068219    steps: 129     evaluation reward: 1.37\n",
      "episode: 1054   score: 1.0   memory length: 208861   epsilon: 0.6854532400068285    steps: 153     evaluation reward: 1.35\n",
      "episode: 1055   score: 1.0   memory length: 209038   epsilon: 0.6851027800068361    steps: 177     evaluation reward: 1.36\n",
      "episode: 1056   score: 3.0   memory length: 209285   epsilon: 0.6846137200068467    steps: 247     evaluation reward: 1.38\n",
      "episode: 1057   score: 1.0   memory length: 209461   epsilon: 0.6842652400068543    steps: 176     evaluation reward: 1.36\n",
      "episode: 1058   score: 0.0   memory length: 209584   epsilon: 0.6840217000068596    steps: 123     evaluation reward: 1.33\n",
      "episode: 1059   score: 0.0   memory length: 209708   epsilon: 0.6837761800068649    steps: 124     evaluation reward: 1.31\n",
      "episode: 1060   score: 2.0   memory length: 209907   epsilon: 0.6833821600068735    steps: 199     evaluation reward: 1.33\n",
      "episode: 1061   score: 1.0   memory length: 210081   epsilon: 0.6830376400068809    steps: 174     evaluation reward: 1.33\n",
      "episode: 1062   score: 0.0   memory length: 210206   epsilon: 0.6827901400068863    steps: 125     evaluation reward: 1.33\n",
      "episode: 1063   score: 2.0   memory length: 210424   epsilon: 0.6823585000068957    steps: 218     evaluation reward: 1.35\n",
      "episode: 1064   score: 0.0   memory length: 210547   epsilon: 0.682114960006901    steps: 123     evaluation reward: 1.34\n",
      "episode: 1065   score: 1.0   memory length: 210717   epsilon: 0.6817783600069083    steps: 170     evaluation reward: 1.35\n",
      "episode: 1066   score: 0.0   memory length: 210845   epsilon: 0.6815249200069138    steps: 128     evaluation reward: 1.29\n",
      "episode: 1067   score: 0.0   memory length: 210970   epsilon: 0.6812774200069192    steps: 125     evaluation reward: 1.28\n",
      "episode: 1068   score: 3.0   memory length: 211220   epsilon: 0.6807824200069299    steps: 250     evaluation reward: 1.3\n",
      "episode: 1069   score: 0.0   memory length: 211344   epsilon: 0.6805369000069352    steps: 124     evaluation reward: 1.28\n",
      "episode: 1070   score: 2.0   memory length: 211551   epsilon: 0.6801270400069441    steps: 207     evaluation reward: 1.3\n",
      "episode: 1071   score: 2.0   memory length: 211753   epsilon: 0.6797270800069528    steps: 202     evaluation reward: 1.31\n",
      "episode: 1072   score: 1.0   memory length: 211905   epsilon: 0.6794261200069593    steps: 152     evaluation reward: 1.29\n",
      "episode: 1073   score: 0.0   memory length: 212033   epsilon: 0.6791726800069648    steps: 128     evaluation reward: 1.29\n",
      "episode: 1074   score: 2.0   memory length: 212230   epsilon: 0.6787826200069733    steps: 197     evaluation reward: 1.29\n",
      "episode: 1075   score: 4.0   memory length: 212528   epsilon: 0.6781925800069861    steps: 298     evaluation reward: 1.32\n",
      "episode: 1076   score: 2.0   memory length: 212749   epsilon: 0.6777550000069956    steps: 221     evaluation reward: 1.34\n",
      "episode: 1077   score: 2.0   memory length: 212950   epsilon: 0.6773570200070043    steps: 201     evaluation reward: 1.34\n",
      "episode: 1078   score: 2.0   memory length: 213154   epsilon: 0.676953100007013    steps: 204     evaluation reward: 1.34\n",
      "episode: 1079   score: 1.0   memory length: 213325   epsilon: 0.6766145200070204    steps: 171     evaluation reward: 1.32\n",
      "episode: 1080   score: 1.0   memory length: 213496   epsilon: 0.6762759400070277    steps: 171     evaluation reward: 1.31\n",
      "episode: 1081   score: 1.0   memory length: 213666   epsilon: 0.675939340007035    steps: 170     evaluation reward: 1.32\n",
      "episode: 1082   score: 2.0   memory length: 213872   epsilon: 0.6755314600070439    steps: 206     evaluation reward: 1.32\n",
      "episode: 1083   score: 1.0   memory length: 214045   epsilon: 0.6751889200070513    steps: 173     evaluation reward: 1.31\n",
      "episode: 1084   score: 1.0   memory length: 214217   epsilon: 0.6748483600070587    steps: 172     evaluation reward: 1.29\n",
      "episode: 1085   score: 1.0   memory length: 214374   epsilon: 0.6745375000070655    steps: 157     evaluation reward: 1.29\n",
      "episode: 1086   score: 1.0   memory length: 214547   epsilon: 0.6741949600070729    steps: 173     evaluation reward: 1.3\n",
      "episode: 1087   score: 0.0   memory length: 214670   epsilon: 0.6739514200070782    steps: 123     evaluation reward: 1.29\n",
      "episode: 1088   score: 0.0   memory length: 214792   epsilon: 0.6737098600070834    steps: 122     evaluation reward: 1.28\n",
      "episode: 1089   score: 0.0   memory length: 214917   epsilon: 0.6734623600070888    steps: 125     evaluation reward: 1.28\n",
      "episode: 1090   score: 0.0   memory length: 215051   epsilon: 0.6731970400070946    steps: 134     evaluation reward: 1.24\n",
      "episode: 1091   score: 1.0   memory length: 215208   epsilon: 0.6728861800071013    steps: 157     evaluation reward: 1.25\n",
      "episode: 1092   score: 1.0   memory length: 215360   epsilon: 0.6725852200071079    steps: 152     evaluation reward: 1.25\n",
      "episode: 1093   score: 2.0   memory length: 215560   epsilon: 0.6721892200071165    steps: 200     evaluation reward: 1.26\n",
      "episode: 1094   score: 2.0   memory length: 215742   epsilon: 0.6718288600071243    steps: 182     evaluation reward: 1.27\n",
      "episode: 1095   score: 1.0   memory length: 215915   epsilon: 0.6714863200071317    steps: 173     evaluation reward: 1.28\n",
      "episode: 1096   score: 0.0   memory length: 216042   epsilon: 0.6712348600071372    steps: 127     evaluation reward: 1.24\n",
      "episode: 1097   score: 0.0   memory length: 216170   epsilon: 0.6709814200071427    steps: 128     evaluation reward: 1.22\n",
      "episode: 1098   score: 2.0   memory length: 216370   epsilon: 0.6705854200071513    steps: 200     evaluation reward: 1.21\n",
      "episode: 1099   score: 3.0   memory length: 216617   epsilon: 0.6700963600071619    steps: 247     evaluation reward: 1.2\n",
      "episode: 1100   score: 0.0   memory length: 216744   epsilon: 0.6698449000071673    steps: 127     evaluation reward: 1.2\n",
      "episode: 1101   score: 3.0   memory length: 216991   epsilon: 0.669355840007178    steps: 247     evaluation reward: 1.23\n",
      "episode: 1102   score: 2.0   memory length: 217210   epsilon: 0.6689222200071874    steps: 219     evaluation reward: 1.25\n",
      "episode: 1103   score: 0.0   memory length: 217335   epsilon: 0.6686747200071927    steps: 125     evaluation reward: 1.25\n",
      "episode: 1104   score: 1.0   memory length: 217488   epsilon: 0.6683717800071993    steps: 153     evaluation reward: 1.16\n",
      "episode: 1105   score: 1.0   memory length: 217659   epsilon: 0.6680332000072067    steps: 171     evaluation reward: 1.15\n",
      "episode: 1106   score: 1.0   memory length: 217830   epsilon: 0.667694620007214    steps: 171     evaluation reward: 1.16\n",
      "episode: 1107   score: 0.0   memory length: 217954   epsilon: 0.6674491000072194    steps: 124     evaluation reward: 1.16\n",
      "episode: 1108   score: 1.0   memory length: 218131   epsilon: 0.667098640007227    steps: 177     evaluation reward: 1.14\n",
      "episode: 1109   score: 1.0   memory length: 218301   epsilon: 0.6667620400072343    steps: 170     evaluation reward: 1.15\n",
      "episode: 1110   score: 0.0   memory length: 218424   epsilon: 0.6665185000072396    steps: 123     evaluation reward: 1.15\n",
      "episode: 1111   score: 3.0   memory length: 218656   epsilon: 0.6660591400072495    steps: 232     evaluation reward: 1.18\n",
      "episode: 1112   score: 0.0   memory length: 218780   epsilon: 0.6658136200072549    steps: 124     evaluation reward: 1.17\n",
      "episode: 1113   score: 0.0   memory length: 218902   epsilon: 0.6655720600072601    steps: 122     evaluation reward: 1.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1114   score: 3.0   memory length: 219149   epsilon: 0.6650830000072707    steps: 247     evaluation reward: 1.18\n",
      "episode: 1115   score: 2.0   memory length: 219349   epsilon: 0.6646870000072793    steps: 200     evaluation reward: 1.19\n",
      "episode: 1116   score: 3.0   memory length: 219579   epsilon: 0.6642316000072892    steps: 230     evaluation reward: 1.22\n",
      "episode: 1117   score: 3.0   memory length: 219830   epsilon: 0.6637346200073    steps: 251     evaluation reward: 1.24\n",
      "episode: 1118   score: 0.0   memory length: 219955   epsilon: 0.6634871200073054    steps: 125     evaluation reward: 1.22\n",
      "episode: 1119   score: 1.0   memory length: 220107   epsilon: 0.6631861600073119    steps: 152     evaluation reward: 1.2\n",
      "episode: 1120   score: 1.0   memory length: 220260   epsilon: 0.6628832200073185    steps: 153     evaluation reward: 1.21\n",
      "episode: 1121   score: 1.0   memory length: 220412   epsilon: 0.662582260007325    steps: 152     evaluation reward: 1.19\n",
      "episode: 1122   score: 0.0   memory length: 220539   epsilon: 0.6623308000073305    steps: 127     evaluation reward: 1.16\n",
      "episode: 1123   score: 0.0   memory length: 220664   epsilon: 0.6620833000073358    steps: 125     evaluation reward: 1.14\n",
      "episode: 1124   score: 5.0   memory length: 221007   epsilon: 0.6614041600073506    steps: 343     evaluation reward: 1.19\n",
      "episode: 1125   score: 2.0   memory length: 221207   epsilon: 0.6610081600073592    steps: 200     evaluation reward: 1.19\n",
      "episode: 1126   score: 0.0   memory length: 221332   epsilon: 0.6607606600073646    steps: 125     evaluation reward: 1.17\n",
      "episode: 1127   score: 2.0   memory length: 221556   epsilon: 0.6603171400073742    steps: 224     evaluation reward: 1.17\n",
      "episode: 1128   score: 1.0   memory length: 221729   epsilon: 0.6599746000073816    steps: 173     evaluation reward: 1.16\n",
      "episode: 1129   score: 2.0   memory length: 221934   epsilon: 0.6595687000073904    steps: 205     evaluation reward: 1.17\n",
      "episode: 1130   score: 2.0   memory length: 222156   epsilon: 0.6591291400074    steps: 222     evaluation reward: 1.19\n",
      "episode: 1131   score: 3.0   memory length: 222408   epsilon: 0.6586301800074108    steps: 252     evaluation reward: 1.2\n",
      "episode: 1132   score: 1.0   memory length: 222581   epsilon: 0.6582876400074182    steps: 173     evaluation reward: 1.2\n",
      "episode: 1133   score: 0.0   memory length: 222703   epsilon: 0.6580460800074235    steps: 122     evaluation reward: 1.19\n",
      "episode: 1134   score: 2.0   memory length: 222902   epsilon: 0.657652060007432    steps: 199     evaluation reward: 1.19\n",
      "episode: 1135   score: 0.0   memory length: 223026   epsilon: 0.6574065400074374    steps: 124     evaluation reward: 1.19\n",
      "episode: 1136   score: 2.0   memory length: 223223   epsilon: 0.6570164800074458    steps: 197     evaluation reward: 1.21\n",
      "episode: 1137   score: 2.0   memory length: 223420   epsilon: 0.6566264200074543    steps: 197     evaluation reward: 1.21\n",
      "episode: 1138   score: 2.0   memory length: 223607   epsilon: 0.6562561600074623    steps: 187     evaluation reward: 1.23\n",
      "episode: 1139   score: 1.0   memory length: 223763   epsilon: 0.655947280007469    steps: 156     evaluation reward: 1.24\n",
      "episode: 1140   score: 4.0   memory length: 224008   epsilon: 0.6554621800074796    steps: 245     evaluation reward: 1.28\n",
      "episode: 1141   score: 0.0   memory length: 224141   epsilon: 0.6551988400074853    steps: 133     evaluation reward: 1.27\n",
      "episode: 1142   score: 0.0   memory length: 224266   epsilon: 0.6549513400074907    steps: 125     evaluation reward: 1.26\n",
      "episode: 1143   score: 0.0   memory length: 224392   epsilon: 0.6547018600074961    steps: 126     evaluation reward: 1.24\n",
      "episode: 1144   score: 1.0   memory length: 224547   epsilon: 0.6543949600075027    steps: 155     evaluation reward: 1.25\n",
      "episode: 1145   score: 1.0   memory length: 224717   epsilon: 0.65405836000751    steps: 170     evaluation reward: 1.26\n",
      "episode: 1146   score: 3.0   memory length: 224963   epsilon: 0.6535712800075206    steps: 246     evaluation reward: 1.26\n",
      "episode: 1147   score: 2.0   memory length: 225168   epsilon: 0.6531653800075294    steps: 205     evaluation reward: 1.24\n",
      "episode: 1148   score: 0.0   memory length: 225293   epsilon: 0.6529178800075348    steps: 125     evaluation reward: 1.22\n",
      "episode: 1149   score: 2.0   memory length: 225492   epsilon: 0.6525238600075434    steps: 199     evaluation reward: 1.23\n",
      "episode: 1150   score: 2.0   memory length: 225695   epsilon: 0.6521219200075521    steps: 203     evaluation reward: 1.24\n",
      "episode: 1151   score: 2.0   memory length: 225893   epsilon: 0.6517298800075606    steps: 198     evaluation reward: 1.26\n",
      "episode: 1152   score: 1.0   memory length: 226048   epsilon: 0.6514229800075673    steps: 155     evaluation reward: 1.25\n",
      "episode: 1153   score: 2.0   memory length: 226248   epsilon: 0.6510269800075759    steps: 200     evaluation reward: 1.27\n",
      "episode: 1154   score: 0.0   memory length: 226374   epsilon: 0.6507775000075813    steps: 126     evaluation reward: 1.26\n",
      "episode: 1155   score: 1.0   memory length: 226526   epsilon: 0.6504765400075878    steps: 152     evaluation reward: 1.26\n",
      "episode: 1156   score: 4.0   memory length: 226786   epsilon: 0.649961740007599    steps: 260     evaluation reward: 1.27\n",
      "episode: 1157   score: 0.0   memory length: 226908   epsilon: 0.6497201800076042    steps: 122     evaluation reward: 1.26\n",
      "episode: 1158   score: 2.0   memory length: 227111   epsilon: 0.649318240007613    steps: 203     evaluation reward: 1.28\n",
      "episode: 1159   score: 3.0   memory length: 227362   epsilon: 0.6488212600076237    steps: 251     evaluation reward: 1.31\n",
      "episode: 1160   score: 1.0   memory length: 227519   epsilon: 0.6485104000076305    steps: 157     evaluation reward: 1.3\n",
      "episode: 1161   score: 0.0   memory length: 227648   epsilon: 0.648254980007636    steps: 129     evaluation reward: 1.29\n",
      "episode: 1162   score: 3.0   memory length: 227874   epsilon: 0.6478075000076458    steps: 226     evaluation reward: 1.32\n",
      "episode: 1163   score: 3.0   memory length: 228125   epsilon: 0.6473105200076565    steps: 251     evaluation reward: 1.33\n",
      "episode: 1164   score: 1.0   memory length: 228295   epsilon: 0.6469739200076638    steps: 170     evaluation reward: 1.34\n",
      "episode: 1165   score: 1.0   memory length: 228448   epsilon: 0.6466709800076704    steps: 153     evaluation reward: 1.34\n",
      "episode: 1166   score: 2.0   memory length: 228647   epsilon: 0.646276960007679    steps: 199     evaluation reward: 1.36\n",
      "episode: 1167   score: 2.0   memory length: 228849   epsilon: 0.6458770000076877    steps: 202     evaluation reward: 1.38\n",
      "episode: 1168   score: 0.0   memory length: 228974   epsilon: 0.645629500007693    steps: 125     evaluation reward: 1.35\n",
      "episode: 1169   score: 0.0   memory length: 229096   epsilon: 0.6453879400076983    steps: 122     evaluation reward: 1.35\n",
      "episode: 1170   score: 0.0   memory length: 229221   epsilon: 0.6451404400077037    steps: 125     evaluation reward: 1.33\n",
      "episode: 1171   score: 6.0   memory length: 229569   epsilon: 0.6444514000077186    steps: 348     evaluation reward: 1.37\n",
      "episode: 1172   score: 0.0   memory length: 229696   epsilon: 0.6441999400077241    steps: 127     evaluation reward: 1.36\n",
      "episode: 1173   score: 5.0   memory length: 230020   epsilon: 0.643558420007738    steps: 324     evaluation reward: 1.41\n",
      "episode: 1174   score: 1.0   memory length: 230188   epsilon: 0.6432257800077452    steps: 168     evaluation reward: 1.4\n",
      "episode: 1175   score: 2.0   memory length: 230406   epsilon: 0.6427941400077546    steps: 218     evaluation reward: 1.38\n",
      "episode: 1176   score: 2.0   memory length: 230604   epsilon: 0.6424021000077631    steps: 198     evaluation reward: 1.38\n",
      "episode: 1177   score: 1.0   memory length: 230778   epsilon: 0.6420575800077706    steps: 174     evaluation reward: 1.37\n",
      "episode: 1178   score: 3.0   memory length: 231053   epsilon: 0.6415130800077824    steps: 275     evaluation reward: 1.38\n",
      "episode: 1179   score: 0.0   memory length: 231176   epsilon: 0.6412695400077877    steps: 123     evaluation reward: 1.37\n",
      "episode: 1180   score: 1.0   memory length: 231351   epsilon: 0.6409230400077952    steps: 175     evaluation reward: 1.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1181   score: 2.0   memory length: 231549   epsilon: 0.6405310000078037    steps: 198     evaluation reward: 1.38\n",
      "episode: 1182   score: 2.0   memory length: 231746   epsilon: 0.6401409400078122    steps: 197     evaluation reward: 1.38\n",
      "episode: 1183   score: 0.0   memory length: 231877   epsilon: 0.6398815600078178    steps: 131     evaluation reward: 1.37\n",
      "episode: 1184   score: 1.0   memory length: 232031   epsilon: 0.6395766400078244    steps: 154     evaluation reward: 1.37\n",
      "episode: 1185   score: 0.0   memory length: 232155   epsilon: 0.6393311200078298    steps: 124     evaluation reward: 1.36\n",
      "episode: 1186   score: 0.0   memory length: 232280   epsilon: 0.6390836200078351    steps: 125     evaluation reward: 1.35\n",
      "episode: 1187   score: 0.0   memory length: 232414   epsilon: 0.6388183000078409    steps: 134     evaluation reward: 1.35\n",
      "episode: 1188   score: 2.0   memory length: 232634   epsilon: 0.6383827000078504    steps: 220     evaluation reward: 1.37\n",
      "episode: 1189   score: 2.0   memory length: 232857   epsilon: 0.6379411600078599    steps: 223     evaluation reward: 1.39\n",
      "episode: 1190   score: 2.0   memory length: 233037   epsilon: 0.6375847600078677    steps: 180     evaluation reward: 1.41\n",
      "episode: 1191   score: 0.0   memory length: 233162   epsilon: 0.637337260007873    steps: 125     evaluation reward: 1.4\n",
      "episode: 1192   score: 0.0   memory length: 233291   epsilon: 0.6370818400078786    steps: 129     evaluation reward: 1.39\n",
      "episode: 1193   score: 1.0   memory length: 233444   epsilon: 0.6367789000078852    steps: 153     evaluation reward: 1.38\n",
      "episode: 1194   score: 1.0   memory length: 233597   epsilon: 0.6364759600078918    steps: 153     evaluation reward: 1.37\n",
      "episode: 1195   score: 0.0   memory length: 233721   epsilon: 0.6362304400078971    steps: 124     evaluation reward: 1.36\n",
      "episode: 1196   score: 0.0   memory length: 233847   epsilon: 0.6359809600079025    steps: 126     evaluation reward: 1.36\n",
      "episode: 1197   score: 3.0   memory length: 234113   epsilon: 0.6354542800079139    steps: 266     evaluation reward: 1.39\n",
      "episode: 1198   score: 0.0   memory length: 234239   epsilon: 0.6352048000079193    steps: 126     evaluation reward: 1.37\n",
      "episode: 1199   score: 0.0   memory length: 234363   epsilon: 0.6349592800079247    steps: 124     evaluation reward: 1.34\n",
      "episode: 1200   score: 0.0   memory length: 234485   epsilon: 0.6347177200079299    steps: 122     evaluation reward: 1.34\n",
      "episode: 1201   score: 0.0   memory length: 234607   epsilon: 0.6344761600079352    steps: 122     evaluation reward: 1.31\n",
      "episode: 1202   score: 0.0   memory length: 234729   epsilon: 0.6342346000079404    steps: 122     evaluation reward: 1.29\n",
      "episode: 1203   score: 0.0   memory length: 234853   epsilon: 0.6339890800079457    steps: 124     evaluation reward: 1.29\n",
      "episode: 1204   score: 1.0   memory length: 235022   epsilon: 0.633654460007953    steps: 169     evaluation reward: 1.29\n",
      "episode: 1205   score: 0.0   memory length: 235150   epsilon: 0.6334010200079585    steps: 128     evaluation reward: 1.28\n",
      "episode: 1206   score: 1.0   memory length: 235321   epsilon: 0.6330624400079659    steps: 171     evaluation reward: 1.28\n",
      "episode: 1207   score: 0.0   memory length: 235447   epsilon: 0.6328129600079713    steps: 126     evaluation reward: 1.28\n",
      "episode: 1208   score: 1.0   memory length: 235617   epsilon: 0.6324763600079786    steps: 170     evaluation reward: 1.28\n",
      "episode: 1209   score: 2.0   memory length: 235818   epsilon: 0.6320783800079872    steps: 201     evaluation reward: 1.29\n",
      "episode: 1210   score: 1.0   memory length: 235973   epsilon: 0.6317714800079939    steps: 155     evaluation reward: 1.3\n",
      "episode: 1211   score: 0.0   memory length: 236099   epsilon: 0.6315220000079993    steps: 126     evaluation reward: 1.27\n",
      "episode: 1212   score: 0.0   memory length: 236227   epsilon: 0.6312685600080048    steps: 128     evaluation reward: 1.27\n",
      "episode: 1213   score: 3.0   memory length: 236500   epsilon: 0.6307280200080165    steps: 273     evaluation reward: 1.3\n",
      "episode: 1214   score: 2.0   memory length: 236701   epsilon: 0.6303300400080252    steps: 201     evaluation reward: 1.29\n",
      "episode: 1215   score: 2.0   memory length: 236924   epsilon: 0.6298885000080348    steps: 223     evaluation reward: 1.29\n",
      "episode: 1216   score: 2.0   memory length: 237128   epsilon: 0.6294845800080435    steps: 204     evaluation reward: 1.28\n",
      "episode: 1217   score: 1.0   memory length: 237298   epsilon: 0.6291479800080508    steps: 170     evaluation reward: 1.26\n",
      "episode: 1218   score: 2.0   memory length: 237513   epsilon: 0.6287222800080601    steps: 215     evaluation reward: 1.28\n",
      "episode: 1219   score: 2.0   memory length: 237712   epsilon: 0.6283282600080686    steps: 199     evaluation reward: 1.29\n",
      "episode: 1220   score: 2.0   memory length: 237915   epsilon: 0.6279263200080774    steps: 203     evaluation reward: 1.3\n",
      "episode: 1221   score: 1.0   memory length: 238087   epsilon: 0.6275857600080847    steps: 172     evaluation reward: 1.3\n",
      "episode: 1222   score: 0.0   memory length: 238213   epsilon: 0.6273362800080902    steps: 126     evaluation reward: 1.3\n",
      "episode: 1223   score: 0.0   memory length: 238338   epsilon: 0.6270887800080955    steps: 125     evaluation reward: 1.3\n",
      "episode: 1224   score: 0.0   memory length: 238461   epsilon: 0.6268452400081008    steps: 123     evaluation reward: 1.25\n",
      "episode: 1225   score: 0.0   memory length: 238586   epsilon: 0.6265977400081062    steps: 125     evaluation reward: 1.23\n",
      "episode: 1226   score: 1.0   memory length: 238757   epsilon: 0.6262591600081135    steps: 171     evaluation reward: 1.24\n",
      "episode: 1227   score: 0.0   memory length: 238883   epsilon: 0.626009680008119    steps: 126     evaluation reward: 1.22\n",
      "episode: 1228   score: 2.0   memory length: 239082   epsilon: 0.6256156600081275    steps: 199     evaluation reward: 1.23\n",
      "episode: 1229   score: 1.0   memory length: 239240   epsilon: 0.6253028200081343    steps: 158     evaluation reward: 1.22\n",
      "episode: 1230   score: 2.0   memory length: 239437   epsilon: 0.6249127600081428    steps: 197     evaluation reward: 1.22\n",
      "episode: 1231   score: 0.0   memory length: 239559   epsilon: 0.624671200008148    steps: 122     evaluation reward: 1.19\n",
      "episode: 1232   score: 4.0   memory length: 239837   epsilon: 0.62412076000816    steps: 278     evaluation reward: 1.22\n",
      "episode: 1233   score: 1.0   memory length: 239990   epsilon: 0.6238178200081665    steps: 153     evaluation reward: 1.23\n",
      "episode: 1234   score: 2.0   memory length: 240190   epsilon: 0.6234218200081751    steps: 200     evaluation reward: 1.23\n",
      "episode: 1235   score: 0.0   memory length: 240315   epsilon: 0.6231743200081805    steps: 125     evaluation reward: 1.23\n",
      "episode: 1236   score: 2.0   memory length: 240500   epsilon: 0.6228080200081885    steps: 185     evaluation reward: 1.23\n",
      "episode: 1237   score: 3.0   memory length: 240765   epsilon: 0.6222833200081999    steps: 265     evaluation reward: 1.24\n",
      "episode: 1238   score: 2.0   memory length: 240963   epsilon: 0.6218912800082084    steps: 198     evaluation reward: 1.24\n",
      "episode: 1239   score: 1.0   memory length: 241136   epsilon: 0.6215487400082158    steps: 173     evaluation reward: 1.24\n",
      "episode: 1240   score: 0.0   memory length: 241261   epsilon: 0.6213012400082212    steps: 125     evaluation reward: 1.2\n",
      "episode: 1241   score: 1.0   memory length: 241416   epsilon: 0.6209943400082278    steps: 155     evaluation reward: 1.21\n",
      "episode: 1242   score: 2.0   memory length: 241597   epsilon: 0.6206359600082356    steps: 181     evaluation reward: 1.23\n",
      "episode: 1243   score: 1.0   memory length: 241769   epsilon: 0.620295400008243    steps: 172     evaluation reward: 1.24\n",
      "episode: 1244   score: 4.0   memory length: 242067   epsilon: 0.6197053600082558    steps: 298     evaluation reward: 1.27\n",
      "episode: 1245   score: 0.0   memory length: 242197   epsilon: 0.6194479600082614    steps: 130     evaluation reward: 1.26\n",
      "episode: 1246   score: 2.0   memory length: 242399   epsilon: 0.6190480000082701    steps: 202     evaluation reward: 1.25\n",
      "episode: 1247   score: 1.0   memory length: 242574   epsilon: 0.6187015000082776    steps: 175     evaluation reward: 1.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1248   score: 1.0   memory length: 242743   epsilon: 0.6183668800082849    steps: 169     evaluation reward: 1.25\n",
      "episode: 1249   score: 0.0   memory length: 242870   epsilon: 0.6181154200082903    steps: 127     evaluation reward: 1.23\n",
      "episode: 1250   score: 0.0   memory length: 242993   epsilon: 0.6178718800082956    steps: 123     evaluation reward: 1.21\n",
      "episode: 1251   score: 2.0   memory length: 243215   epsilon: 0.6174323200083052    steps: 222     evaluation reward: 1.21\n",
      "episode: 1252   score: 2.0   memory length: 243416   epsilon: 0.6170343400083138    steps: 201     evaluation reward: 1.22\n",
      "episode: 1253   score: 0.0   memory length: 243540   epsilon: 0.6167888200083191    steps: 124     evaluation reward: 1.2\n",
      "episode: 1254   score: 0.0   memory length: 243662   epsilon: 0.6165472600083244    steps: 122     evaluation reward: 1.2\n",
      "episode: 1255   score: 0.0   memory length: 243786   epsilon: 0.6163017400083297    steps: 124     evaluation reward: 1.19\n",
      "episode: 1256   score: 2.0   memory length: 244005   epsilon: 0.6158681200083391    steps: 219     evaluation reward: 1.17\n",
      "episode: 1257   score: 1.0   memory length: 244157   epsilon: 0.6155671600083457    steps: 152     evaluation reward: 1.18\n",
      "episode: 1258   score: 1.0   memory length: 244307   epsilon: 0.6152701600083521    steps: 150     evaluation reward: 1.17\n",
      "episode: 1259   score: 2.0   memory length: 244494   epsilon: 0.6148999000083601    steps: 187     evaluation reward: 1.16\n",
      "episode: 1260   score: 4.0   memory length: 244742   epsilon: 0.6144088600083708    steps: 248     evaluation reward: 1.19\n",
      "episode: 1261   score: 2.0   memory length: 244927   epsilon: 0.6140425600083788    steps: 185     evaluation reward: 1.21\n",
      "episode: 1262   score: 0.0   memory length: 245055   epsilon: 0.6137891200083843    steps: 128     evaluation reward: 1.18\n",
      "episode: 1263   score: 2.0   memory length: 245273   epsilon: 0.6133574800083936    steps: 218     evaluation reward: 1.17\n",
      "episode: 1264   score: 1.0   memory length: 245427   epsilon: 0.6130525600084002    steps: 154     evaluation reward: 1.17\n",
      "episode: 1265   score: 2.0   memory length: 245627   epsilon: 0.6126565600084088    steps: 200     evaluation reward: 1.18\n",
      "episode: 1266   score: 0.0   memory length: 245754   epsilon: 0.6124051000084143    steps: 127     evaluation reward: 1.16\n",
      "episode: 1267   score: 0.0   memory length: 245876   epsilon: 0.6121635400084195    steps: 122     evaluation reward: 1.14\n",
      "episode: 1268   score: 0.0   memory length: 245998   epsilon: 0.6119219800084248    steps: 122     evaluation reward: 1.14\n",
      "episode: 1269   score: 2.0   memory length: 246204   epsilon: 0.6115141000084336    steps: 206     evaluation reward: 1.16\n",
      "episode: 1270   score: 1.0   memory length: 246376   epsilon: 0.611173540008441    steps: 172     evaluation reward: 1.17\n",
      "episode: 1271   score: 2.0   memory length: 246581   epsilon: 0.6107676400084499    steps: 205     evaluation reward: 1.13\n",
      "episode: 1272   score: 0.0   memory length: 246703   epsilon: 0.6105260800084551    steps: 122     evaluation reward: 1.13\n",
      "episode: 1273   score: 1.0   memory length: 246874   epsilon: 0.6101875000084624    steps: 171     evaluation reward: 1.09\n",
      "episode: 1274   score: 1.0   memory length: 247026   epsilon: 0.609886540008469    steps: 152     evaluation reward: 1.09\n",
      "episode: 1275   score: 2.0   memory length: 247247   epsilon: 0.6094489600084785    steps: 221     evaluation reward: 1.09\n",
      "episode: 1276   score: 0.0   memory length: 247371   epsilon: 0.6092034400084838    steps: 124     evaluation reward: 1.07\n",
      "episode: 1277   score: 2.0   memory length: 247570   epsilon: 0.6088094200084924    steps: 199     evaluation reward: 1.08\n",
      "episode: 1278   score: 2.0   memory length: 247769   epsilon: 0.6084154000085009    steps: 199     evaluation reward: 1.07\n",
      "episode: 1279   score: 1.0   memory length: 247939   epsilon: 0.6080788000085082    steps: 170     evaluation reward: 1.08\n",
      "episode: 1280   score: 2.0   memory length: 248142   epsilon: 0.607676860008517    steps: 203     evaluation reward: 1.09\n",
      "episode: 1281   score: 0.0   memory length: 248266   epsilon: 0.6074313400085223    steps: 124     evaluation reward: 1.07\n",
      "episode: 1282   score: 1.0   memory length: 248438   epsilon: 0.6070907800085297    steps: 172     evaluation reward: 1.06\n",
      "episode: 1283   score: 2.0   memory length: 248640   epsilon: 0.6066908200085384    steps: 202     evaluation reward: 1.08\n",
      "episode: 1284   score: 3.0   memory length: 248865   epsilon: 0.606245320008548    steps: 225     evaluation reward: 1.1\n",
      "episode: 1285   score: 1.0   memory length: 249035   epsilon: 0.6059087200085553    steps: 170     evaluation reward: 1.11\n",
      "episode: 1286   score: 1.0   memory length: 249187   epsilon: 0.6056077600085619    steps: 152     evaluation reward: 1.12\n",
      "episode: 1287   score: 0.0   memory length: 249309   epsilon: 0.6053662000085671    steps: 122     evaluation reward: 1.12\n",
      "episode: 1288   score: 2.0   memory length: 249532   epsilon: 0.6049246600085767    steps: 223     evaluation reward: 1.12\n",
      "episode: 1289   score: 1.0   memory length: 249684   epsilon: 0.6046237000085832    steps: 152     evaluation reward: 1.11\n",
      "episode: 1290   score: 0.0   memory length: 249806   epsilon: 0.6043821400085885    steps: 122     evaluation reward: 1.09\n",
      "now time :  2019-09-25 17:00:52.589584\n",
      "episode: 1291   score: 2.0   memory length: 250005   epsilon: 0.603988120008597    steps: 199     evaluation reward: 1.11\n",
      "episode: 1292   score: 3.0   memory length: 250253   epsilon: 0.6034970800086077    steps: 248     evaluation reward: 1.14\n",
      "episode: 1293   score: 2.0   memory length: 250472   epsilon: 0.6030634600086171    steps: 219     evaluation reward: 1.15\n",
      "episode: 1294   score: 2.0   memory length: 250672   epsilon: 0.6026674600086257    steps: 200     evaluation reward: 1.16\n",
      "episode: 1295   score: 2.0   memory length: 250869   epsilon: 0.6022774000086342    steps: 197     evaluation reward: 1.18\n",
      "episode: 1296   score: 1.0   memory length: 251021   epsilon: 0.6019764400086407    steps: 152     evaluation reward: 1.19\n",
      "episode: 1297   score: 1.0   memory length: 251182   epsilon: 0.6016576600086476    steps: 161     evaluation reward: 1.17\n",
      "episode: 1298   score: 2.0   memory length: 251380   epsilon: 0.6012656200086561    steps: 198     evaluation reward: 1.19\n",
      "episode: 1299   score: 1.0   memory length: 251549   epsilon: 0.6009310000086634    steps: 169     evaluation reward: 1.2\n",
      "episode: 1300   score: 0.0   memory length: 251672   epsilon: 0.6006874600086687    steps: 123     evaluation reward: 1.2\n",
      "episode: 1301   score: 0.0   memory length: 251796   epsilon: 0.600441940008674    steps: 124     evaluation reward: 1.2\n",
      "episode: 1302   score: 2.0   memory length: 252014   epsilon: 0.6000103000086834    steps: 218     evaluation reward: 1.22\n",
      "episode: 1303   score: 1.0   memory length: 252189   epsilon: 0.5996638000086909    steps: 175     evaluation reward: 1.23\n",
      "episode: 1304   score: 1.0   memory length: 252358   epsilon: 0.5993291800086982    steps: 169     evaluation reward: 1.23\n",
      "episode: 1305   score: 2.0   memory length: 252556   epsilon: 0.5989371400087067    steps: 198     evaluation reward: 1.25\n",
      "episode: 1306   score: 1.0   memory length: 252726   epsilon: 0.598600540008714    steps: 170     evaluation reward: 1.25\n",
      "episode: 1307   score: 1.0   memory length: 252895   epsilon: 0.5982659200087213    steps: 169     evaluation reward: 1.26\n",
      "episode: 1308   score: 2.0   memory length: 253096   epsilon: 0.5978679400087299    steps: 201     evaluation reward: 1.27\n",
      "episode: 1309   score: 2.0   memory length: 253280   epsilon: 0.5975036200087378    steps: 184     evaluation reward: 1.27\n",
      "episode: 1310   score: 0.0   memory length: 253404   epsilon: 0.5972581000087431    steps: 124     evaluation reward: 1.26\n",
      "episode: 1311   score: 1.0   memory length: 253577   epsilon: 0.5969155600087506    steps: 173     evaluation reward: 1.27\n",
      "episode: 1312   score: 1.0   memory length: 253729   epsilon: 0.5966146000087571    steps: 152     evaluation reward: 1.28\n",
      "episode: 1313   score: 0.0   memory length: 253855   epsilon: 0.5963651200087625    steps: 126     evaluation reward: 1.25\n",
      "episode: 1314   score: 2.0   memory length: 254078   epsilon: 0.5959235800087721    steps: 223     evaluation reward: 1.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1315   score: 2.0   memory length: 254275   epsilon: 0.5955335200087806    steps: 197     evaluation reward: 1.25\n",
      "episode: 1316   score: 0.0   memory length: 254398   epsilon: 0.5952899800087859    steps: 123     evaluation reward: 1.23\n",
      "episode: 1317   score: 4.0   memory length: 254689   epsilon: 0.5947138000087984    steps: 291     evaluation reward: 1.26\n",
      "episode: 1318   score: 1.0   memory length: 254840   epsilon: 0.5944148200088049    steps: 151     evaluation reward: 1.25\n",
      "episode: 1319   score: 1.0   memory length: 254990   epsilon: 0.5941178200088113    steps: 150     evaluation reward: 1.24\n",
      "episode: 1320   score: 0.0   memory length: 255113   epsilon: 0.5938742800088166    steps: 123     evaluation reward: 1.22\n",
      "episode: 1321   score: 0.0   memory length: 255236   epsilon: 0.5936307400088219    steps: 123     evaluation reward: 1.21\n",
      "episode: 1322   score: 4.0   memory length: 255485   epsilon: 0.5931377200088326    steps: 249     evaluation reward: 1.25\n",
      "episode: 1323   score: 2.0   memory length: 255669   epsilon: 0.5927734000088405    steps: 184     evaluation reward: 1.27\n",
      "episode: 1324   score: 2.0   memory length: 255870   epsilon: 0.5923754200088491    steps: 201     evaluation reward: 1.29\n",
      "episode: 1325   score: 3.0   memory length: 256118   epsilon: 0.5918843800088598    steps: 248     evaluation reward: 1.32\n",
      "episode: 1326   score: 1.0   memory length: 256287   epsilon: 0.591549760008867    steps: 169     evaluation reward: 1.32\n",
      "episode: 1327   score: 0.0   memory length: 256410   epsilon: 0.5913062200088723    steps: 123     evaluation reward: 1.32\n",
      "episode: 1328   score: 2.0   memory length: 256608   epsilon: 0.5909141800088809    steps: 198     evaluation reward: 1.32\n",
      "episode: 1329   score: 0.0   memory length: 256732   epsilon: 0.5906686600088862    steps: 124     evaluation reward: 1.31\n",
      "episode: 1330   score: 0.0   memory length: 256855   epsilon: 0.5904251200088915    steps: 123     evaluation reward: 1.29\n",
      "episode: 1331   score: 0.0   memory length: 256983   epsilon: 0.590171680008897    steps: 128     evaluation reward: 1.29\n",
      "episode: 1332   score: 1.0   memory length: 257154   epsilon: 0.5898331000089043    steps: 171     evaluation reward: 1.26\n",
      "episode: 1333   score: 3.0   memory length: 257380   epsilon: 0.589385620008914    steps: 226     evaluation reward: 1.28\n",
      "episode: 1334   score: 0.0   memory length: 257505   epsilon: 0.5891381200089194    steps: 125     evaluation reward: 1.26\n",
      "episode: 1335   score: 2.0   memory length: 257702   epsilon: 0.5887480600089279    steps: 197     evaluation reward: 1.28\n",
      "episode: 1336   score: 0.0   memory length: 257831   epsilon: 0.5884926400089334    steps: 129     evaluation reward: 1.26\n",
      "episode: 1337   score: 1.0   memory length: 258007   epsilon: 0.588144160008941    steps: 176     evaluation reward: 1.24\n",
      "episode: 1338   score: 3.0   memory length: 258223   epsilon: 0.5877164800089503    steps: 216     evaluation reward: 1.25\n",
      "episode: 1339   score: 0.0   memory length: 258348   epsilon: 0.5874689800089556    steps: 125     evaluation reward: 1.24\n",
      "episode: 1340   score: 2.0   memory length: 258548   epsilon: 0.5870729800089642    steps: 200     evaluation reward: 1.26\n",
      "episode: 1341   score: 1.0   memory length: 258698   epsilon: 0.5867759800089707    steps: 150     evaluation reward: 1.26\n",
      "episode: 1342   score: 2.0   memory length: 258916   epsilon: 0.5863443400089801    steps: 218     evaluation reward: 1.26\n",
      "episode: 1343   score: 2.0   memory length: 259139   epsilon: 0.5859028000089896    steps: 223     evaluation reward: 1.27\n",
      "episode: 1344   score: 1.0   memory length: 259308   epsilon: 0.5855681800089969    steps: 169     evaluation reward: 1.24\n",
      "episode: 1345   score: 2.0   memory length: 259492   epsilon: 0.5852038600090048    steps: 184     evaluation reward: 1.26\n",
      "episode: 1346   score: 0.0   memory length: 259620   epsilon: 0.5849504200090103    steps: 128     evaluation reward: 1.24\n",
      "episode: 1347   score: 0.0   memory length: 259743   epsilon: 0.5847068800090156    steps: 123     evaluation reward: 1.23\n",
      "episode: 1348   score: 3.0   memory length: 259993   epsilon: 0.5842118800090264    steps: 250     evaluation reward: 1.25\n",
      "episode: 1349   score: 2.0   memory length: 260208   epsilon: 0.5837861800090356    steps: 215     evaluation reward: 1.27\n",
      "episode: 1350   score: 3.0   memory length: 260439   epsilon: 0.5833288000090455    steps: 231     evaluation reward: 1.3\n",
      "episode: 1351   score: 1.0   memory length: 260599   epsilon: 0.5830120000090524    steps: 160     evaluation reward: 1.29\n",
      "episode: 1352   score: 2.0   memory length: 260781   epsilon: 0.5826516400090602    steps: 182     evaluation reward: 1.29\n",
      "episode: 1353   score: 3.0   memory length: 261030   epsilon: 0.5821586200090709    steps: 249     evaluation reward: 1.32\n",
      "episode: 1354   score: 0.0   memory length: 261152   epsilon: 0.5819170600090762    steps: 122     evaluation reward: 1.32\n",
      "episode: 1355   score: 4.0   memory length: 261447   epsilon: 0.5813329600090889    steps: 295     evaluation reward: 1.36\n",
      "episode: 1356   score: 1.0   memory length: 261599   epsilon: 0.5810320000090954    steps: 152     evaluation reward: 1.35\n",
      "episode: 1357   score: 2.0   memory length: 261798   epsilon: 0.5806379800091039    steps: 199     evaluation reward: 1.36\n",
      "episode: 1358   score: 0.0   memory length: 261923   epsilon: 0.5803904800091093    steps: 125     evaluation reward: 1.35\n",
      "episode: 1359   score: 2.0   memory length: 262123   epsilon: 0.5799944800091179    steps: 200     evaluation reward: 1.35\n",
      "episode: 1360   score: 0.0   memory length: 262246   epsilon: 0.5797509400091232    steps: 123     evaluation reward: 1.31\n",
      "episode: 1361   score: 3.0   memory length: 262513   epsilon: 0.5792222800091347    steps: 267     evaluation reward: 1.32\n",
      "episode: 1362   score: 3.0   memory length: 262764   epsilon: 0.5787253000091455    steps: 251     evaluation reward: 1.35\n",
      "episode: 1363   score: 0.0   memory length: 262886   epsilon: 0.5784837400091507    steps: 122     evaluation reward: 1.33\n",
      "episode: 1364   score: 0.0   memory length: 263008   epsilon: 0.578242180009156    steps: 122     evaluation reward: 1.32\n",
      "episode: 1365   score: 0.0   memory length: 263133   epsilon: 0.5779946800091613    steps: 125     evaluation reward: 1.3\n",
      "episode: 1366   score: 1.0   memory length: 263306   epsilon: 0.5776521400091688    steps: 173     evaluation reward: 1.31\n",
      "episode: 1367   score: 2.0   memory length: 263527   epsilon: 0.5772145600091783    steps: 221     evaluation reward: 1.33\n",
      "episode: 1368   score: 2.0   memory length: 263711   epsilon: 0.5768502400091862    steps: 184     evaluation reward: 1.35\n",
      "episode: 1369   score: 0.0   memory length: 263837   epsilon: 0.5766007600091916    steps: 126     evaluation reward: 1.33\n",
      "episode: 1370   score: 1.0   memory length: 263990   epsilon: 0.5762978200091982    steps: 153     evaluation reward: 1.33\n",
      "episode: 1371   score: 0.0   memory length: 264112   epsilon: 0.5760562600092034    steps: 122     evaluation reward: 1.31\n",
      "episode: 1372   score: 2.0   memory length: 264319   epsilon: 0.5756464000092123    steps: 207     evaluation reward: 1.33\n",
      "episode: 1373   score: 2.0   memory length: 264522   epsilon: 0.575244460009221    steps: 203     evaluation reward: 1.34\n",
      "episode: 1374   score: 1.0   memory length: 264697   epsilon: 0.5748979600092285    steps: 175     evaluation reward: 1.34\n",
      "episode: 1375   score: 1.0   memory length: 264865   epsilon: 0.5745653200092358    steps: 168     evaluation reward: 1.33\n",
      "episode: 1376   score: 4.0   memory length: 265163   epsilon: 0.5739752800092486    steps: 298     evaluation reward: 1.37\n",
      "episode: 1377   score: 0.0   memory length: 265287   epsilon: 0.5737297600092539    steps: 124     evaluation reward: 1.35\n",
      "episode: 1378   score: 1.0   memory length: 265442   epsilon: 0.5734228600092606    steps: 155     evaluation reward: 1.34\n",
      "episode: 1379   score: 2.0   memory length: 265645   epsilon: 0.5730209200092693    steps: 203     evaluation reward: 1.35\n",
      "episode: 1380   score: 1.0   memory length: 265800   epsilon: 0.572714020009276    steps: 155     evaluation reward: 1.34\n",
      "episode: 1381   score: 1.0   memory length: 265953   epsilon: 0.5724110800092825    steps: 153     evaluation reward: 1.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1382   score: 2.0   memory length: 266175   epsilon: 0.5719715200092921    steps: 222     evaluation reward: 1.36\n",
      "episode: 1383   score: 4.0   memory length: 266450   epsilon: 0.5714270200093039    steps: 275     evaluation reward: 1.38\n",
      "episode: 1384   score: 2.0   memory length: 266669   epsilon: 0.5709934000093133    steps: 219     evaluation reward: 1.37\n",
      "episode: 1385   score: 0.0   memory length: 266793   epsilon: 0.5707478800093186    steps: 124     evaluation reward: 1.36\n",
      "episode: 1386   score: 0.0   memory length: 266921   epsilon: 0.5704944400093241    steps: 128     evaluation reward: 1.35\n",
      "episode: 1387   score: 0.0   memory length: 267046   epsilon: 0.5702469400093295    steps: 125     evaluation reward: 1.35\n",
      "episode: 1388   score: 3.0   memory length: 267279   epsilon: 0.5697856000093395    steps: 233     evaluation reward: 1.36\n",
      "episode: 1389   score: 1.0   memory length: 267451   epsilon: 0.5694450400093469    steps: 172     evaluation reward: 1.36\n",
      "episode: 1390   score: 1.0   memory length: 267621   epsilon: 0.5691084400093542    steps: 170     evaluation reward: 1.37\n",
      "episode: 1391   score: 1.0   memory length: 267795   epsilon: 0.5687639200093617    steps: 174     evaluation reward: 1.36\n",
      "episode: 1392   score: 0.0   memory length: 267919   epsilon: 0.568518400009367    steps: 124     evaluation reward: 1.33\n",
      "episode: 1393   score: 1.0   memory length: 268091   epsilon: 0.5681778400093744    steps: 172     evaluation reward: 1.32\n",
      "episode: 1394   score: 0.0   memory length: 268218   epsilon: 0.5679263800093799    steps: 127     evaluation reward: 1.3\n",
      "episode: 1395   score: 0.0   memory length: 268345   epsilon: 0.5676749200093854    steps: 127     evaluation reward: 1.28\n",
      "episode: 1396   score: 2.0   memory length: 268545   epsilon: 0.567278920009394    steps: 200     evaluation reward: 1.29\n",
      "episode: 1397   score: 1.0   memory length: 268717   epsilon: 0.5669383600094013    steps: 172     evaluation reward: 1.29\n",
      "episode: 1398   score: 1.0   memory length: 268871   epsilon: 0.566633440009408    steps: 154     evaluation reward: 1.28\n",
      "episode: 1399   score: 0.0   memory length: 268994   epsilon: 0.5663899000094132    steps: 123     evaluation reward: 1.27\n",
      "episode: 1400   score: 3.0   memory length: 269262   epsilon: 0.5658592600094248    steps: 268     evaluation reward: 1.3\n",
      "episode: 1401   score: 2.0   memory length: 269463   epsilon: 0.5654612800094334    steps: 201     evaluation reward: 1.32\n",
      "episode: 1402   score: 4.0   memory length: 269758   epsilon: 0.5648771800094461    steps: 295     evaluation reward: 1.34\n",
      "episode: 1403   score: 2.0   memory length: 269957   epsilon: 0.5644831600094546    steps: 199     evaluation reward: 1.35\n",
      "episode: 1404   score: 1.0   memory length: 270125   epsilon: 0.5641505200094619    steps: 168     evaluation reward: 1.35\n",
      "episode: 1405   score: 1.0   memory length: 270298   epsilon: 0.5638079800094693    steps: 173     evaluation reward: 1.34\n",
      "episode: 1406   score: 0.0   memory length: 270426   epsilon: 0.5635545400094748    steps: 128     evaluation reward: 1.33\n",
      "episode: 1407   score: 3.0   memory length: 270671   epsilon: 0.5630694400094853    steps: 245     evaluation reward: 1.35\n",
      "episode: 1408   score: 0.0   memory length: 270793   epsilon: 0.5628278800094906    steps: 122     evaluation reward: 1.33\n",
      "episode: 1409   score: 0.0   memory length: 270916   epsilon: 0.5625843400094959    steps: 123     evaluation reward: 1.31\n",
      "episode: 1410   score: 1.0   memory length: 271090   epsilon: 0.5622398200095033    steps: 174     evaluation reward: 1.32\n",
      "episode: 1411   score: 1.0   memory length: 271242   epsilon: 0.5619388600095099    steps: 152     evaluation reward: 1.32\n",
      "episode: 1412   score: 4.0   memory length: 271503   epsilon: 0.5614220800095211    steps: 261     evaluation reward: 1.35\n",
      "episode: 1413   score: 2.0   memory length: 271724   epsilon: 0.5609845000095306    steps: 221     evaluation reward: 1.37\n",
      "episode: 1414   score: 1.0   memory length: 271875   epsilon: 0.5606855200095371    steps: 151     evaluation reward: 1.36\n",
      "episode: 1415   score: 4.0   memory length: 272121   epsilon: 0.5601984400095477    steps: 246     evaluation reward: 1.38\n",
      "episode: 1416   score: 2.0   memory length: 272342   epsilon: 0.5597608600095572    steps: 221     evaluation reward: 1.4\n",
      "episode: 1417   score: 0.0   memory length: 272464   epsilon: 0.5595193000095624    steps: 122     evaluation reward: 1.36\n",
      "episode: 1418   score: 0.0   memory length: 272586   epsilon: 0.5592777400095676    steps: 122     evaluation reward: 1.35\n",
      "episode: 1419   score: 2.0   memory length: 272808   epsilon: 0.5588381800095772    steps: 222     evaluation reward: 1.36\n",
      "episode: 1420   score: 1.0   memory length: 272980   epsilon: 0.5584976200095846    steps: 172     evaluation reward: 1.37\n",
      "episode: 1421   score: 0.0   memory length: 273106   epsilon: 0.55824814000959    steps: 126     evaluation reward: 1.37\n",
      "episode: 1422   score: 3.0   memory length: 273375   epsilon: 0.5577155200096016    steps: 269     evaluation reward: 1.36\n",
      "episode: 1423   score: 3.0   memory length: 273621   epsilon: 0.5572284400096121    steps: 246     evaluation reward: 1.37\n",
      "episode: 1424   score: 2.0   memory length: 273844   epsilon: 0.5567869000096217    steps: 223     evaluation reward: 1.37\n",
      "episode: 1425   score: 2.0   memory length: 274024   epsilon: 0.5564305000096295    steps: 180     evaluation reward: 1.36\n",
      "episode: 1426   score: 1.0   memory length: 274196   epsilon: 0.5560899400096369    steps: 172     evaluation reward: 1.36\n",
      "episode: 1427   score: 0.0   memory length: 274321   epsilon: 0.5558424400096422    steps: 125     evaluation reward: 1.36\n",
      "episode: 1428   score: 2.0   memory length: 274518   epsilon: 0.5554523800096507    steps: 197     evaluation reward: 1.36\n",
      "episode: 1429   score: 0.0   memory length: 274642   epsilon: 0.555206860009656    steps: 124     evaluation reward: 1.36\n",
      "episode: 1430   score: 3.0   memory length: 274871   epsilon: 0.5547534400096659    steps: 229     evaluation reward: 1.39\n",
      "episode: 1431   score: 0.0   memory length: 274995   epsilon: 0.5545079200096712    steps: 124     evaluation reward: 1.39\n",
      "episode: 1432   score: 0.0   memory length: 275121   epsilon: 0.5542584400096766    steps: 126     evaluation reward: 1.38\n",
      "episode: 1433   score: 1.0   memory length: 275292   epsilon: 0.553919860009684    steps: 171     evaluation reward: 1.36\n",
      "episode: 1434   score: 1.0   memory length: 275466   epsilon: 0.5535753400096914    steps: 174     evaluation reward: 1.37\n",
      "episode: 1435   score: 1.0   memory length: 275616   epsilon: 0.5532783400096979    steps: 150     evaluation reward: 1.36\n",
      "episode: 1436   score: 5.0   memory length: 275943   epsilon: 0.5526308800097119    steps: 327     evaluation reward: 1.41\n",
      "episode: 1437   score: 1.0   memory length: 276114   epsilon: 0.5522923000097193    steps: 171     evaluation reward: 1.41\n",
      "episode: 1438   score: 0.0   memory length: 276236   epsilon: 0.5520507400097245    steps: 122     evaluation reward: 1.38\n",
      "episode: 1439   score: 0.0   memory length: 276364   epsilon: 0.55179730000973    steps: 128     evaluation reward: 1.38\n",
      "episode: 1440   score: 2.0   memory length: 276562   epsilon: 0.5514052600097386    steps: 198     evaluation reward: 1.38\n",
      "episode: 1441   score: 1.0   memory length: 276716   epsilon: 0.5511003400097452    steps: 154     evaluation reward: 1.38\n",
      "episode: 1442   score: 2.0   memory length: 276916   epsilon: 0.5507043400097538    steps: 200     evaluation reward: 1.38\n",
      "episode: 1443   score: 4.0   memory length: 277194   epsilon: 0.5501539000097657    steps: 278     evaluation reward: 1.4\n",
      "episode: 1444   score: 0.0   memory length: 277320   epsilon: 0.5499044200097711    steps: 126     evaluation reward: 1.39\n",
      "episode: 1445   score: 0.0   memory length: 277448   epsilon: 0.5496509800097766    steps: 128     evaluation reward: 1.37\n",
      "episode: 1446   score: 3.0   memory length: 277691   epsilon: 0.5491698400097871    steps: 243     evaluation reward: 1.4\n",
      "episode: 1447   score: 1.0   memory length: 277859   epsilon: 0.5488372000097943    steps: 168     evaluation reward: 1.41\n",
      "episode: 1448   score: 1.0   memory length: 278030   epsilon: 0.5484986200098017    steps: 171     evaluation reward: 1.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1449   score: 2.0   memory length: 278228   epsilon: 0.5481065800098102    steps: 198     evaluation reward: 1.39\n",
      "episode: 1450   score: 0.0   memory length: 278352   epsilon: 0.5478610600098155    steps: 124     evaluation reward: 1.36\n",
      "episode: 1451   score: 2.0   memory length: 278549   epsilon: 0.547471000009824    steps: 197     evaluation reward: 1.37\n",
      "episode: 1452   score: 2.0   memory length: 278769   epsilon: 0.5470354000098334    steps: 220     evaluation reward: 1.37\n",
      "episode: 1453   score: 0.0   memory length: 278893   epsilon: 0.5467898800098387    steps: 124     evaluation reward: 1.34\n",
      "episode: 1454   score: 0.0   memory length: 279017   epsilon: 0.5465443600098441    steps: 124     evaluation reward: 1.34\n",
      "episode: 1455   score: 1.0   memory length: 279186   epsilon: 0.5462097400098513    steps: 169     evaluation reward: 1.31\n",
      "episode: 1456   score: 3.0   memory length: 279459   epsilon: 0.5456692000098631    steps: 273     evaluation reward: 1.33\n",
      "episode: 1457   score: 0.0   memory length: 279583   epsilon: 0.5454236800098684    steps: 124     evaluation reward: 1.31\n",
      "episode: 1458   score: 1.0   memory length: 279753   epsilon: 0.5450870800098757    steps: 170     evaluation reward: 1.32\n",
      "episode: 1459   score: 2.0   memory length: 279934   epsilon: 0.5447287000098835    steps: 181     evaluation reward: 1.32\n",
      "episode: 1460   score: 4.0   memory length: 280198   epsilon: 0.5442059800098948    steps: 264     evaluation reward: 1.36\n",
      "episode: 1461   score: 0.0   memory length: 280322   epsilon: 0.5439604600099002    steps: 124     evaluation reward: 1.33\n",
      "episode: 1462   score: 2.0   memory length: 280540   epsilon: 0.5435288200099095    steps: 218     evaluation reward: 1.32\n",
      "episode: 1463   score: 0.0   memory length: 280664   epsilon: 0.5432833000099149    steps: 124     evaluation reward: 1.32\n",
      "episode: 1464   score: 0.0   memory length: 280788   epsilon: 0.5430377800099202    steps: 124     evaluation reward: 1.32\n",
      "episode: 1465   score: 0.0   memory length: 280912   epsilon: 0.5427922600099255    steps: 124     evaluation reward: 1.32\n",
      "episode: 1466   score: 2.0   memory length: 281112   epsilon: 0.5423962600099341    steps: 200     evaluation reward: 1.33\n",
      "episode: 1467   score: 0.0   memory length: 281236   epsilon: 0.5421507400099395    steps: 124     evaluation reward: 1.31\n",
      "episode: 1468   score: 2.0   memory length: 281435   epsilon: 0.541756720009948    steps: 199     evaluation reward: 1.31\n",
      "episode: 1469   score: 1.0   memory length: 281607   epsilon: 0.5414161600099554    steps: 172     evaluation reward: 1.32\n",
      "episode: 1470   score: 3.0   memory length: 281852   epsilon: 0.5409310600099659    steps: 245     evaluation reward: 1.34\n",
      "episode: 1471   score: 2.0   memory length: 282051   epsilon: 0.5405370400099745    steps: 199     evaluation reward: 1.36\n",
      "episode: 1472   score: 0.0   memory length: 282175   epsilon: 0.5402915200099798    steps: 124     evaluation reward: 1.34\n",
      "episode: 1473   score: 0.0   memory length: 282298   epsilon: 0.5400479800099851    steps: 123     evaluation reward: 1.32\n",
      "episode: 1474   score: 5.0   memory length: 282642   epsilon: 0.5393668600099999    steps: 344     evaluation reward: 1.36\n",
      "episode: 1475   score: 0.0   memory length: 282766   epsilon: 0.5391213400100052    steps: 124     evaluation reward: 1.35\n",
      "episode: 1476   score: 0.0   memory length: 282891   epsilon: 0.5388738400100106    steps: 125     evaluation reward: 1.31\n",
      "episode: 1477   score: 2.0   memory length: 283091   epsilon: 0.5384778400100192    steps: 200     evaluation reward: 1.33\n",
      "episode: 1478   score: 2.0   memory length: 283276   epsilon: 0.5381115400100271    steps: 185     evaluation reward: 1.34\n",
      "episode: 1479   score: 1.0   memory length: 283446   epsilon: 0.5377749400100345    steps: 170     evaluation reward: 1.33\n",
      "episode: 1480   score: 0.0   memory length: 283571   epsilon: 0.5375274400100398    steps: 125     evaluation reward: 1.32\n",
      "episode: 1481   score: 0.0   memory length: 283693   epsilon: 0.5372858800100451    steps: 122     evaluation reward: 1.31\n",
      "episode: 1482   score: 1.0   memory length: 283845   epsilon: 0.5369849200100516    steps: 152     evaluation reward: 1.3\n",
      "episode: 1483   score: 0.0   memory length: 283968   epsilon: 0.5367413800100569    steps: 123     evaluation reward: 1.26\n",
      "episode: 1484   score: 2.0   memory length: 284166   epsilon: 0.5363493400100654    steps: 198     evaluation reward: 1.26\n",
      "episode: 1485   score: 0.0   memory length: 284293   epsilon: 0.5360978800100709    steps: 127     evaluation reward: 1.26\n",
      "episode: 1486   score: 1.0   memory length: 284444   epsilon: 0.5357989000100774    steps: 151     evaluation reward: 1.27\n",
      "episode: 1487   score: 0.0   memory length: 284567   epsilon: 0.5355553600100826    steps: 123     evaluation reward: 1.27\n",
      "episode: 1488   score: 0.0   memory length: 284689   epsilon: 0.5353138000100879    steps: 122     evaluation reward: 1.24\n",
      "episode: 1489   score: 1.0   memory length: 284858   epsilon: 0.5349791800100951    steps: 169     evaluation reward: 1.24\n",
      "episode: 1490   score: 1.0   memory length: 285029   epsilon: 0.5346406000101025    steps: 171     evaluation reward: 1.24\n",
      "episode: 1491   score: 2.0   memory length: 285227   epsilon: 0.534248560010111    steps: 198     evaluation reward: 1.25\n",
      "episode: 1492   score: 0.0   memory length: 285353   epsilon: 0.5339990800101164    steps: 126     evaluation reward: 1.25\n",
      "episode: 1493   score: 1.0   memory length: 285525   epsilon: 0.5336585200101238    steps: 172     evaluation reward: 1.25\n",
      "episode: 1494   score: 0.0   memory length: 285648   epsilon: 0.5334149800101291    steps: 123     evaluation reward: 1.25\n",
      "episode: 1495   score: 3.0   memory length: 285875   epsilon: 0.5329655200101389    steps: 227     evaluation reward: 1.28\n",
      "episode: 1496   score: 2.0   memory length: 286074   epsilon: 0.5325715000101474    steps: 199     evaluation reward: 1.28\n",
      "episode: 1497   score: 1.0   memory length: 286245   epsilon: 0.5322329200101548    steps: 171     evaluation reward: 1.28\n",
      "episode: 1498   score: 0.0   memory length: 286368   epsilon: 0.53198938001016    steps: 123     evaluation reward: 1.27\n",
      "episode: 1499   score: 0.0   memory length: 286493   epsilon: 0.5317418800101654    steps: 125     evaluation reward: 1.27\n",
      "episode: 1500   score: 1.0   memory length: 286664   epsilon: 0.5314033000101728    steps: 171     evaluation reward: 1.25\n",
      "episode: 1501   score: 4.0   memory length: 286967   epsilon: 0.5308033600101858    steps: 303     evaluation reward: 1.27\n",
      "episode: 1502   score: 1.0   memory length: 287120   epsilon: 0.5305004200101924    steps: 153     evaluation reward: 1.24\n",
      "episode: 1503   score: 1.0   memory length: 287290   epsilon: 0.5301638200101997    steps: 170     evaluation reward: 1.23\n",
      "episode: 1504   score: 3.0   memory length: 287539   epsilon: 0.5296708000102104    steps: 249     evaluation reward: 1.25\n",
      "episode: 1505   score: 0.0   memory length: 287665   epsilon: 0.5294213200102158    steps: 126     evaluation reward: 1.24\n",
      "episode: 1506   score: 1.0   memory length: 287834   epsilon: 0.5290867000102231    steps: 169     evaluation reward: 1.25\n",
      "episode: 1507   score: 2.0   memory length: 288031   epsilon: 0.5286966400102315    steps: 197     evaluation reward: 1.24\n",
      "episode: 1508   score: 0.0   memory length: 288154   epsilon: 0.5284531000102368    steps: 123     evaluation reward: 1.24\n",
      "episode: 1509   score: 3.0   memory length: 288427   epsilon: 0.5279125600102486    steps: 273     evaluation reward: 1.27\n",
      "episode: 1510   score: 3.0   memory length: 288656   epsilon: 0.5274591400102584    steps: 229     evaluation reward: 1.29\n",
      "episode: 1511   score: 3.0   memory length: 288904   epsilon: 0.5269681000102691    steps: 248     evaluation reward: 1.31\n",
      "episode: 1512   score: 1.0   memory length: 289074   epsilon: 0.5266315000102764    steps: 170     evaluation reward: 1.28\n",
      "episode: 1513   score: 5.0   memory length: 289405   epsilon: 0.5259761200102906    steps: 331     evaluation reward: 1.31\n",
      "episode: 1514   score: 2.0   memory length: 289604   epsilon: 0.5255821000102991    steps: 199     evaluation reward: 1.32\n",
      "episode: 1515   score: 2.0   memory length: 289801   epsilon: 0.5251920400103076    steps: 197     evaluation reward: 1.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1516   score: 2.0   memory length: 290004   epsilon: 0.5247901000103163    steps: 203     evaluation reward: 1.3\n",
      "episode: 1517   score: 2.0   memory length: 290202   epsilon: 0.5243980600103249    steps: 198     evaluation reward: 1.32\n",
      "episode: 1518   score: 0.0   memory length: 290326   epsilon: 0.5241525400103302    steps: 124     evaluation reward: 1.32\n",
      "episode: 1519   score: 1.0   memory length: 290495   epsilon: 0.5238179200103374    steps: 169     evaluation reward: 1.31\n",
      "episode: 1520   score: 2.0   memory length: 290696   epsilon: 0.5234199400103461    steps: 201     evaluation reward: 1.32\n",
      "episode: 1521   score: 1.0   memory length: 290847   epsilon: 0.5231209600103526    steps: 151     evaluation reward: 1.33\n",
      "episode: 1522   score: 3.0   memory length: 291119   epsilon: 0.5225824000103643    steps: 272     evaluation reward: 1.33\n",
      "episode: 1523   score: 2.0   memory length: 291316   epsilon: 0.5221923400103727    steps: 197     evaluation reward: 1.32\n",
      "episode: 1524   score: 4.0   memory length: 291611   epsilon: 0.5216082400103854    steps: 295     evaluation reward: 1.34\n",
      "episode: 1525   score: 4.0   memory length: 291906   epsilon: 0.5210241400103981    steps: 295     evaluation reward: 1.36\n",
      "episode: 1526   score: 1.0   memory length: 292078   epsilon: 0.5206835800104055    steps: 172     evaluation reward: 1.36\n",
      "episode: 1527   score: 1.0   memory length: 292248   epsilon: 0.5203469800104128    steps: 170     evaluation reward: 1.37\n",
      "episode: 1528   score: 0.0   memory length: 292372   epsilon: 0.5201014600104181    steps: 124     evaluation reward: 1.35\n",
      "episode: 1529   score: 4.0   memory length: 292632   epsilon: 0.5195866600104293    steps: 260     evaluation reward: 1.39\n",
      "episode: 1530   score: 4.0   memory length: 292926   epsilon: 0.5190045400104419    steps: 294     evaluation reward: 1.4\n",
      "episode: 1531   score: 0.0   memory length: 293051   epsilon: 0.5187570400104473    steps: 125     evaluation reward: 1.4\n",
      "episode: 1532   score: 1.0   memory length: 293203   epsilon: 0.5184560800104538    steps: 152     evaluation reward: 1.41\n",
      "episode: 1533   score: 1.0   memory length: 293355   epsilon: 0.5181551200104604    steps: 152     evaluation reward: 1.41\n",
      "episode: 1534   score: 1.0   memory length: 293524   epsilon: 0.5178205000104676    steps: 169     evaluation reward: 1.41\n",
      "episode: 1535   score: 1.0   memory length: 293676   epsilon: 0.5175195400104742    steps: 152     evaluation reward: 1.41\n",
      "episode: 1536   score: 1.0   memory length: 293848   epsilon: 0.5171789800104816    steps: 172     evaluation reward: 1.37\n",
      "episode: 1537   score: 1.0   memory length: 293999   epsilon: 0.5168800000104881    steps: 151     evaluation reward: 1.37\n",
      "episode: 1538   score: 2.0   memory length: 294218   epsilon: 0.5164463800104975    steps: 219     evaluation reward: 1.39\n",
      "episode: 1539   score: 3.0   memory length: 294462   epsilon: 0.515963260010508    steps: 244     evaluation reward: 1.42\n",
      "episode: 1540   score: 2.0   memory length: 294660   epsilon: 0.5155712200105165    steps: 198     evaluation reward: 1.42\n",
      "episode: 1541   score: 1.0   memory length: 294812   epsilon: 0.515270260010523    steps: 152     evaluation reward: 1.42\n",
      "episode: 1542   score: 2.0   memory length: 295012   epsilon: 0.5148742600105316    steps: 200     evaluation reward: 1.42\n",
      "episode: 1543   score: 1.0   memory length: 295184   epsilon: 0.514533700010539    steps: 172     evaluation reward: 1.39\n",
      "episode: 1544   score: 1.0   memory length: 295355   epsilon: 0.5141951200105463    steps: 171     evaluation reward: 1.4\n",
      "episode: 1545   score: 2.0   memory length: 295553   epsilon: 0.5138030800105549    steps: 198     evaluation reward: 1.42\n",
      "episode: 1546   score: 2.0   memory length: 295771   epsilon: 0.5133714400105642    steps: 218     evaluation reward: 1.41\n",
      "episode: 1547   score: 0.0   memory length: 295898   epsilon: 0.5131199800105697    steps: 127     evaluation reward: 1.4\n",
      "episode: 1548   score: 0.0   memory length: 296027   epsilon: 0.5128645600105752    steps: 129     evaluation reward: 1.39\n",
      "episode: 1549   score: 1.0   memory length: 296196   epsilon: 0.5125299400105825    steps: 169     evaluation reward: 1.38\n",
      "episode: 1550   score: 2.0   memory length: 296414   epsilon: 0.5120983000105919    steps: 218     evaluation reward: 1.4\n",
      "episode: 1551   score: 1.0   memory length: 296584   epsilon: 0.5117617000105992    steps: 170     evaluation reward: 1.39\n",
      "episode: 1552   score: 3.0   memory length: 296832   epsilon: 0.5112706600106098    steps: 248     evaluation reward: 1.4\n",
      "episode: 1553   score: 4.0   memory length: 297126   epsilon: 0.5106885400106225    steps: 294     evaluation reward: 1.44\n",
      "episode: 1554   score: 1.0   memory length: 297295   epsilon: 0.5103539200106297    steps: 169     evaluation reward: 1.45\n",
      "episode: 1555   score: 3.0   memory length: 297509   epsilon: 0.5099302000106389    steps: 214     evaluation reward: 1.47\n",
      "episode: 1556   score: 1.0   memory length: 297661   epsilon: 0.5096292400106455    steps: 152     evaluation reward: 1.45\n",
      "episode: 1557   score: 0.0   memory length: 297784   epsilon: 0.5093857000106508    steps: 123     evaluation reward: 1.45\n",
      "episode: 1558   score: 1.0   memory length: 297955   epsilon: 0.5090471200106581    steps: 171     evaluation reward: 1.45\n",
      "episode: 1559   score: 0.0   memory length: 298078   epsilon: 0.5088035800106634    steps: 123     evaluation reward: 1.43\n",
      "episode: 1560   score: 0.0   memory length: 298205   epsilon: 0.5085521200106689    steps: 127     evaluation reward: 1.39\n",
      "episode: 1561   score: 0.0   memory length: 298329   epsilon: 0.5083066000106742    steps: 124     evaluation reward: 1.39\n",
      "episode: 1562   score: 0.0   memory length: 298451   epsilon: 0.5080650400106794    steps: 122     evaluation reward: 1.37\n",
      "episode: 1563   score: 0.0   memory length: 298575   epsilon: 0.5078195200106848    steps: 124     evaluation reward: 1.37\n",
      "episode: 1564   score: 2.0   memory length: 298775   epsilon: 0.5074235200106934    steps: 200     evaluation reward: 1.39\n",
      "episode: 1565   score: 2.0   memory length: 298973   epsilon: 0.5070314800107019    steps: 198     evaluation reward: 1.41\n",
      "episode: 1566   score: 0.0   memory length: 299096   epsilon: 0.5067879400107071    steps: 123     evaluation reward: 1.39\n",
      "episode: 1567   score: 3.0   memory length: 299340   epsilon: 0.5063048200107176    steps: 244     evaluation reward: 1.42\n",
      "episode: 1568   score: 1.0   memory length: 299516   epsilon: 0.5059563400107252    steps: 176     evaluation reward: 1.41\n",
      "episode: 1569   score: 0.0   memory length: 299639   epsilon: 0.5057128000107305    steps: 123     evaluation reward: 1.4\n",
      "episode: 1570   score: 1.0   memory length: 299808   epsilon: 0.5053781800107378    steps: 169     evaluation reward: 1.38\n",
      "now time :  2019-09-25 17:14:46.121575\n",
      "episode: 1571   score: 2.0   memory length: 300008   epsilon: 0.5049821800107464    steps: 200     evaluation reward: 1.38\n",
      "episode: 1572   score: 0.0   memory length: 300130   epsilon: 0.5047406200107516    steps: 122     evaluation reward: 1.38\n",
      "episode: 1573   score: 0.0   memory length: 300252   epsilon: 0.5044990600107568    steps: 122     evaluation reward: 1.38\n",
      "episode: 1574   score: 0.0   memory length: 300374   epsilon: 0.5042575000107621    steps: 122     evaluation reward: 1.33\n",
      "episode: 1575   score: 1.0   memory length: 300546   epsilon: 0.5039169400107695    steps: 172     evaluation reward: 1.34\n",
      "episode: 1576   score: 1.0   memory length: 300700   epsilon: 0.5036120200107761    steps: 154     evaluation reward: 1.35\n",
      "episode: 1577   score: 1.0   memory length: 300851   epsilon: 0.5033130400107826    steps: 151     evaluation reward: 1.34\n",
      "episode: 1578   score: 0.0   memory length: 300977   epsilon: 0.503063560010788    steps: 126     evaluation reward: 1.32\n",
      "episode: 1579   score: 0.0   memory length: 301100   epsilon: 0.5028200200107933    steps: 123     evaluation reward: 1.31\n",
      "episode: 1580   score: 0.0   memory length: 301222   epsilon: 0.5025784600107985    steps: 122     evaluation reward: 1.31\n",
      "episode: 1581   score: 1.0   memory length: 301373   epsilon: 0.502279480010805    steps: 151     evaluation reward: 1.32\n",
      "episode: 1582   score: 2.0   memory length: 301573   epsilon: 0.5018834800108136    steps: 200     evaluation reward: 1.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1583   score: 0.0   memory length: 301695   epsilon: 0.5016419200108189    steps: 122     evaluation reward: 1.33\n",
      "episode: 1584   score: 1.0   memory length: 301870   epsilon: 0.5012954200108264    steps: 175     evaluation reward: 1.32\n",
      "episode: 1585   score: 2.0   memory length: 302072   epsilon: 0.5008954600108351    steps: 202     evaluation reward: 1.34\n",
      "episode: 1586   score: 1.0   memory length: 302226   epsilon: 0.5005905400108417    steps: 154     evaluation reward: 1.34\n",
      "episode: 1587   score: 2.0   memory length: 302444   epsilon: 0.5001589000108511    steps: 218     evaluation reward: 1.36\n",
      "episode: 1588   score: 1.0   memory length: 302597   epsilon: 0.4998559600108536    steps: 153     evaluation reward: 1.37\n",
      "episode: 1589   score: 4.0   memory length: 302894   epsilon: 0.49926790001084986    steps: 297     evaluation reward: 1.4\n",
      "episode: 1590   score: 1.0   memory length: 303049   epsilon: 0.4989610000108479    steps: 155     evaluation reward: 1.4\n",
      "episode: 1591   score: 3.0   memory length: 303317   epsilon: 0.49843036001084456    steps: 268     evaluation reward: 1.41\n",
      "episode: 1592   score: 3.0   memory length: 303544   epsilon: 0.4979809000108417    steps: 227     evaluation reward: 1.44\n",
      "episode: 1593   score: 2.0   memory length: 303742   epsilon: 0.49758886001083924    steps: 198     evaluation reward: 1.45\n",
      "episode: 1594   score: 0.0   memory length: 303865   epsilon: 0.4973453200108377    steps: 123     evaluation reward: 1.45\n",
      "episode: 1595   score: 3.0   memory length: 304128   epsilon: 0.4968245800108344    steps: 263     evaluation reward: 1.45\n",
      "episode: 1596   score: 4.0   memory length: 304446   epsilon: 0.4961949400108304    steps: 318     evaluation reward: 1.47\n",
      "episode: 1597   score: 0.0   memory length: 304570   epsilon: 0.49594942001082887    steps: 124     evaluation reward: 1.46\n",
      "episode: 1598   score: 0.0   memory length: 304692   epsilon: 0.49570786001082734    steps: 122     evaluation reward: 1.46\n",
      "episode: 1599   score: 1.0   memory length: 304863   epsilon: 0.4953692800108252    steps: 171     evaluation reward: 1.47\n",
      "episode: 1600   score: 0.0   memory length: 304985   epsilon: 0.49512772001082367    steps: 122     evaluation reward: 1.46\n",
      "episode: 1601   score: 0.0   memory length: 305107   epsilon: 0.49488616001082214    steps: 122     evaluation reward: 1.42\n",
      "episode: 1602   score: 1.0   memory length: 305283   epsilon: 0.49453768001081994    steps: 176     evaluation reward: 1.42\n",
      "episode: 1603   score: 0.0   memory length: 305407   epsilon: 0.4942921600108184    steps: 124     evaluation reward: 1.41\n",
      "episode: 1604   score: 0.0   memory length: 305530   epsilon: 0.49404862001081684    steps: 123     evaluation reward: 1.38\n",
      "episode: 1605   score: 0.0   memory length: 305653   epsilon: 0.4938050800108153    steps: 123     evaluation reward: 1.38\n",
      "episode: 1606   score: 2.0   memory length: 305873   epsilon: 0.49336948001081254    steps: 220     evaluation reward: 1.39\n",
      "episode: 1607   score: 0.0   memory length: 305997   epsilon: 0.493123960010811    steps: 124     evaluation reward: 1.37\n",
      "episode: 1608   score: 3.0   memory length: 306245   epsilon: 0.4926329200108079    steps: 248     evaluation reward: 1.4\n",
      "episode: 1609   score: 1.0   memory length: 306396   epsilon: 0.492333940010806    steps: 151     evaluation reward: 1.38\n",
      "episode: 1610   score: 0.0   memory length: 306518   epsilon: 0.49209238001080446    steps: 122     evaluation reward: 1.35\n",
      "episode: 1611   score: 0.0   memory length: 306640   epsilon: 0.49185082001080294    steps: 122     evaluation reward: 1.32\n",
      "episode: 1612   score: 1.0   memory length: 306809   epsilon: 0.4915162000108008    steps: 169     evaluation reward: 1.32\n",
      "episode: 1613   score: 3.0   memory length: 307076   epsilon: 0.4909875400107975    steps: 267     evaluation reward: 1.3\n",
      "episode: 1614   score: 0.0   memory length: 307198   epsilon: 0.49074598001079595    steps: 122     evaluation reward: 1.28\n",
      "episode: 1615   score: 0.0   memory length: 307322   epsilon: 0.4905004600107944    steps: 124     evaluation reward: 1.26\n",
      "episode: 1616   score: 1.0   memory length: 307491   epsilon: 0.4901658400107923    steps: 169     evaluation reward: 1.25\n",
      "episode: 1617   score: 2.0   memory length: 307688   epsilon: 0.4897757800107898    steps: 197     evaluation reward: 1.25\n",
      "episode: 1618   score: 1.0   memory length: 307858   epsilon: 0.4894391800107877    steps: 170     evaluation reward: 1.26\n",
      "episode: 1619   score: 1.0   memory length: 308011   epsilon: 0.48913624001078576    steps: 153     evaluation reward: 1.26\n",
      "episode: 1620   score: 1.0   memory length: 308162   epsilon: 0.48883726001078387    steps: 151     evaluation reward: 1.25\n",
      "episode: 1621   score: 1.0   memory length: 308312   epsilon: 0.488540260010782    steps: 150     evaluation reward: 1.25\n",
      "episode: 1622   score: 2.0   memory length: 308531   epsilon: 0.48810664001077925    steps: 219     evaluation reward: 1.24\n",
      "episode: 1623   score: 0.0   memory length: 308654   epsilon: 0.4878631000107777    steps: 123     evaluation reward: 1.22\n",
      "episode: 1624   score: 0.0   memory length: 308776   epsilon: 0.4876215400107762    steps: 122     evaluation reward: 1.18\n",
      "episode: 1625   score: 0.0   memory length: 308899   epsilon: 0.48737800001077464    steps: 123     evaluation reward: 1.14\n",
      "episode: 1626   score: 1.0   memory length: 309070   epsilon: 0.4870394200107725    steps: 171     evaluation reward: 1.14\n",
      "episode: 1627   score: 0.0   memory length: 309194   epsilon: 0.48679390001077094    steps: 124     evaluation reward: 1.13\n",
      "episode: 1628   score: 2.0   memory length: 309397   epsilon: 0.4863919600107684    steps: 203     evaluation reward: 1.15\n",
      "episode: 1629   score: 0.0   memory length: 309521   epsilon: 0.48614644001076684    steps: 124     evaluation reward: 1.11\n",
      "episode: 1630   score: 2.0   memory length: 309745   epsilon: 0.48570292001076404    steps: 224     evaluation reward: 1.09\n",
      "episode: 1631   score: 0.0   memory length: 309869   epsilon: 0.4854574000107625    steps: 124     evaluation reward: 1.09\n",
      "episode: 1632   score: 3.0   memory length: 310095   epsilon: 0.48500992001075965    steps: 226     evaluation reward: 1.11\n",
      "episode: 1633   score: 1.0   memory length: 310246   epsilon: 0.48471094001075776    steps: 151     evaluation reward: 1.11\n",
      "episode: 1634   score: 3.0   memory length: 310474   epsilon: 0.4842595000107549    steps: 228     evaluation reward: 1.13\n",
      "episode: 1635   score: 1.0   memory length: 310628   epsilon: 0.483954580010753    steps: 154     evaluation reward: 1.13\n",
      "episode: 1636   score: 1.0   memory length: 310778   epsilon: 0.4836575800107511    steps: 150     evaluation reward: 1.13\n",
      "episode: 1637   score: 2.0   memory length: 310976   epsilon: 0.4832655400107486    steps: 198     evaluation reward: 1.14\n",
      "episode: 1638   score: 1.0   memory length: 311126   epsilon: 0.48296854001074674    steps: 150     evaluation reward: 1.13\n",
      "episode: 1639   score: 2.0   memory length: 311347   epsilon: 0.48253096001074397    steps: 221     evaluation reward: 1.12\n",
      "episode: 1640   score: 2.0   memory length: 311532   epsilon: 0.48216466001074165    steps: 185     evaluation reward: 1.12\n",
      "episode: 1641   score: 2.0   memory length: 311731   epsilon: 0.48177064001073916    steps: 199     evaluation reward: 1.13\n",
      "episode: 1642   score: 1.0   memory length: 311881   epsilon: 0.4814736400107373    steps: 150     evaluation reward: 1.12\n",
      "episode: 1643   score: 2.0   memory length: 312079   epsilon: 0.4810816000107348    steps: 198     evaluation reward: 1.13\n",
      "episode: 1644   score: 2.0   memory length: 312278   epsilon: 0.4806875800107323    steps: 199     evaluation reward: 1.14\n",
      "episode: 1645   score: 2.0   memory length: 312477   epsilon: 0.4802935600107298    steps: 199     evaluation reward: 1.14\n",
      "episode: 1646   score: 3.0   memory length: 312723   epsilon: 0.47980648001072673    steps: 246     evaluation reward: 1.15\n",
      "episode: 1647   score: 0.0   memory length: 312845   epsilon: 0.4795649200107252    steps: 122     evaluation reward: 1.15\n",
      "episode: 1648   score: 2.0   memory length: 313042   epsilon: 0.47917486001072274    steps: 197     evaluation reward: 1.17\n",
      "episode: 1649   score: 1.0   memory length: 313194   epsilon: 0.47887390001072083    steps: 152     evaluation reward: 1.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1650   score: 2.0   memory length: 313392   epsilon: 0.47848186001071835    steps: 198     evaluation reward: 1.17\n",
      "episode: 1651   score: 2.0   memory length: 313589   epsilon: 0.4780918000107159    steps: 197     evaluation reward: 1.18\n",
      "episode: 1652   score: 4.0   memory length: 313846   epsilon: 0.47758294001071266    steps: 257     evaluation reward: 1.19\n",
      "episode: 1653   score: 0.0   memory length: 313969   epsilon: 0.4773394000107111    steps: 123     evaluation reward: 1.15\n",
      "episode: 1654   score: 0.0   memory length: 314093   epsilon: 0.47709388001070957    steps: 124     evaluation reward: 1.14\n",
      "episode: 1655   score: 0.0   memory length: 314215   epsilon: 0.47685232001070804    steps: 122     evaluation reward: 1.11\n",
      "episode: 1656   score: 0.0   memory length: 314338   epsilon: 0.4766087800107065    steps: 123     evaluation reward: 1.1\n",
      "episode: 1657   score: 2.0   memory length: 314519   epsilon: 0.47625040001070423    steps: 181     evaluation reward: 1.12\n",
      "episode: 1658   score: 0.0   memory length: 314641   epsilon: 0.4760088400107027    steps: 122     evaluation reward: 1.11\n",
      "episode: 1659   score: 2.0   memory length: 314838   epsilon: 0.47561878001070024    steps: 197     evaluation reward: 1.13\n",
      "episode: 1660   score: 2.0   memory length: 315038   epsilon: 0.47522278001069773    steps: 200     evaluation reward: 1.15\n",
      "episode: 1661   score: 0.0   memory length: 315160   epsilon: 0.4749812200106962    steps: 122     evaluation reward: 1.15\n",
      "episode: 1662   score: 0.0   memory length: 315283   epsilon: 0.47473768001069466    steps: 123     evaluation reward: 1.15\n",
      "episode: 1663   score: 0.0   memory length: 315405   epsilon: 0.47449612001069313    steps: 122     evaluation reward: 1.15\n",
      "episode: 1664   score: 1.0   memory length: 315578   epsilon: 0.47415358001069097    steps: 173     evaluation reward: 1.14\n",
      "episode: 1665   score: 1.0   memory length: 315729   epsilon: 0.4738546000106891    steps: 151     evaluation reward: 1.13\n",
      "episode: 1666   score: 2.0   memory length: 315930   epsilon: 0.47345662001068656    steps: 201     evaluation reward: 1.15\n",
      "episode: 1667   score: 1.0   memory length: 316082   epsilon: 0.47315566001068465    steps: 152     evaluation reward: 1.13\n",
      "episode: 1668   score: 4.0   memory length: 316340   epsilon: 0.4726448200106814    steps: 258     evaluation reward: 1.16\n",
      "episode: 1669   score: 2.0   memory length: 316538   epsilon: 0.47225278001067894    steps: 198     evaluation reward: 1.18\n",
      "episode: 1670   score: 0.0   memory length: 316660   epsilon: 0.4720112200106774    steps: 122     evaluation reward: 1.17\n",
      "episode: 1671   score: 1.0   memory length: 316813   epsilon: 0.4717082800106755    steps: 153     evaluation reward: 1.16\n",
      "episode: 1672   score: 2.0   memory length: 317011   epsilon: 0.471316240010673    steps: 198     evaluation reward: 1.18\n",
      "episode: 1673   score: 2.0   memory length: 317227   epsilon: 0.4708885600106703    steps: 216     evaluation reward: 1.2\n",
      "episode: 1674   score: 0.0   memory length: 317351   epsilon: 0.47064304001066876    steps: 124     evaluation reward: 1.2\n",
      "episode: 1675   score: 1.0   memory length: 317504   epsilon: 0.47034010001066684    steps: 153     evaluation reward: 1.2\n",
      "episode: 1676   score: 0.0   memory length: 317626   epsilon: 0.4700985400106653    steps: 122     evaluation reward: 1.19\n",
      "episode: 1677   score: 2.0   memory length: 317825   epsilon: 0.4697045200106628    steps: 199     evaluation reward: 1.2\n",
      "episode: 1678   score: 1.0   memory length: 317994   epsilon: 0.4693699000106607    steps: 169     evaluation reward: 1.21\n",
      "episode: 1679   score: 4.0   memory length: 318270   epsilon: 0.46882342001065724    steps: 276     evaluation reward: 1.25\n",
      "episode: 1680   score: 2.0   memory length: 318488   epsilon: 0.4683917800106545    steps: 218     evaluation reward: 1.27\n",
      "episode: 1681   score: 1.0   memory length: 318659   epsilon: 0.46805320001065237    steps: 171     evaluation reward: 1.27\n",
      "episode: 1682   score: 2.0   memory length: 318881   epsilon: 0.4676136400106496    steps: 222     evaluation reward: 1.27\n",
      "episode: 1683   score: 0.0   memory length: 319005   epsilon: 0.46736812001064804    steps: 124     evaluation reward: 1.27\n",
      "episode: 1684   score: 1.0   memory length: 319174   epsilon: 0.4670335000106459    steps: 169     evaluation reward: 1.27\n",
      "episode: 1685   score: 2.0   memory length: 319371   epsilon: 0.46664344001064345    steps: 197     evaluation reward: 1.27\n",
      "episode: 1686   score: 0.0   memory length: 319495   epsilon: 0.4663979200106419    steps: 124     evaluation reward: 1.26\n",
      "episode: 1687   score: 1.0   memory length: 319665   epsilon: 0.46606132001063977    steps: 170     evaluation reward: 1.25\n",
      "episode: 1688   score: 1.0   memory length: 319836   epsilon: 0.4657227400106376    steps: 171     evaluation reward: 1.25\n",
      "episode: 1689   score: 1.0   memory length: 320007   epsilon: 0.4653841600106355    steps: 171     evaluation reward: 1.22\n",
      "episode: 1690   score: 3.0   memory length: 320237   epsilon: 0.4649287600106326    steps: 230     evaluation reward: 1.24\n",
      "episode: 1691   score: 0.0   memory length: 320359   epsilon: 0.4646872000106311    steps: 122     evaluation reward: 1.21\n",
      "episode: 1692   score: 3.0   memory length: 320609   epsilon: 0.46419220001062794    steps: 250     evaluation reward: 1.21\n",
      "episode: 1693   score: 0.0   memory length: 320732   epsilon: 0.4639486600106264    steps: 123     evaluation reward: 1.19\n",
      "episode: 1694   score: 0.0   memory length: 320856   epsilon: 0.46370314001062485    steps: 124     evaluation reward: 1.19\n",
      "episode: 1695   score: 1.0   memory length: 321009   epsilon: 0.46340020001062293    steps: 153     evaluation reward: 1.17\n",
      "episode: 1696   score: 0.0   memory length: 321132   epsilon: 0.4631566600106214    steps: 123     evaluation reward: 1.13\n",
      "episode: 1697   score: 0.0   memory length: 321259   epsilon: 0.4629052000106198    steps: 127     evaluation reward: 1.13\n",
      "episode: 1698   score: 2.0   memory length: 321459   epsilon: 0.4625092000106173    steps: 200     evaluation reward: 1.15\n",
      "episode: 1699   score: 4.0   memory length: 321738   epsilon: 0.4619567800106138    steps: 279     evaluation reward: 1.18\n",
      "episode: 1700   score: 1.0   memory length: 321891   epsilon: 0.4616538400106119    steps: 153     evaluation reward: 1.19\n",
      "episode: 1701   score: 1.0   memory length: 322044   epsilon: 0.46135090001060997    steps: 153     evaluation reward: 1.2\n",
      "episode: 1702   score: 1.0   memory length: 322213   epsilon: 0.46101628001060785    steps: 169     evaluation reward: 1.2\n",
      "episode: 1703   score: 0.0   memory length: 322337   epsilon: 0.4607707600106063    steps: 124     evaluation reward: 1.2\n",
      "episode: 1704   score: 0.0   memory length: 322460   epsilon: 0.46052722001060475    steps: 123     evaluation reward: 1.2\n",
      "episode: 1705   score: 1.0   memory length: 322616   epsilon: 0.4602183400106028    steps: 156     evaluation reward: 1.21\n",
      "episode: 1706   score: 1.0   memory length: 322772   epsilon: 0.45990946001060085    steps: 156     evaluation reward: 1.2\n",
      "episode: 1707   score: 1.0   memory length: 322943   epsilon: 0.4595708800105987    steps: 171     evaluation reward: 1.21\n",
      "episode: 1708   score: 2.0   memory length: 323162   epsilon: 0.45913726001059596    steps: 219     evaluation reward: 1.2\n",
      "episode: 1709   score: 1.0   memory length: 323315   epsilon: 0.45883432001059404    steps: 153     evaluation reward: 1.2\n",
      "episode: 1710   score: 3.0   memory length: 323540   epsilon: 0.4583888200105912    steps: 225     evaluation reward: 1.23\n",
      "episode: 1711   score: 1.0   memory length: 323693   epsilon: 0.4580858800105893    steps: 153     evaluation reward: 1.24\n",
      "episode: 1712   score: 2.0   memory length: 323913   epsilon: 0.45765028001058655    steps: 220     evaluation reward: 1.25\n",
      "episode: 1713   score: 0.0   memory length: 324038   epsilon: 0.457402780010585    steps: 125     evaluation reward: 1.22\n",
      "episode: 1714   score: 2.0   memory length: 324238   epsilon: 0.4570067800105825    steps: 200     evaluation reward: 1.24\n",
      "episode: 1715   score: 1.0   memory length: 324390   epsilon: 0.4567058200105806    steps: 152     evaluation reward: 1.25\n",
      "episode: 1716   score: 2.0   memory length: 324587   epsilon: 0.4563157600105781    steps: 197     evaluation reward: 1.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1717   score: 1.0   memory length: 324738   epsilon: 0.4560167800105762    steps: 151     evaluation reward: 1.25\n",
      "episode: 1718   score: 1.0   memory length: 324888   epsilon: 0.45571978001057434    steps: 150     evaluation reward: 1.25\n",
      "episode: 1719   score: 2.0   memory length: 325108   epsilon: 0.4552841800105716    steps: 220     evaluation reward: 1.26\n",
      "episode: 1720   score: 0.0   memory length: 325232   epsilon: 0.45503866001057003    steps: 124     evaluation reward: 1.25\n",
      "episode: 1721   score: 0.0   memory length: 325357   epsilon: 0.45479116001056846    steps: 125     evaluation reward: 1.24\n",
      "episode: 1722   score: 3.0   memory length: 325603   epsilon: 0.4543040800105654    steps: 246     evaluation reward: 1.25\n",
      "episode: 1723   score: 2.0   memory length: 325789   epsilon: 0.45393580001056305    steps: 186     evaluation reward: 1.27\n",
      "episode: 1724   score: 0.0   memory length: 325911   epsilon: 0.4536942400105615    steps: 122     evaluation reward: 1.27\n",
      "episode: 1725   score: 0.0   memory length: 326035   epsilon: 0.45344872001055997    steps: 124     evaluation reward: 1.27\n",
      "episode: 1726   score: 0.0   memory length: 326162   epsilon: 0.4531972600105584    steps: 127     evaluation reward: 1.26\n",
      "episode: 1727   score: 2.0   memory length: 326359   epsilon: 0.4528072000105559    steps: 197     evaluation reward: 1.28\n",
      "episode: 1728   score: 4.0   memory length: 326615   epsilon: 0.4523003200105527    steps: 256     evaluation reward: 1.3\n",
      "episode: 1729   score: 1.0   memory length: 326767   epsilon: 0.4519993600105508    steps: 152     evaluation reward: 1.31\n",
      "episode: 1730   score: 2.0   memory length: 326964   epsilon: 0.45160930001054833    steps: 197     evaluation reward: 1.31\n",
      "episode: 1731   score: 0.0   memory length: 327086   epsilon: 0.4513677400105468    steps: 122     evaluation reward: 1.31\n",
      "episode: 1732   score: 1.0   memory length: 327236   epsilon: 0.4510707400105449    steps: 150     evaluation reward: 1.29\n",
      "episode: 1733   score: 3.0   memory length: 327483   epsilon: 0.45058168001054183    steps: 247     evaluation reward: 1.31\n",
      "episode: 1734   score: 3.0   memory length: 327729   epsilon: 0.45009460001053875    steps: 246     evaluation reward: 1.31\n",
      "episode: 1735   score: 0.0   memory length: 327851   epsilon: 0.4498530400105372    steps: 122     evaluation reward: 1.3\n",
      "episode: 1736   score: 0.0   memory length: 327974   epsilon: 0.4496095000105357    steps: 123     evaluation reward: 1.29\n",
      "episode: 1737   score: 0.0   memory length: 328100   epsilon: 0.4493600200105341    steps: 126     evaluation reward: 1.27\n",
      "episode: 1738   score: 0.0   memory length: 328224   epsilon: 0.44911450001053255    steps: 124     evaluation reward: 1.26\n",
      "episode: 1739   score: 0.0   memory length: 328346   epsilon: 0.448872940010531    steps: 122     evaluation reward: 1.24\n",
      "episode: 1740   score: 1.0   memory length: 328500   epsilon: 0.4485680200105291    steps: 154     evaluation reward: 1.23\n",
      "episode: 1741   score: 2.0   memory length: 328700   epsilon: 0.4481720200105266    steps: 200     evaluation reward: 1.23\n",
      "episode: 1742   score: 0.0   memory length: 328825   epsilon: 0.447924520010525    steps: 125     evaluation reward: 1.22\n",
      "episode: 1743   score: 0.0   memory length: 328948   epsilon: 0.4476809800105235    steps: 123     evaluation reward: 1.2\n",
      "episode: 1744   score: 0.0   memory length: 329072   epsilon: 0.4474354600105219    steps: 124     evaluation reward: 1.18\n",
      "episode: 1745   score: 0.0   memory length: 329197   epsilon: 0.44718796001052036    steps: 125     evaluation reward: 1.16\n",
      "episode: 1746   score: 2.0   memory length: 329395   epsilon: 0.4467959200105179    steps: 198     evaluation reward: 1.15\n",
      "episode: 1747   score: 0.0   memory length: 329518   epsilon: 0.44655238001051634    steps: 123     evaluation reward: 1.15\n",
      "episode: 1748   score: 0.0   memory length: 329641   epsilon: 0.4463088400105148    steps: 123     evaluation reward: 1.13\n",
      "episode: 1749   score: 0.0   memory length: 329764   epsilon: 0.44606530001051325    steps: 123     evaluation reward: 1.12\n",
      "episode: 1750   score: 0.0   memory length: 329886   epsilon: 0.4458237400105117    steps: 122     evaluation reward: 1.1\n",
      "episode: 1751   score: 2.0   memory length: 330084   epsilon: 0.44543170001050925    steps: 198     evaluation reward: 1.1\n",
      "episode: 1752   score: 1.0   memory length: 330235   epsilon: 0.44513272001050735    steps: 151     evaluation reward: 1.07\n",
      "episode: 1753   score: 3.0   memory length: 330480   epsilon: 0.4446476200105043    steps: 245     evaluation reward: 1.1\n",
      "episode: 1754   score: 0.0   memory length: 330604   epsilon: 0.44440210001050273    steps: 124     evaluation reward: 1.1\n",
      "episode: 1755   score: 1.0   memory length: 330755   epsilon: 0.44410312001050084    steps: 151     evaluation reward: 1.11\n",
      "episode: 1756   score: 0.0   memory length: 330878   epsilon: 0.4438595800104993    steps: 123     evaluation reward: 1.11\n",
      "episode: 1757   score: 1.0   memory length: 331048   epsilon: 0.44352298001049717    steps: 170     evaluation reward: 1.1\n",
      "episode: 1758   score: 1.0   memory length: 331201   epsilon: 0.44322004001049525    steps: 153     evaluation reward: 1.11\n",
      "episode: 1759   score: 1.0   memory length: 331352   epsilon: 0.44292106001049336    steps: 151     evaluation reward: 1.1\n",
      "episode: 1760   score: 2.0   memory length: 331552   epsilon: 0.44252506001049086    steps: 200     evaluation reward: 1.1\n",
      "episode: 1761   score: 1.0   memory length: 331706   epsilon: 0.4422201400104889    steps: 154     evaluation reward: 1.11\n",
      "episode: 1762   score: 4.0   memory length: 331983   epsilon: 0.44167168001048546    steps: 277     evaluation reward: 1.15\n",
      "episode: 1763   score: 1.0   memory length: 332156   epsilon: 0.4413291400104833    steps: 173     evaluation reward: 1.16\n",
      "episode: 1764   score: 1.0   memory length: 332306   epsilon: 0.4410321400104814    steps: 150     evaluation reward: 1.16\n",
      "episode: 1765   score: 1.0   memory length: 332459   epsilon: 0.4407292000104795    steps: 153     evaluation reward: 1.16\n",
      "episode: 1766   score: 0.0   memory length: 332583   epsilon: 0.44048368001047794    steps: 124     evaluation reward: 1.14\n",
      "episode: 1767   score: 1.0   memory length: 332734   epsilon: 0.44018470001047605    steps: 151     evaluation reward: 1.14\n",
      "episode: 1768   score: 2.0   memory length: 332932   epsilon: 0.43979266001047357    steps: 198     evaluation reward: 1.12\n",
      "episode: 1769   score: 0.0   memory length: 333055   epsilon: 0.439549120010472    steps: 123     evaluation reward: 1.1\n",
      "episode: 1770   score: 2.0   memory length: 333237   epsilon: 0.43918876001046975    steps: 182     evaluation reward: 1.12\n",
      "episode: 1771   score: 2.0   memory length: 333438   epsilon: 0.43879078001046723    steps: 201     evaluation reward: 1.13\n",
      "episode: 1772   score: 1.0   memory length: 333607   epsilon: 0.4384561600104651    steps: 169     evaluation reward: 1.12\n",
      "episode: 1773   score: 2.0   memory length: 333805   epsilon: 0.43806412001046263    steps: 198     evaluation reward: 1.12\n",
      "episode: 1774   score: 0.0   memory length: 333927   epsilon: 0.4378225600104611    steps: 122     evaluation reward: 1.12\n",
      "episode: 1775   score: 0.0   memory length: 334049   epsilon: 0.4375810000104596    steps: 122     evaluation reward: 1.11\n",
      "episode: 1776   score: 0.0   memory length: 334173   epsilon: 0.437335480010458    steps: 124     evaluation reward: 1.11\n",
      "episode: 1777   score: 3.0   memory length: 334386   epsilon: 0.43691374001045535    steps: 213     evaluation reward: 1.12\n",
      "episode: 1778   score: 0.0   memory length: 334510   epsilon: 0.4366682200104538    steps: 124     evaluation reward: 1.11\n",
      "episode: 1779   score: 2.0   memory length: 334707   epsilon: 0.43627816001045133    steps: 197     evaluation reward: 1.09\n",
      "episode: 1780   score: 1.0   memory length: 334860   epsilon: 0.4359752200104494    steps: 153     evaluation reward: 1.08\n",
      "episode: 1781   score: 2.0   memory length: 335039   epsilon: 0.4356208000104472    steps: 179     evaluation reward: 1.09\n",
      "episode: 1782   score: 1.0   memory length: 335212   epsilon: 0.435278260010445    steps: 173     evaluation reward: 1.08\n",
      "episode: 1783   score: 0.0   memory length: 335335   epsilon: 0.43503472001044347    steps: 123     evaluation reward: 1.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1784   score: 1.0   memory length: 335487   epsilon: 0.43473376001044156    steps: 152     evaluation reward: 1.08\n",
      "episode: 1785   score: 2.0   memory length: 335685   epsilon: 0.4343417200104391    steps: 198     evaluation reward: 1.08\n",
      "episode: 1786   score: 2.0   memory length: 335885   epsilon: 0.4339457200104366    steps: 200     evaluation reward: 1.1\n",
      "episode: 1787   score: 3.0   memory length: 336112   epsilon: 0.43349626001043373    steps: 227     evaluation reward: 1.12\n",
      "episode: 1788   score: 0.0   memory length: 336237   epsilon: 0.43324876001043217    steps: 125     evaluation reward: 1.11\n",
      "episode: 1789   score: 2.0   memory length: 336435   epsilon: 0.4328567200104297    steps: 198     evaluation reward: 1.12\n",
      "episode: 1790   score: 3.0   memory length: 336680   epsilon: 0.4323716200104266    steps: 245     evaluation reward: 1.12\n",
      "episode: 1791   score: 1.0   memory length: 336833   epsilon: 0.4320686800104247    steps: 153     evaluation reward: 1.13\n",
      "episode: 1792   score: 0.0   memory length: 336956   epsilon: 0.43182514001042316    steps: 123     evaluation reward: 1.1\n",
      "episode: 1793   score: 1.0   memory length: 337129   epsilon: 0.431482600010421    steps: 173     evaluation reward: 1.11\n",
      "episode: 1794   score: 1.0   memory length: 337281   epsilon: 0.4311816400104191    steps: 152     evaluation reward: 1.12\n",
      "episode: 1795   score: 1.0   memory length: 337432   epsilon: 0.4308826600104172    steps: 151     evaluation reward: 1.12\n",
      "episode: 1796   score: 0.0   memory length: 337554   epsilon: 0.43064110001041567    steps: 122     evaluation reward: 1.12\n",
      "episode: 1797   score: 4.0   memory length: 337833   epsilon: 0.43008868001041217    steps: 279     evaluation reward: 1.16\n",
      "episode: 1798   score: 3.0   memory length: 338101   epsilon: 0.4295580400104088    steps: 268     evaluation reward: 1.17\n",
      "episode: 1799   score: 1.0   memory length: 338252   epsilon: 0.4292590600104069    steps: 151     evaluation reward: 1.14\n",
      "episode: 1800   score: 1.0   memory length: 338404   epsilon: 0.428958100010405    steps: 152     evaluation reward: 1.14\n",
      "episode: 1801   score: 3.0   memory length: 338631   epsilon: 0.4285086400104022    steps: 227     evaluation reward: 1.16\n",
      "episode: 1802   score: 0.0   memory length: 338753   epsilon: 0.42826708001040065    steps: 122     evaluation reward: 1.15\n",
      "episode: 1803   score: 0.0   memory length: 338875   epsilon: 0.4280255200103991    steps: 122     evaluation reward: 1.15\n",
      "episode: 1804   score: 2.0   memory length: 339074   epsilon: 0.4276315000103966    steps: 199     evaluation reward: 1.17\n",
      "episode: 1805   score: 0.0   memory length: 339201   epsilon: 0.42738004001039503    steps: 127     evaluation reward: 1.16\n",
      "episode: 1806   score: 2.0   memory length: 339401   epsilon: 0.42698404001039253    steps: 200     evaluation reward: 1.17\n",
      "episode: 1807   score: 2.0   memory length: 339600   epsilon: 0.42659002001039004    steps: 199     evaluation reward: 1.18\n",
      "episode: 1808   score: 1.0   memory length: 339751   epsilon: 0.42629104001038814    steps: 151     evaluation reward: 1.17\n",
      "episode: 1809   score: 0.0   memory length: 339873   epsilon: 0.4260494800103866    steps: 122     evaluation reward: 1.16\n",
      "episode: 1810   score: 0.0   memory length: 339996   epsilon: 0.4258059400103851    steps: 123     evaluation reward: 1.13\n",
      "episode: 1811   score: 1.0   memory length: 340150   epsilon: 0.42550102001038315    steps: 154     evaluation reward: 1.13\n",
      "episode: 1812   score: 0.0   memory length: 340272   epsilon: 0.4252594600103816    steps: 122     evaluation reward: 1.11\n",
      "episode: 1813   score: 0.0   memory length: 340394   epsilon: 0.4250179000103801    steps: 122     evaluation reward: 1.11\n",
      "episode: 1814   score: 3.0   memory length: 340622   epsilon: 0.42456646001037723    steps: 228     evaluation reward: 1.12\n",
      "episode: 1815   score: 0.0   memory length: 340745   epsilon: 0.4243229200103757    steps: 123     evaluation reward: 1.11\n",
      "episode: 1816   score: 0.0   memory length: 340868   epsilon: 0.42407938001037415    steps: 123     evaluation reward: 1.09\n",
      "episode: 1817   score: 0.0   memory length: 340990   epsilon: 0.4238378200103726    steps: 122     evaluation reward: 1.08\n",
      "episode: 1818   score: 0.0   memory length: 341112   epsilon: 0.4235962600103711    steps: 122     evaluation reward: 1.07\n",
      "episode: 1819   score: 2.0   memory length: 341312   epsilon: 0.4232002600103686    steps: 200     evaluation reward: 1.07\n",
      "episode: 1820   score: 2.0   memory length: 341510   epsilon: 0.4228082200103661    steps: 198     evaluation reward: 1.09\n",
      "episode: 1821   score: 2.0   memory length: 341689   epsilon: 0.42245380001036387    steps: 179     evaluation reward: 1.11\n",
      "episode: 1822   score: 1.0   memory length: 341842   epsilon: 0.42215086001036195    steps: 153     evaluation reward: 1.09\n",
      "episode: 1823   score: 1.0   memory length: 342013   epsilon: 0.4218122800103598    steps: 171     evaluation reward: 1.08\n",
      "episode: 1824   score: 2.0   memory length: 342231   epsilon: 0.4213806400103571    steps: 218     evaluation reward: 1.1\n",
      "episode: 1825   score: 3.0   memory length: 342460   epsilon: 0.4209272200103542    steps: 229     evaluation reward: 1.13\n",
      "episode: 1826   score: 2.0   memory length: 342658   epsilon: 0.42053518001035173    steps: 198     evaluation reward: 1.15\n",
      "episode: 1827   score: 2.0   memory length: 342859   epsilon: 0.4201372000103492    steps: 201     evaluation reward: 1.15\n",
      "episode: 1828   score: 0.0   memory length: 342981   epsilon: 0.4198956400103477    steps: 122     evaluation reward: 1.11\n",
      "episode: 1829   score: 2.0   memory length: 343178   epsilon: 0.4195055800103452    steps: 197     evaluation reward: 1.12\n",
      "episode: 1830   score: 0.0   memory length: 343303   epsilon: 0.41925808001034365    steps: 125     evaluation reward: 1.1\n",
      "episode: 1831   score: 0.0   memory length: 343429   epsilon: 0.41900860001034207    steps: 126     evaluation reward: 1.1\n",
      "episode: 1832   score: 2.0   memory length: 343646   epsilon: 0.41857894001033935    steps: 217     evaluation reward: 1.11\n",
      "episode: 1833   score: 1.0   memory length: 343815   epsilon: 0.41824432001033723    steps: 169     evaluation reward: 1.09\n",
      "episode: 1834   score: 0.0   memory length: 343938   epsilon: 0.4180007800103357    steps: 123     evaluation reward: 1.06\n",
      "episode: 1835   score: 0.0   memory length: 344060   epsilon: 0.41775922001033416    steps: 122     evaluation reward: 1.06\n",
      "episode: 1836   score: 1.0   memory length: 344228   epsilon: 0.41742658001033206    steps: 168     evaluation reward: 1.07\n",
      "episode: 1837   score: 1.0   memory length: 344398   epsilon: 0.41708998001032993    steps: 170     evaluation reward: 1.08\n",
      "episode: 1838   score: 3.0   memory length: 344630   epsilon: 0.416630620010327    steps: 232     evaluation reward: 1.11\n",
      "episode: 1839   score: 0.0   memory length: 344753   epsilon: 0.4163870800103255    steps: 123     evaluation reward: 1.11\n",
      "episode: 1840   score: 0.0   memory length: 344877   epsilon: 0.41614156001032393    steps: 124     evaluation reward: 1.1\n",
      "episode: 1841   score: 3.0   memory length: 345121   epsilon: 0.4156584400103209    steps: 244     evaluation reward: 1.11\n",
      "episode: 1842   score: 3.0   memory length: 345346   epsilon: 0.41521294001031805    steps: 225     evaluation reward: 1.14\n",
      "episode: 1843   score: 1.0   memory length: 345516   epsilon: 0.4148763400103159    steps: 170     evaluation reward: 1.15\n",
      "episode: 1844   score: 0.0   memory length: 345638   epsilon: 0.4146347800103144    steps: 122     evaluation reward: 1.15\n",
      "episode: 1845   score: 1.0   memory length: 345809   epsilon: 0.41429620001031225    steps: 171     evaluation reward: 1.16\n",
      "episode: 1846   score: 1.0   memory length: 345959   epsilon: 0.4139992000103104    steps: 150     evaluation reward: 1.15\n",
      "episode: 1847   score: 0.0   memory length: 346081   epsilon: 0.41375764001030885    steps: 122     evaluation reward: 1.15\n",
      "episode: 1848   score: 2.0   memory length: 346280   epsilon: 0.41336362001030635    steps: 199     evaluation reward: 1.17\n",
      "episode: 1849   score: 3.0   memory length: 346525   epsilon: 0.4128785200103033    steps: 245     evaluation reward: 1.2\n",
      "episode: 1850   score: 3.0   memory length: 346773   epsilon: 0.4123874800103002    steps: 248     evaluation reward: 1.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1851   score: 2.0   memory length: 346973   epsilon: 0.4119914800102977    steps: 200     evaluation reward: 1.23\n",
      "episode: 1852   score: 3.0   memory length: 347222   epsilon: 0.41149846001029455    steps: 249     evaluation reward: 1.25\n",
      "episode: 1853   score: 0.0   memory length: 347344   epsilon: 0.411256900010293    steps: 122     evaluation reward: 1.22\n",
      "episode: 1854   score: 0.0   memory length: 347467   epsilon: 0.4110133600102915    steps: 123     evaluation reward: 1.22\n",
      "episode: 1855   score: 3.0   memory length: 347718   epsilon: 0.41051638001028834    steps: 251     evaluation reward: 1.24\n",
      "episode: 1856   score: 1.0   memory length: 347888   epsilon: 0.4101797800102862    steps: 170     evaluation reward: 1.25\n",
      "episode: 1857   score: 2.0   memory length: 348088   epsilon: 0.4097837800102837    steps: 200     evaluation reward: 1.26\n",
      "episode: 1858   score: 1.0   memory length: 348240   epsilon: 0.4094828200102818    steps: 152     evaluation reward: 1.26\n",
      "episode: 1859   score: 0.0   memory length: 348362   epsilon: 0.40924126001028027    steps: 122     evaluation reward: 1.25\n",
      "episode: 1860   score: 2.0   memory length: 348561   epsilon: 0.4088472400102778    steps: 199     evaluation reward: 1.25\n",
      "episode: 1861   score: 2.0   memory length: 348759   epsilon: 0.4084552000102753    steps: 198     evaluation reward: 1.26\n",
      "episode: 1862   score: 2.0   memory length: 348960   epsilon: 0.4080572200102728    steps: 201     evaluation reward: 1.24\n",
      "episode: 1863   score: 1.0   memory length: 349131   epsilon: 0.40771864001027064    steps: 171     evaluation reward: 1.24\n",
      "episode: 1864   score: 2.0   memory length: 349329   epsilon: 0.40732660001026816    steps: 198     evaluation reward: 1.25\n",
      "episode: 1865   score: 1.0   memory length: 349479   epsilon: 0.4070296000102663    steps: 150     evaluation reward: 1.25\n",
      "episode: 1866   score: 3.0   memory length: 349725   epsilon: 0.4065425200102632    steps: 246     evaluation reward: 1.28\n",
      "now time :  2019-09-25 17:29:38.405417\n",
      "episode: 1867   score: 4.0   memory length: 350002   epsilon: 0.4059940600102597    steps: 277     evaluation reward: 1.31\n",
      "episode: 1868   score: 0.0   memory length: 350127   epsilon: 0.40574656001025816    steps: 125     evaluation reward: 1.29\n",
      "episode: 1869   score: 0.0   memory length: 350254   epsilon: 0.40549510001025657    steps: 127     evaluation reward: 1.29\n",
      "episode: 1870   score: 1.0   memory length: 350404   epsilon: 0.4051981000102547    steps: 150     evaluation reward: 1.28\n",
      "episode: 1871   score: 0.0   memory length: 350528   epsilon: 0.40495258001025314    steps: 124     evaluation reward: 1.26\n",
      "episode: 1872   score: 2.0   memory length: 350725   epsilon: 0.40456252001025067    steps: 197     evaluation reward: 1.27\n",
      "episode: 1873   score: 0.0   memory length: 350848   epsilon: 0.40431898001024913    steps: 123     evaluation reward: 1.25\n",
      "episode: 1874   score: 2.0   memory length: 351046   epsilon: 0.40392694001024665    steps: 198     evaluation reward: 1.27\n",
      "episode: 1875   score: 1.0   memory length: 351196   epsilon: 0.40362994001024477    steps: 150     evaluation reward: 1.28\n",
      "episode: 1876   score: 1.0   memory length: 351347   epsilon: 0.4033309600102429    steps: 151     evaluation reward: 1.29\n",
      "episode: 1877   score: 1.0   memory length: 351516   epsilon: 0.40299634001024076    steps: 169     evaluation reward: 1.27\n",
      "episode: 1878   score: 2.0   memory length: 351698   epsilon: 0.4026359800102385    steps: 182     evaluation reward: 1.29\n",
      "episode: 1879   score: 3.0   memory length: 351946   epsilon: 0.4021449400102354    steps: 248     evaluation reward: 1.3\n",
      "episode: 1880   score: 1.0   memory length: 352115   epsilon: 0.40181032001023326    steps: 169     evaluation reward: 1.3\n",
      "episode: 1881   score: 1.0   memory length: 352285   epsilon: 0.4014737200102311    steps: 170     evaluation reward: 1.29\n",
      "episode: 1882   score: 3.0   memory length: 352534   epsilon: 0.400980700010228    steps: 249     evaluation reward: 1.31\n",
      "episode: 1883   score: 2.0   memory length: 352734   epsilon: 0.4005847000102255    steps: 200     evaluation reward: 1.33\n",
      "episode: 1884   score: 1.0   memory length: 352884   epsilon: 0.4002877000102236    steps: 150     evaluation reward: 1.33\n",
      "episode: 1885   score: 0.0   memory length: 353006   epsilon: 0.4000461400102221    steps: 122     evaluation reward: 1.31\n",
      "episode: 1886   score: 0.0   memory length: 353129   epsilon: 0.39980260001022055    steps: 123     evaluation reward: 1.29\n",
      "episode: 1887   score: 0.0   memory length: 353255   epsilon: 0.399553120010219    steps: 126     evaluation reward: 1.26\n",
      "episode: 1888   score: 2.0   memory length: 353453   epsilon: 0.3991610800102165    steps: 198     evaluation reward: 1.28\n",
      "episode: 1889   score: 1.0   memory length: 353606   epsilon: 0.3988581400102146    steps: 153     evaluation reward: 1.27\n",
      "episode: 1890   score: 1.0   memory length: 353759   epsilon: 0.39855520001021266    steps: 153     evaluation reward: 1.25\n",
      "episode: 1891   score: 1.0   memory length: 353914   epsilon: 0.3982483000102107    steps: 155     evaluation reward: 1.25\n",
      "episode: 1892   score: 1.0   memory length: 354067   epsilon: 0.3979453600102088    steps: 153     evaluation reward: 1.26\n",
      "episode: 1893   score: 1.0   memory length: 354220   epsilon: 0.3976424200102069    steps: 153     evaluation reward: 1.26\n",
      "episode: 1894   score: 0.0   memory length: 354342   epsilon: 0.39740086001020536    steps: 122     evaluation reward: 1.25\n",
      "episode: 1895   score: 2.0   memory length: 354527   epsilon: 0.39703456001020304    steps: 185     evaluation reward: 1.26\n",
      "episode: 1896   score: 0.0   memory length: 354650   epsilon: 0.3967910200102015    steps: 123     evaluation reward: 1.26\n",
      "episode: 1897   score: 1.0   memory length: 354821   epsilon: 0.39645244001019936    steps: 171     evaluation reward: 1.23\n",
      "episode: 1898   score: 0.0   memory length: 354943   epsilon: 0.39621088001019783    steps: 122     evaluation reward: 1.2\n",
      "episode: 1899   score: 1.0   memory length: 355093   epsilon: 0.39591388001019595    steps: 150     evaluation reward: 1.2\n",
      "episode: 1900   score: 1.0   memory length: 355263   epsilon: 0.3955772800101938    steps: 170     evaluation reward: 1.2\n",
      "episode: 1901   score: 1.0   memory length: 355433   epsilon: 0.3952406800101917    steps: 170     evaluation reward: 1.18\n",
      "episode: 1902   score: 0.0   memory length: 355557   epsilon: 0.39499516001019014    steps: 124     evaluation reward: 1.18\n",
      "episode: 1903   score: 2.0   memory length: 355754   epsilon: 0.39460510001018767    steps: 197     evaluation reward: 1.2\n",
      "episode: 1904   score: 0.0   memory length: 355877   epsilon: 0.39436156001018613    steps: 123     evaluation reward: 1.18\n",
      "episode: 1905   score: 0.0   memory length: 355999   epsilon: 0.3941200000101846    steps: 122     evaluation reward: 1.18\n",
      "episode: 1906   score: 1.0   memory length: 356171   epsilon: 0.39377944001018245    steps: 172     evaluation reward: 1.17\n",
      "episode: 1907   score: 0.0   memory length: 356293   epsilon: 0.3935378800101809    steps: 122     evaluation reward: 1.15\n",
      "episode: 1908   score: 1.0   memory length: 356444   epsilon: 0.393238900010179    steps: 151     evaluation reward: 1.15\n",
      "episode: 1909   score: 1.0   memory length: 356597   epsilon: 0.3929359600101771    steps: 153     evaluation reward: 1.16\n",
      "episode: 1910   score: 1.0   memory length: 356751   epsilon: 0.3926310400101752    steps: 154     evaluation reward: 1.17\n",
      "episode: 1911   score: 1.0   memory length: 356923   epsilon: 0.392290480010173    steps: 172     evaluation reward: 1.17\n",
      "episode: 1912   score: 0.0   memory length: 357045   epsilon: 0.3920489200101715    steps: 122     evaluation reward: 1.17\n",
      "episode: 1913   score: 1.0   memory length: 357195   epsilon: 0.3917519200101696    steps: 150     evaluation reward: 1.18\n",
      "episode: 1914   score: 2.0   memory length: 357392   epsilon: 0.39136186001016715    steps: 197     evaluation reward: 1.17\n",
      "episode: 1915   score: 1.0   memory length: 357561   epsilon: 0.39102724001016503    steps: 169     evaluation reward: 1.18\n",
      "episode: 1916   score: 0.0   memory length: 357685   epsilon: 0.3907817200101635    steps: 124     evaluation reward: 1.18\n",
      "episode: 1917   score: 2.0   memory length: 357884   epsilon: 0.390387700010161    steps: 199     evaluation reward: 1.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1918   score: 1.0   memory length: 358035   epsilon: 0.3900887200101591    steps: 151     evaluation reward: 1.21\n",
      "episode: 1919   score: 2.0   memory length: 358232   epsilon: 0.38969866001015663    steps: 197     evaluation reward: 1.21\n",
      "episode: 1920   score: 1.0   memory length: 358400   epsilon: 0.3893660200101545    steps: 168     evaluation reward: 1.2\n",
      "episode: 1921   score: 0.0   memory length: 358523   epsilon: 0.389122480010153    steps: 123     evaluation reward: 1.18\n",
      "episode: 1922   score: 0.0   memory length: 358645   epsilon: 0.38888092001015145    steps: 122     evaluation reward: 1.17\n",
      "episode: 1923   score: 2.0   memory length: 358863   epsilon: 0.3884492800101487    steps: 218     evaluation reward: 1.18\n",
      "episode: 1924   score: 0.0   memory length: 358985   epsilon: 0.3882077200101472    steps: 122     evaluation reward: 1.16\n",
      "episode: 1925   score: 2.0   memory length: 359167   epsilon: 0.3878473600101449    steps: 182     evaluation reward: 1.15\n",
      "episode: 1926   score: 1.0   memory length: 359339   epsilon: 0.38750680001014276    steps: 172     evaluation reward: 1.14\n",
      "episode: 1927   score: 2.0   memory length: 359536   epsilon: 0.3871167400101403    steps: 197     evaluation reward: 1.14\n",
      "episode: 1928   score: 1.0   memory length: 359687   epsilon: 0.3868177600101384    steps: 151     evaluation reward: 1.15\n",
      "episode: 1929   score: 1.0   memory length: 359859   epsilon: 0.38647720001013625    steps: 172     evaluation reward: 1.14\n",
      "episode: 1930   score: 1.0   memory length: 360010   epsilon: 0.38617822001013435    steps: 151     evaluation reward: 1.15\n",
      "episode: 1931   score: 0.0   memory length: 360134   epsilon: 0.3859327000101328    steps: 124     evaluation reward: 1.15\n",
      "episode: 1932   score: 2.0   memory length: 360331   epsilon: 0.38554264001013033    steps: 197     evaluation reward: 1.15\n",
      "episode: 1933   score: 2.0   memory length: 360529   epsilon: 0.38515060001012785    steps: 198     evaluation reward: 1.16\n",
      "episode: 1934   score: 1.0   memory length: 360701   epsilon: 0.3848100400101257    steps: 172     evaluation reward: 1.17\n",
      "episode: 1935   score: 0.0   memory length: 360823   epsilon: 0.38456848001012417    steps: 122     evaluation reward: 1.17\n",
      "episode: 1936   score: 2.0   memory length: 361020   epsilon: 0.3841784200101217    steps: 197     evaluation reward: 1.18\n",
      "episode: 1937   score: 0.0   memory length: 361142   epsilon: 0.3839368600101202    steps: 122     evaluation reward: 1.17\n",
      "episode: 1938   score: 0.0   memory length: 361265   epsilon: 0.38369332001011863    steps: 123     evaluation reward: 1.14\n",
      "episode: 1939   score: 4.0   memory length: 361546   epsilon: 0.3831369400101151    steps: 281     evaluation reward: 1.18\n",
      "episode: 1940   score: 0.0   memory length: 361668   epsilon: 0.3828953800101136    steps: 122     evaluation reward: 1.18\n",
      "episode: 1941   score: 2.0   memory length: 361867   epsilon: 0.3825013600101111    steps: 199     evaluation reward: 1.17\n",
      "episode: 1942   score: 2.0   memory length: 362064   epsilon: 0.3821113000101086    steps: 197     evaluation reward: 1.16\n",
      "episode: 1943   score: 0.0   memory length: 362190   epsilon: 0.38186182001010704    steps: 126     evaluation reward: 1.15\n",
      "episode: 1944   score: 1.0   memory length: 362340   epsilon: 0.38156482001010517    steps: 150     evaluation reward: 1.16\n",
      "episode: 1945   score: 3.0   memory length: 362612   epsilon: 0.38102626001010176    steps: 272     evaluation reward: 1.18\n",
      "episode: 1946   score: 2.0   memory length: 362809   epsilon: 0.3806362000100993    steps: 197     evaluation reward: 1.19\n",
      "episode: 1947   score: 2.0   memory length: 363033   epsilon: 0.3801926800100965    steps: 224     evaluation reward: 1.21\n",
      "episode: 1948   score: 1.0   memory length: 363184   epsilon: 0.3798937000100946    steps: 151     evaluation reward: 1.2\n",
      "episode: 1949   score: 1.0   memory length: 363352   epsilon: 0.3795610600100925    steps: 168     evaluation reward: 1.18\n",
      "episode: 1950   score: 1.0   memory length: 363523   epsilon: 0.37922248001009035    steps: 171     evaluation reward: 1.16\n",
      "episode: 1951   score: 1.0   memory length: 363673   epsilon: 0.37892548001008847    steps: 150     evaluation reward: 1.15\n",
      "episode: 1952   score: 1.0   memory length: 363843   epsilon: 0.37858888001008634    steps: 170     evaluation reward: 1.13\n",
      "episode: 1953   score: 2.0   memory length: 364043   epsilon: 0.37819288001008383    steps: 200     evaluation reward: 1.15\n",
      "episode: 1954   score: 1.0   memory length: 364212   epsilon: 0.3778582600100817    steps: 169     evaluation reward: 1.16\n",
      "episode: 1955   score: 2.0   memory length: 364411   epsilon: 0.3774642400100792    steps: 199     evaluation reward: 1.15\n",
      "episode: 1956   score: 0.0   memory length: 364534   epsilon: 0.3772207000100777    steps: 123     evaluation reward: 1.14\n",
      "episode: 1957   score: 0.0   memory length: 364658   epsilon: 0.3769751800100761    steps: 124     evaluation reward: 1.12\n",
      "episode: 1958   score: 0.0   memory length: 364782   epsilon: 0.3767296600100746    steps: 124     evaluation reward: 1.11\n",
      "episode: 1959   score: 0.0   memory length: 364905   epsilon: 0.37648612001007303    steps: 123     evaluation reward: 1.11\n",
      "episode: 1960   score: 4.0   memory length: 365163   epsilon: 0.3759752800100698    steps: 258     evaluation reward: 1.13\n",
      "episode: 1961   score: 0.0   memory length: 365285   epsilon: 0.3757337200100683    steps: 122     evaluation reward: 1.11\n",
      "episode: 1962   score: 0.0   memory length: 365407   epsilon: 0.37549216001006674    steps: 122     evaluation reward: 1.09\n",
      "episode: 1963   score: 2.0   memory length: 365604   epsilon: 0.3751021000100643    steps: 197     evaluation reward: 1.1\n",
      "episode: 1964   score: 0.0   memory length: 365726   epsilon: 0.37486054001006275    steps: 122     evaluation reward: 1.08\n",
      "episode: 1965   score: 3.0   memory length: 365972   epsilon: 0.37437346001005967    steps: 246     evaluation reward: 1.1\n",
      "episode: 1966   score: 3.0   memory length: 366219   epsilon: 0.3738844000100566    steps: 247     evaluation reward: 1.1\n",
      "episode: 1967   score: 1.0   memory length: 366389   epsilon: 0.37354780001005444    steps: 170     evaluation reward: 1.07\n",
      "episode: 1968   score: 0.0   memory length: 366512   epsilon: 0.3733042600100529    steps: 123     evaluation reward: 1.07\n",
      "episode: 1969   score: 3.0   memory length: 366741   epsilon: 0.37285084001005003    steps: 229     evaluation reward: 1.1\n",
      "episode: 1970   score: 1.0   memory length: 366891   epsilon: 0.37255384001004815    steps: 150     evaluation reward: 1.1\n",
      "episode: 1971   score: 2.0   memory length: 367110   epsilon: 0.3721202200100454    steps: 219     evaluation reward: 1.12\n",
      "episode: 1972   score: 1.0   memory length: 367260   epsilon: 0.37182322001004353    steps: 150     evaluation reward: 1.11\n",
      "episode: 1973   score: 1.0   memory length: 367428   epsilon: 0.3714905800100414    steps: 168     evaluation reward: 1.12\n",
      "episode: 1974   score: 2.0   memory length: 367611   epsilon: 0.37112824001003913    steps: 183     evaluation reward: 1.12\n",
      "episode: 1975   score: 2.0   memory length: 367834   epsilon: 0.37068670001003634    steps: 223     evaluation reward: 1.13\n",
      "episode: 1976   score: 2.0   memory length: 368052   epsilon: 0.3702550600100336    steps: 218     evaluation reward: 1.14\n",
      "episode: 1977   score: 0.0   memory length: 368176   epsilon: 0.37000954001003206    steps: 124     evaluation reward: 1.13\n",
      "episode: 1978   score: 0.0   memory length: 368299   epsilon: 0.3697660000100305    steps: 123     evaluation reward: 1.11\n",
      "episode: 1979   score: 3.0   memory length: 368548   epsilon: 0.3692729800100274    steps: 249     evaluation reward: 1.11\n",
      "episode: 1980   score: 0.0   memory length: 368672   epsilon: 0.36902746001002584    steps: 124     evaluation reward: 1.1\n",
      "episode: 1981   score: 2.0   memory length: 368892   epsilon: 0.3685918600100231    steps: 220     evaluation reward: 1.11\n",
      "episode: 1982   score: 1.0   memory length: 369063   epsilon: 0.36825328001002094    steps: 171     evaluation reward: 1.09\n",
      "episode: 1983   score: 4.0   memory length: 369361   epsilon: 0.3676632400100172    steps: 298     evaluation reward: 1.11\n",
      "episode: 1984   score: 2.0   memory length: 369578   epsilon: 0.3672335800100145    steps: 217     evaluation reward: 1.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1985   score: 2.0   memory length: 369777   epsilon: 0.366839560010012    steps: 199     evaluation reward: 1.14\n",
      "episode: 1986   score: 3.0   memory length: 370046   epsilon: 0.36630694001000863    steps: 269     evaluation reward: 1.17\n",
      "episode: 1987   score: 2.0   memory length: 370228   epsilon: 0.36594658001000635    steps: 182     evaluation reward: 1.19\n",
      "episode: 1988   score: 4.0   memory length: 370501   epsilon: 0.36540604001000293    steps: 273     evaluation reward: 1.21\n",
      "episode: 1989   score: 2.0   memory length: 370720   epsilon: 0.3649724200100002    steps: 219     evaluation reward: 1.22\n",
      "episode: 1990   score: 0.0   memory length: 370842   epsilon: 0.36473086000999866    steps: 122     evaluation reward: 1.21\n",
      "episode: 1991   score: 3.0   memory length: 371090   epsilon: 0.36423982000999555    steps: 248     evaluation reward: 1.23\n",
      "episode: 1992   score: 2.0   memory length: 371289   epsilon: 0.36384580000999306    steps: 199     evaluation reward: 1.24\n",
      "episode: 1993   score: 2.0   memory length: 371487   epsilon: 0.3634537600099906    steps: 198     evaluation reward: 1.25\n",
      "episode: 1994   score: 1.0   memory length: 371641   epsilon: 0.36314884000998865    steps: 154     evaluation reward: 1.26\n",
      "episode: 1995   score: 0.0   memory length: 371763   epsilon: 0.3629072800099871    steps: 122     evaluation reward: 1.24\n",
      "episode: 1996   score: 2.0   memory length: 371961   epsilon: 0.36251524000998464    steps: 198     evaluation reward: 1.26\n",
      "episode: 1997   score: 0.0   memory length: 372084   epsilon: 0.3622717000099831    steps: 123     evaluation reward: 1.25\n",
      "episode: 1998   score: 1.0   memory length: 372235   epsilon: 0.3619727200099812    steps: 151     evaluation reward: 1.26\n",
      "episode: 1999   score: 4.0   memory length: 372530   epsilon: 0.3613886200099775    steps: 295     evaluation reward: 1.29\n",
      "episode: 2000   score: 3.0   memory length: 372778   epsilon: 0.3608975800099744    steps: 248     evaluation reward: 1.31\n",
      "episode: 2001   score: 1.0   memory length: 372947   epsilon: 0.3605629600099723    steps: 169     evaluation reward: 1.31\n",
      "episode: 2002   score: 2.0   memory length: 373144   epsilon: 0.3601729000099698    steps: 197     evaluation reward: 1.33\n",
      "episode: 2003   score: 1.0   memory length: 373298   epsilon: 0.3598679800099679    steps: 154     evaluation reward: 1.32\n",
      "episode: 2004   score: 1.0   memory length: 373466   epsilon: 0.3595353400099658    steps: 168     evaluation reward: 1.33\n",
      "episode: 2005   score: 0.0   memory length: 373590   epsilon: 0.35928982000996423    steps: 124     evaluation reward: 1.33\n",
      "episode: 2006   score: 1.0   memory length: 373759   epsilon: 0.3589552000099621    steps: 169     evaluation reward: 1.33\n",
      "episode: 2007   score: 0.0   memory length: 373882   epsilon: 0.3587116600099606    steps: 123     evaluation reward: 1.33\n",
      "episode: 2008   score: 1.0   memory length: 374050   epsilon: 0.35837902000995847    steps: 168     evaluation reward: 1.33\n",
      "episode: 2009   score: 2.0   memory length: 374268   epsilon: 0.35794738000995574    steps: 218     evaluation reward: 1.34\n",
      "episode: 2010   score: 3.0   memory length: 374516   epsilon: 0.35745634000995263    steps: 248     evaluation reward: 1.36\n",
      "episode: 2011   score: 1.0   memory length: 374690   epsilon: 0.35711182000995045    steps: 174     evaluation reward: 1.36\n",
      "episode: 2012   score: 1.0   memory length: 374840   epsilon: 0.3568148200099486    steps: 150     evaluation reward: 1.37\n",
      "episode: 2013   score: 1.0   memory length: 375010   epsilon: 0.35647822000994644    steps: 170     evaluation reward: 1.37\n",
      "episode: 2014   score: 0.0   memory length: 375132   epsilon: 0.3562366600099449    steps: 122     evaluation reward: 1.35\n",
      "episode: 2015   score: 0.0   memory length: 375254   epsilon: 0.3559951000099434    steps: 122     evaluation reward: 1.34\n",
      "episode: 2016   score: 0.0   memory length: 375378   epsilon: 0.35574958000994183    steps: 124     evaluation reward: 1.34\n",
      "episode: 2017   score: 0.0   memory length: 375501   epsilon: 0.3555060400099403    steps: 123     evaluation reward: 1.32\n",
      "episode: 2018   score: 0.0   memory length: 375625   epsilon: 0.35526052000993874    steps: 124     evaluation reward: 1.31\n",
      "episode: 2019   score: 0.0   memory length: 375748   epsilon: 0.3550169800099372    steps: 123     evaluation reward: 1.29\n",
      "episode: 2020   score: 0.0   memory length: 375870   epsilon: 0.35477542000993567    steps: 122     evaluation reward: 1.28\n",
      "episode: 2021   score: 3.0   memory length: 376119   epsilon: 0.35428240000993255    steps: 249     evaluation reward: 1.31\n",
      "episode: 2022   score: 0.0   memory length: 376242   epsilon: 0.354038860009931    steps: 123     evaluation reward: 1.31\n",
      "episode: 2023   score: 0.0   memory length: 376365   epsilon: 0.35379532000992947    steps: 123     evaluation reward: 1.29\n",
      "episode: 2024   score: 1.0   memory length: 376536   epsilon: 0.35345674000992733    steps: 171     evaluation reward: 1.3\n",
      "episode: 2025   score: 0.0   memory length: 376662   epsilon: 0.35320726000992575    steps: 126     evaluation reward: 1.28\n",
      "episode: 2026   score: 0.0   memory length: 376784   epsilon: 0.3529657000099242    steps: 122     evaluation reward: 1.27\n",
      "episode: 2027   score: 2.0   memory length: 376983   epsilon: 0.35257168000992173    steps: 199     evaluation reward: 1.27\n",
      "episode: 2028   score: 3.0   memory length: 377211   epsilon: 0.3521202400099189    steps: 228     evaluation reward: 1.29\n",
      "episode: 2029   score: 1.0   memory length: 377381   epsilon: 0.35178364000991674    steps: 170     evaluation reward: 1.29\n",
      "episode: 2030   score: 1.0   memory length: 377549   epsilon: 0.35145100000991464    steps: 168     evaluation reward: 1.29\n",
      "episode: 2031   score: 0.0   memory length: 377672   epsilon: 0.3512074600099131    steps: 123     evaluation reward: 1.29\n",
      "episode: 2032   score: 2.0   memory length: 377870   epsilon: 0.3508154200099106    steps: 198     evaluation reward: 1.29\n",
      "episode: 2033   score: 0.0   memory length: 377996   epsilon: 0.35056594000990904    steps: 126     evaluation reward: 1.27\n",
      "episode: 2034   score: 1.0   memory length: 378147   epsilon: 0.35026696000990715    steps: 151     evaluation reward: 1.27\n",
      "episode: 2035   score: 2.0   memory length: 378327   epsilon: 0.3499105600099049    steps: 180     evaluation reward: 1.29\n",
      "episode: 2036   score: 1.0   memory length: 378495   epsilon: 0.3495779200099028    steps: 168     evaluation reward: 1.28\n",
      "episode: 2037   score: 2.0   memory length: 378693   epsilon: 0.3491858800099003    steps: 198     evaluation reward: 1.3\n",
      "episode: 2038   score: 0.0   memory length: 378817   epsilon: 0.34894036000989875    steps: 124     evaluation reward: 1.3\n",
      "episode: 2039   score: 0.0   memory length: 378940   epsilon: 0.3486968200098972    steps: 123     evaluation reward: 1.26\n",
      "episode: 2040   score: 0.0   memory length: 379062   epsilon: 0.3484552600098957    steps: 122     evaluation reward: 1.26\n",
      "episode: 2041   score: 0.0   memory length: 379185   epsilon: 0.34821172000989414    steps: 123     evaluation reward: 1.24\n",
      "episode: 2042   score: 2.0   memory length: 379384   epsilon: 0.34781770000989165    steps: 199     evaluation reward: 1.24\n",
      "episode: 2043   score: 1.0   memory length: 379558   epsilon: 0.34747318000988947    steps: 174     evaluation reward: 1.25\n",
      "episode: 2044   score: 0.0   memory length: 379680   epsilon: 0.34723162000988794    steps: 122     evaluation reward: 1.24\n",
      "episode: 2045   score: 3.0   memory length: 379927   epsilon: 0.34674256000988485    steps: 247     evaluation reward: 1.24\n",
      "episode: 2046   score: 1.0   memory length: 380078   epsilon: 0.34644358000988296    steps: 151     evaluation reward: 1.23\n",
      "episode: 2047   score: 3.0   memory length: 380324   epsilon: 0.3459565000098799    steps: 246     evaluation reward: 1.24\n",
      "episode: 2048   score: 2.0   memory length: 380522   epsilon: 0.3455644600098774    steps: 198     evaluation reward: 1.25\n",
      "episode: 2049   score: 2.0   memory length: 380703   epsilon: 0.3452060800098751    steps: 181     evaluation reward: 1.26\n",
      "episode: 2050   score: 1.0   memory length: 380854   epsilon: 0.34490710000987324    steps: 151     evaluation reward: 1.26\n",
      "episode: 2051   score: 2.0   memory length: 381074   epsilon: 0.3444715000098705    steps: 220     evaluation reward: 1.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2052   score: 0.0   memory length: 381196   epsilon: 0.34422994000986895    steps: 122     evaluation reward: 1.26\n",
      "episode: 2053   score: 3.0   memory length: 381443   epsilon: 0.34374088000986586    steps: 247     evaluation reward: 1.27\n",
      "episode: 2054   score: 2.0   memory length: 381640   epsilon: 0.3433508200098634    steps: 197     evaluation reward: 1.28\n",
      "episode: 2055   score: 3.0   memory length: 381907   epsilon: 0.34282216000986004    steps: 267     evaluation reward: 1.29\n",
      "episode: 2056   score: 0.0   memory length: 382031   epsilon: 0.3425766400098585    steps: 124     evaluation reward: 1.29\n",
      "episode: 2057   score: 2.0   memory length: 382230   epsilon: 0.342182620009856    steps: 199     evaluation reward: 1.31\n",
      "episode: 2058   score: 2.0   memory length: 382428   epsilon: 0.3417905800098535    steps: 198     evaluation reward: 1.33\n",
      "episode: 2059   score: 1.0   memory length: 382579   epsilon: 0.3414916000098516    steps: 151     evaluation reward: 1.34\n",
      "episode: 2060   score: 2.0   memory length: 382776   epsilon: 0.34110154000984916    steps: 197     evaluation reward: 1.32\n",
      "episode: 2061   score: 0.0   memory length: 382899   epsilon: 0.3408580000098476    steps: 123     evaluation reward: 1.32\n",
      "episode: 2062   score: 2.0   memory length: 383096   epsilon: 0.34046794000984515    steps: 197     evaluation reward: 1.34\n",
      "episode: 2063   score: 1.0   memory length: 383266   epsilon: 0.340131340009843    steps: 170     evaluation reward: 1.33\n",
      "episode: 2064   score: 2.0   memory length: 383465   epsilon: 0.3397373200098405    steps: 199     evaluation reward: 1.35\n",
      "episode: 2065   score: 0.0   memory length: 383587   epsilon: 0.339495760009839    steps: 122     evaluation reward: 1.32\n",
      "episode: 2066   score: 0.0   memory length: 383712   epsilon: 0.33924826000983743    steps: 125     evaluation reward: 1.29\n",
      "episode: 2067   score: 0.0   memory length: 383834   epsilon: 0.3390067000098359    steps: 122     evaluation reward: 1.28\n",
      "episode: 2068   score: 1.0   memory length: 383985   epsilon: 0.338707720009834    steps: 151     evaluation reward: 1.29\n",
      "episode: 2069   score: 1.0   memory length: 384138   epsilon: 0.3384047800098321    steps: 153     evaluation reward: 1.27\n",
      "episode: 2070   score: 0.0   memory length: 384260   epsilon: 0.33816322000983057    steps: 122     evaluation reward: 1.26\n",
      "episode: 2071   score: 0.0   memory length: 384383   epsilon: 0.337919680009829    steps: 123     evaluation reward: 1.24\n",
      "episode: 2072   score: 2.0   memory length: 384580   epsilon: 0.33752962000982656    steps: 197     evaluation reward: 1.25\n",
      "episode: 2073   score: 0.0   memory length: 384702   epsilon: 0.33728806000982503    steps: 122     evaluation reward: 1.24\n",
      "episode: 2074   score: 0.0   memory length: 384824   epsilon: 0.3370465000098235    steps: 122     evaluation reward: 1.22\n",
      "episode: 2075   score: 0.0   memory length: 384948   epsilon: 0.33680098000982195    steps: 124     evaluation reward: 1.2\n",
      "episode: 2076   score: 0.0   memory length: 385070   epsilon: 0.3365594200098204    steps: 122     evaluation reward: 1.18\n",
      "episode: 2077   score: 2.0   memory length: 385268   epsilon: 0.33616738000981794    steps: 198     evaluation reward: 1.2\n",
      "episode: 2078   score: 0.0   memory length: 385393   epsilon: 0.3359198800098164    steps: 125     evaluation reward: 1.2\n",
      "episode: 2079   score: 2.0   memory length: 385592   epsilon: 0.3355258600098139    steps: 199     evaluation reward: 1.19\n",
      "episode: 2080   score: 0.0   memory length: 385714   epsilon: 0.33528430000981235    steps: 122     evaluation reward: 1.19\n",
      "episode: 2081   score: 1.0   memory length: 385864   epsilon: 0.3349873000098105    steps: 150     evaluation reward: 1.18\n",
      "episode: 2082   score: 4.0   memory length: 386176   epsilon: 0.33436954000980657    steps: 312     evaluation reward: 1.21\n",
      "episode: 2083   score: 1.0   memory length: 386327   epsilon: 0.3340705600098047    steps: 151     evaluation reward: 1.18\n",
      "episode: 2084   score: 0.0   memory length: 386449   epsilon: 0.33382900000980315    steps: 122     evaluation reward: 1.16\n",
      "episode: 2085   score: 2.0   memory length: 386646   epsilon: 0.3334389400098007    steps: 197     evaluation reward: 1.16\n",
      "episode: 2086   score: 2.0   memory length: 386843   epsilon: 0.3330488800097982    steps: 197     evaluation reward: 1.15\n",
      "episode: 2087   score: 0.0   memory length: 386965   epsilon: 0.3328073200097967    steps: 122     evaluation reward: 1.13\n",
      "episode: 2088   score: 0.0   memory length: 387087   epsilon: 0.33256576000979515    steps: 122     evaluation reward: 1.09\n",
      "episode: 2089   score: 0.0   memory length: 387209   epsilon: 0.3323242000097936    steps: 122     evaluation reward: 1.07\n",
      "episode: 2090   score: 0.0   memory length: 387332   epsilon: 0.3320806600097921    steps: 123     evaluation reward: 1.07\n",
      "episode: 2091   score: 2.0   memory length: 387530   epsilon: 0.3316886200097896    steps: 198     evaluation reward: 1.06\n",
      "episode: 2092   score: 1.0   memory length: 387699   epsilon: 0.3313540000097875    steps: 169     evaluation reward: 1.05\n",
      "episode: 2093   score: 0.0   memory length: 387822   epsilon: 0.33111046000978595    steps: 123     evaluation reward: 1.03\n",
      "episode: 2094   score: 0.0   memory length: 387945   epsilon: 0.3308669200097844    steps: 123     evaluation reward: 1.02\n",
      "episode: 2095   score: 1.0   memory length: 388097   epsilon: 0.3305659600097825    steps: 152     evaluation reward: 1.03\n",
      "episode: 2096   score: 2.0   memory length: 388295   epsilon: 0.33017392000978    steps: 198     evaluation reward: 1.03\n",
      "episode: 2097   score: 2.0   memory length: 388512   epsilon: 0.3297442600097773    steps: 217     evaluation reward: 1.05\n",
      "episode: 2098   score: 1.0   memory length: 388665   epsilon: 0.3294413200097754    steps: 153     evaluation reward: 1.05\n",
      "episode: 2099   score: 0.0   memory length: 388787   epsilon: 0.32919976000977386    steps: 122     evaluation reward: 1.01\n",
      "episode: 2100   score: 3.0   memory length: 389033   epsilon: 0.3287126800097708    steps: 246     evaluation reward: 1.01\n",
      "episode: 2101   score: 2.0   memory length: 389232   epsilon: 0.3283186600097683    steps: 199     evaluation reward: 1.02\n",
      "episode: 2102   score: 2.0   memory length: 389429   epsilon: 0.3279286000097658    steps: 197     evaluation reward: 1.02\n",
      "episode: 2103   score: 1.0   memory length: 389579   epsilon: 0.32763160000976393    steps: 150     evaluation reward: 1.02\n",
      "episode: 2104   score: 4.0   memory length: 389853   epsilon: 0.3270890800097605    steps: 274     evaluation reward: 1.05\n",
      "episode: 2105   score: 2.0   memory length: 390051   epsilon: 0.326697040009758    steps: 198     evaluation reward: 1.07\n",
      "episode: 2106   score: 1.0   memory length: 390219   epsilon: 0.3263644000097559    steps: 168     evaluation reward: 1.07\n",
      "episode: 2107   score: 2.0   memory length: 390417   epsilon: 0.32597236000975344    steps: 198     evaluation reward: 1.09\n",
      "episode: 2108   score: 0.0   memory length: 390544   epsilon: 0.32572090000975185    steps: 127     evaluation reward: 1.08\n",
      "episode: 2109   score: 1.0   memory length: 390695   epsilon: 0.32542192000974995    steps: 151     evaluation reward: 1.07\n",
      "episode: 2110   score: 0.0   memory length: 390818   epsilon: 0.3251783800097484    steps: 123     evaluation reward: 1.04\n",
      "episode: 2111   score: 2.0   memory length: 391016   epsilon: 0.32478634000974593    steps: 198     evaluation reward: 1.05\n",
      "episode: 2112   score: 1.0   memory length: 391185   epsilon: 0.3244517200097438    steps: 169     evaluation reward: 1.05\n",
      "episode: 2113   score: 3.0   memory length: 391411   epsilon: 0.324004240009741    steps: 226     evaluation reward: 1.07\n",
      "episode: 2114   score: 2.0   memory length: 391608   epsilon: 0.3236141800097385    steps: 197     evaluation reward: 1.09\n",
      "episode: 2115   score: 3.0   memory length: 391834   epsilon: 0.3231667000097357    steps: 226     evaluation reward: 1.12\n",
      "episode: 2116   score: 2.0   memory length: 392031   epsilon: 0.3227766400097332    steps: 197     evaluation reward: 1.14\n",
      "episode: 2117   score: 0.0   memory length: 392156   epsilon: 0.32252914000973165    steps: 125     evaluation reward: 1.14\n",
      "episode: 2118   score: 3.0   memory length: 392404   epsilon: 0.32203810000972855    steps: 248     evaluation reward: 1.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2119   score: 2.0   memory length: 392602   epsilon: 0.32164606000972606    steps: 198     evaluation reward: 1.19\n",
      "episode: 2120   score: 2.0   memory length: 392799   epsilon: 0.3212560000097236    steps: 197     evaluation reward: 1.21\n",
      "episode: 2121   score: 0.0   memory length: 392922   epsilon: 0.32101246000972206    steps: 123     evaluation reward: 1.18\n",
      "episode: 2122   score: 1.0   memory length: 393073   epsilon: 0.32071348000972016    steps: 151     evaluation reward: 1.19\n",
      "episode: 2123   score: 0.0   memory length: 393200   epsilon: 0.3204620200097186    steps: 127     evaluation reward: 1.19\n",
      "episode: 2124   score: 0.0   memory length: 393322   epsilon: 0.32022046000971705    steps: 122     evaluation reward: 1.18\n",
      "episode: 2125   score: 1.0   memory length: 393476   epsilon: 0.3199155400097151    steps: 154     evaluation reward: 1.19\n",
      "episode: 2126   score: 3.0   memory length: 393722   epsilon: 0.31942846000971203    steps: 246     evaluation reward: 1.22\n",
      "episode: 2127   score: 3.0   memory length: 393947   epsilon: 0.3189829600097092    steps: 225     evaluation reward: 1.23\n",
      "episode: 2128   score: 2.0   memory length: 394144   epsilon: 0.31859290000970675    steps: 197     evaluation reward: 1.22\n",
      "episode: 2129   score: 2.0   memory length: 394362   epsilon: 0.318161260009704    steps: 218     evaluation reward: 1.23\n",
      "episode: 2130   score: 1.0   memory length: 394513   epsilon: 0.3178622800097021    steps: 151     evaluation reward: 1.23\n",
      "episode: 2131   score: 1.0   memory length: 394666   epsilon: 0.3175593400097002    steps: 153     evaluation reward: 1.24\n",
      "episode: 2132   score: 2.0   memory length: 394848   epsilon: 0.31719898000969793    steps: 182     evaluation reward: 1.24\n",
      "episode: 2133   score: 3.0   memory length: 395076   epsilon: 0.31674754000969507    steps: 228     evaluation reward: 1.27\n",
      "episode: 2134   score: 0.0   memory length: 395199   epsilon: 0.31650400000969353    steps: 123     evaluation reward: 1.26\n",
      "episode: 2135   score: 0.0   memory length: 395322   epsilon: 0.316260460009692    steps: 123     evaluation reward: 1.24\n",
      "episode: 2136   score: 0.0   memory length: 395444   epsilon: 0.31601890000969046    steps: 122     evaluation reward: 1.23\n",
      "episode: 2137   score: 0.0   memory length: 395568   epsilon: 0.3157733800096889    steps: 124     evaluation reward: 1.21\n",
      "episode: 2138   score: 3.0   memory length: 395833   epsilon: 0.3152486800096856    steps: 265     evaluation reward: 1.24\n",
      "episode: 2139   score: 3.0   memory length: 396061   epsilon: 0.31479724000968273    steps: 228     evaluation reward: 1.27\n",
      "episode: 2140   score: 3.0   memory length: 396276   epsilon: 0.31437154000968004    steps: 215     evaluation reward: 1.3\n",
      "episode: 2141   score: 0.0   memory length: 396398   epsilon: 0.3141299800096785    steps: 122     evaluation reward: 1.3\n",
      "episode: 2142   score: 0.0   memory length: 396521   epsilon: 0.31388644000967697    steps: 123     evaluation reward: 1.28\n",
      "episode: 2143   score: 0.0   memory length: 396644   epsilon: 0.31364290000967543    steps: 123     evaluation reward: 1.27\n",
      "episode: 2144   score: 0.0   memory length: 396768   epsilon: 0.3133973800096739    steps: 124     evaluation reward: 1.27\n",
      "episode: 2145   score: 1.0   memory length: 396940   epsilon: 0.3130568200096717    steps: 172     evaluation reward: 1.25\n",
      "episode: 2146   score: 0.0   memory length: 397063   epsilon: 0.3128132800096702    steps: 123     evaluation reward: 1.24\n",
      "episode: 2147   score: 0.0   memory length: 397185   epsilon: 0.31257172000966865    steps: 122     evaluation reward: 1.21\n",
      "episode: 2148   score: 1.0   memory length: 397357   epsilon: 0.3122311600096665    steps: 172     evaluation reward: 1.2\n",
      "episode: 2149   score: 1.0   memory length: 397507   epsilon: 0.3119341600096646    steps: 150     evaluation reward: 1.19\n",
      "episode: 2150   score: 0.0   memory length: 397631   epsilon: 0.31168864000966306    steps: 124     evaluation reward: 1.18\n",
      "episode: 2151   score: 3.0   memory length: 397866   epsilon: 0.3112233400096601    steps: 235     evaluation reward: 1.19\n",
      "episode: 2152   score: 0.0   memory length: 397988   epsilon: 0.3109817800096586    steps: 122     evaluation reward: 1.19\n",
      "episode: 2153   score: 0.0   memory length: 398110   epsilon: 0.31074022000965706    steps: 122     evaluation reward: 1.16\n",
      "episode: 2154   score: 1.0   memory length: 398262   epsilon: 0.31043926000965516    steps: 152     evaluation reward: 1.15\n",
      "episode: 2155   score: 2.0   memory length: 398459   epsilon: 0.3100492000096527    steps: 197     evaluation reward: 1.14\n",
      "episode: 2156   score: 2.0   memory length: 398674   epsilon: 0.30962350000965    steps: 215     evaluation reward: 1.16\n",
      "episode: 2157   score: 0.0   memory length: 398797   epsilon: 0.30937996000964846    steps: 123     evaluation reward: 1.14\n",
      "episode: 2158   score: 2.0   memory length: 398995   epsilon: 0.308987920009646    steps: 198     evaluation reward: 1.14\n",
      "episode: 2159   score: 3.0   memory length: 399241   epsilon: 0.3085008400096429    steps: 246     evaluation reward: 1.16\n",
      "episode: 2160   score: 2.0   memory length: 399438   epsilon: 0.30811078000964043    steps: 197     evaluation reward: 1.16\n",
      "episode: 2161   score: 2.0   memory length: 399637   epsilon: 0.30771676000963794    steps: 199     evaluation reward: 1.18\n",
      "episode: 2162   score: 2.0   memory length: 399835   epsilon: 0.30732472000963545    steps: 198     evaluation reward: 1.18\n",
      "now time :  2019-09-25 17:45:23.830071\n",
      "episode: 2163   score: 1.0   memory length: 400006   epsilon: 0.3069861400096333    steps: 171     evaluation reward: 1.18\n",
      "episode: 2164   score: 0.0   memory length: 400128   epsilon: 0.3067445800096318    steps: 122     evaluation reward: 1.16\n",
      "episode: 2165   score: 2.0   memory length: 400325   epsilon: 0.3063545200096293    steps: 197     evaluation reward: 1.18\n",
      "episode: 2166   score: 0.0   memory length: 400450   epsilon: 0.30610702000962775    steps: 125     evaluation reward: 1.18\n",
      "episode: 2167   score: 1.0   memory length: 400619   epsilon: 0.30577240000962563    steps: 169     evaluation reward: 1.19\n",
      "episode: 2168   score: 1.0   memory length: 400770   epsilon: 0.30547342000962374    steps: 151     evaluation reward: 1.19\n",
      "episode: 2169   score: 3.0   memory length: 401016   epsilon: 0.30498634000962066    steps: 246     evaluation reward: 1.21\n",
      "episode: 2170   score: 2.0   memory length: 401215   epsilon: 0.30459232000961817    steps: 199     evaluation reward: 1.23\n",
      "episode: 2171   score: 1.0   memory length: 401383   epsilon: 0.30425968000961606    steps: 168     evaluation reward: 1.24\n",
      "episode: 2172   score: 1.0   memory length: 401553   epsilon: 0.30392308000961393    steps: 170     evaluation reward: 1.23\n",
      "episode: 2173   score: 3.0   memory length: 401799   epsilon: 0.30343600000961085    steps: 246     evaluation reward: 1.26\n",
      "episode: 2174   score: 4.0   memory length: 402075   epsilon: 0.3028895200096074    steps: 276     evaluation reward: 1.3\n",
      "episode: 2175   score: 0.0   memory length: 402198   epsilon: 0.30264598000960585    steps: 123     evaluation reward: 1.3\n",
      "episode: 2176   score: 0.0   memory length: 402320   epsilon: 0.3024044200096043    steps: 122     evaluation reward: 1.3\n",
      "episode: 2177   score: 2.0   memory length: 402517   epsilon: 0.30201436000960186    steps: 197     evaluation reward: 1.3\n",
      "episode: 2178   score: 0.0   memory length: 402639   epsilon: 0.30177280000960033    steps: 122     evaluation reward: 1.3\n",
      "episode: 2179   score: 0.0   memory length: 402763   epsilon: 0.3015272800095988    steps: 124     evaluation reward: 1.28\n",
      "episode: 2180   score: 6.0   memory length: 403122   epsilon: 0.3008164600095943    steps: 359     evaluation reward: 1.34\n",
      "episode: 2181   score: 1.0   memory length: 403272   epsilon: 0.3005194600095924    steps: 150     evaluation reward: 1.34\n",
      "episode: 2182   score: 2.0   memory length: 403469   epsilon: 0.30012940000958993    steps: 197     evaluation reward: 1.32\n",
      "episode: 2183   score: 0.0   memory length: 403592   epsilon: 0.2998858600095884    steps: 123     evaluation reward: 1.31\n",
      "episode: 2184   score: 0.0   memory length: 403716   epsilon: 0.29964034000958684    steps: 124     evaluation reward: 1.31\n",
      "episode: 2185   score: 2.0   memory length: 403914   epsilon: 0.29924830000958436    steps: 198     evaluation reward: 1.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2186   score: 0.0   memory length: 404037   epsilon: 0.2990047600095828    steps: 123     evaluation reward: 1.29\n",
      "episode: 2187   score: 0.0   memory length: 404159   epsilon: 0.2987632000095813    steps: 122     evaluation reward: 1.29\n",
      "episode: 2188   score: 1.0   memory length: 404330   epsilon: 0.29842462000957914    steps: 171     evaluation reward: 1.3\n",
      "episode: 2189   score: 2.0   memory length: 404527   epsilon: 0.2980345600095767    steps: 197     evaluation reward: 1.32\n",
      "episode: 2190   score: 0.0   memory length: 404649   epsilon: 0.29779300000957515    steps: 122     evaluation reward: 1.32\n",
      "episode: 2191   score: 2.0   memory length: 404847   epsilon: 0.29740096000957267    steps: 198     evaluation reward: 1.32\n",
      "episode: 2192   score: 2.0   memory length: 405045   epsilon: 0.2970089200095702    steps: 198     evaluation reward: 1.33\n",
      "episode: 2193   score: 1.0   memory length: 405214   epsilon: 0.29667430000956807    steps: 169     evaluation reward: 1.34\n",
      "episode: 2194   score: 0.0   memory length: 405336   epsilon: 0.29643274000956654    steps: 122     evaluation reward: 1.34\n",
      "episode: 2195   score: 4.0   memory length: 405572   epsilon: 0.2959654600095636    steps: 236     evaluation reward: 1.37\n",
      "episode: 2196   score: 0.0   memory length: 405695   epsilon: 0.29572192000956204    steps: 123     evaluation reward: 1.35\n",
      "episode: 2197   score: 2.0   memory length: 405893   epsilon: 0.29532988000955956    steps: 198     evaluation reward: 1.35\n",
      "episode: 2198   score: 1.0   memory length: 406043   epsilon: 0.2950328800095577    steps: 150     evaluation reward: 1.35\n",
      "episode: 2199   score: 3.0   memory length: 406271   epsilon: 0.29458144000955483    steps: 228     evaluation reward: 1.38\n",
      "episode: 2200   score: 2.0   memory length: 406468   epsilon: 0.29419138000955236    steps: 197     evaluation reward: 1.37\n",
      "episode: 2201   score: 0.0   memory length: 406590   epsilon: 0.29394982000955083    steps: 122     evaluation reward: 1.35\n",
      "episode: 2202   score: 0.0   memory length: 406713   epsilon: 0.2937062800095493    steps: 123     evaluation reward: 1.33\n",
      "episode: 2203   score: 0.0   memory length: 406835   epsilon: 0.29346472000954776    steps: 122     evaluation reward: 1.32\n",
      "episode: 2204   score: 0.0   memory length: 406957   epsilon: 0.29322316000954624    steps: 122     evaluation reward: 1.28\n",
      "episode: 2205   score: 0.0   memory length: 407080   epsilon: 0.2929796200095447    steps: 123     evaluation reward: 1.26\n",
      "episode: 2206   score: 0.0   memory length: 407203   epsilon: 0.29273608000954315    steps: 123     evaluation reward: 1.25\n",
      "episode: 2207   score: 2.0   memory length: 407400   epsilon: 0.2923460200095407    steps: 197     evaluation reward: 1.25\n",
      "episode: 2208   score: 1.0   memory length: 407569   epsilon: 0.29201140000953857    steps: 169     evaluation reward: 1.26\n",
      "episode: 2209   score: 2.0   memory length: 407766   epsilon: 0.2916213400095361    steps: 197     evaluation reward: 1.27\n",
      "episode: 2210   score: 0.0   memory length: 407888   epsilon: 0.2913797800095346    steps: 122     evaluation reward: 1.27\n",
      "episode: 2211   score: 1.0   memory length: 408040   epsilon: 0.29107882000953267    steps: 152     evaluation reward: 1.26\n",
      "episode: 2212   score: 2.0   memory length: 408239   epsilon: 0.2906848000095302    steps: 199     evaluation reward: 1.27\n",
      "episode: 2213   score: 0.0   memory length: 408361   epsilon: 0.29044324000952865    steps: 122     evaluation reward: 1.24\n",
      "episode: 2214   score: 2.0   memory length: 408579   epsilon: 0.2900116000095259    steps: 218     evaluation reward: 1.24\n",
      "episode: 2215   score: 2.0   memory length: 408776   epsilon: 0.28962154000952345    steps: 197     evaluation reward: 1.23\n",
      "episode: 2216   score: 0.0   memory length: 408899   epsilon: 0.2893780000095219    steps: 123     evaluation reward: 1.21\n",
      "episode: 2217   score: 5.0   memory length: 409201   epsilon: 0.2887800400095181    steps: 302     evaluation reward: 1.26\n",
      "episode: 2218   score: 1.0   memory length: 409351   epsilon: 0.28848304000951625    steps: 150     evaluation reward: 1.24\n",
      "episode: 2219   score: 2.0   memory length: 409548   epsilon: 0.2880929800095138    steps: 197     evaluation reward: 1.24\n",
      "episode: 2220   score: 3.0   memory length: 409775   epsilon: 0.28764352000951093    steps: 227     evaluation reward: 1.25\n",
      "episode: 2221   score: 1.0   memory length: 409925   epsilon: 0.28734652000950905    steps: 150     evaluation reward: 1.26\n",
      "episode: 2222   score: 0.0   memory length: 410048   epsilon: 0.2871029800095075    steps: 123     evaluation reward: 1.25\n",
      "episode: 2223   score: 0.0   memory length: 410170   epsilon: 0.286861420009506    steps: 122     evaluation reward: 1.25\n",
      "episode: 2224   score: 1.0   memory length: 410321   epsilon: 0.2865624400095041    steps: 151     evaluation reward: 1.26\n",
      "episode: 2225   score: 2.0   memory length: 410518   epsilon: 0.2861723800095016    steps: 197     evaluation reward: 1.27\n",
      "episode: 2226   score: 0.0   memory length: 410640   epsilon: 0.2859308200095001    steps: 122     evaluation reward: 1.24\n",
      "episode: 2227   score: 2.0   memory length: 410821   epsilon: 0.28557244000949783    steps: 181     evaluation reward: 1.23\n",
      "episode: 2228   score: 2.0   memory length: 411020   epsilon: 0.28517842000949534    steps: 199     evaluation reward: 1.23\n",
      "episode: 2229   score: 0.0   memory length: 411142   epsilon: 0.2849368600094938    steps: 122     evaluation reward: 1.21\n",
      "episode: 2230   score: 2.0   memory length: 411339   epsilon: 0.28454680000949134    steps: 197     evaluation reward: 1.22\n",
      "episode: 2231   score: 3.0   memory length: 411612   epsilon: 0.2840062600094879    steps: 273     evaluation reward: 1.24\n",
      "episode: 2232   score: 0.0   memory length: 411735   epsilon: 0.2837627200094864    steps: 123     evaluation reward: 1.22\n",
      "episode: 2233   score: 5.0   memory length: 412074   epsilon: 0.28309150000948213    steps: 339     evaluation reward: 1.24\n",
      "episode: 2234   score: 1.0   memory length: 412225   epsilon: 0.28279252000948024    steps: 151     evaluation reward: 1.25\n",
      "episode: 2235   score: 0.0   memory length: 412348   epsilon: 0.2825489800094787    steps: 123     evaluation reward: 1.25\n",
      "episode: 2236   score: 2.0   memory length: 412528   epsilon: 0.28219258000947645    steps: 180     evaluation reward: 1.27\n",
      "episode: 2237   score: 1.0   memory length: 412678   epsilon: 0.28189558000947457    steps: 150     evaluation reward: 1.28\n",
      "episode: 2238   score: 3.0   memory length: 412909   epsilon: 0.2814382000094717    steps: 231     evaluation reward: 1.28\n",
      "episode: 2239   score: 0.0   memory length: 413031   epsilon: 0.28119664000947014    steps: 122     evaluation reward: 1.25\n",
      "episode: 2240   score: 0.0   memory length: 413153   epsilon: 0.2809550800094686    steps: 122     evaluation reward: 1.22\n",
      "episode: 2241   score: 2.0   memory length: 413351   epsilon: 0.28056304000946614    steps: 198     evaluation reward: 1.24\n",
      "episode: 2242   score: 0.0   memory length: 413474   epsilon: 0.2803195000094646    steps: 123     evaluation reward: 1.24\n",
      "episode: 2243   score: 1.0   memory length: 413649   epsilon: 0.2799730000094624    steps: 175     evaluation reward: 1.25\n",
      "episode: 2244   score: 1.0   memory length: 413819   epsilon: 0.2796364000094603    steps: 170     evaluation reward: 1.26\n",
      "episode: 2245   score: 1.0   memory length: 413990   epsilon: 0.27929782000945813    steps: 171     evaluation reward: 1.26\n",
      "episode: 2246   score: 1.0   memory length: 414161   epsilon: 0.278959240009456    steps: 171     evaluation reward: 1.27\n",
      "episode: 2247   score: 0.0   memory length: 414285   epsilon: 0.27871372000945444    steps: 124     evaluation reward: 1.27\n",
      "episode: 2248   score: 0.0   memory length: 414408   epsilon: 0.2784701800094529    steps: 123     evaluation reward: 1.26\n",
      "episode: 2249   score: 3.0   memory length: 414634   epsilon: 0.27802270000945006    steps: 226     evaluation reward: 1.28\n",
      "episode: 2250   score: 1.0   memory length: 414804   epsilon: 0.27768610000944793    steps: 170     evaluation reward: 1.29\n",
      "episode: 2251   score: 0.0   memory length: 414927   epsilon: 0.2774425600094464    steps: 123     evaluation reward: 1.26\n",
      "episode: 2252   score: 1.0   memory length: 415077   epsilon: 0.2771455600094445    steps: 150     evaluation reward: 1.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2253   score: 2.0   memory length: 415276   epsilon: 0.276751540009442    steps: 199     evaluation reward: 1.29\n",
      "episode: 2254   score: 0.0   memory length: 415400   epsilon: 0.27650602000944047    steps: 124     evaluation reward: 1.28\n",
      "episode: 2255   score: 1.0   memory length: 415570   epsilon: 0.27616942000943834    steps: 170     evaluation reward: 1.27\n",
      "episode: 2256   score: 4.0   memory length: 415846   epsilon: 0.2756229400094349    steps: 276     evaluation reward: 1.29\n",
      "episode: 2257   score: 1.0   memory length: 416014   epsilon: 0.2752903000094328    steps: 168     evaluation reward: 1.3\n",
      "episode: 2258   score: 0.0   memory length: 416136   epsilon: 0.27504874000943125    steps: 122     evaluation reward: 1.28\n",
      "episode: 2259   score: 0.0   memory length: 416259   epsilon: 0.2748052000094297    steps: 123     evaluation reward: 1.25\n",
      "episode: 2260   score: 0.0   memory length: 416384   epsilon: 0.27455770000942814    steps: 125     evaluation reward: 1.23\n",
      "episode: 2261   score: 2.0   memory length: 416582   epsilon: 0.27416566000942566    steps: 198     evaluation reward: 1.23\n",
      "episode: 2262   score: 2.0   memory length: 416779   epsilon: 0.2737756000094232    steps: 197     evaluation reward: 1.23\n",
      "episode: 2263   score: 2.0   memory length: 416976   epsilon: 0.2733855400094207    steps: 197     evaluation reward: 1.24\n",
      "episode: 2264   score: 2.0   memory length: 417173   epsilon: 0.27299548000941826    steps: 197     evaluation reward: 1.26\n",
      "episode: 2265   score: 1.0   memory length: 417323   epsilon: 0.2726984800094164    steps: 150     evaluation reward: 1.25\n",
      "episode: 2266   score: 0.0   memory length: 417446   epsilon: 0.27245494000941484    steps: 123     evaluation reward: 1.25\n",
      "episode: 2267   score: 0.0   memory length: 417570   epsilon: 0.2722094200094133    steps: 124     evaluation reward: 1.24\n",
      "episode: 2268   score: 1.0   memory length: 417742   epsilon: 0.27186886000941113    steps: 172     evaluation reward: 1.24\n",
      "episode: 2269   score: 0.0   memory length: 417865   epsilon: 0.2716253200094096    steps: 123     evaluation reward: 1.21\n",
      "episode: 2270   score: 1.0   memory length: 418017   epsilon: 0.2713243600094077    steps: 152     evaluation reward: 1.2\n",
      "episode: 2271   score: 1.0   memory length: 418167   epsilon: 0.2710273600094058    steps: 150     evaluation reward: 1.2\n",
      "episode: 2272   score: 1.0   memory length: 418336   epsilon: 0.2706927400094037    steps: 169     evaluation reward: 1.2\n",
      "episode: 2273   score: 0.0   memory length: 418459   epsilon: 0.27044920000940215    steps: 123     evaluation reward: 1.17\n",
      "episode: 2274   score: 0.0   memory length: 418581   epsilon: 0.2702076400094006    steps: 122     evaluation reward: 1.13\n",
      "episode: 2275   score: 2.0   memory length: 418780   epsilon: 0.2698136200093981    steps: 199     evaluation reward: 1.15\n",
      "episode: 2276   score: 1.0   memory length: 418930   epsilon: 0.26951662000939625    steps: 150     evaluation reward: 1.16\n",
      "episode: 2277   score: 2.0   memory length: 419129   epsilon: 0.26912260000939375    steps: 199     evaluation reward: 1.16\n",
      "episode: 2278   score: 1.0   memory length: 419279   epsilon: 0.2688256000093919    steps: 150     evaluation reward: 1.17\n",
      "episode: 2279   score: 0.0   memory length: 419403   epsilon: 0.2685800800093903    steps: 124     evaluation reward: 1.17\n",
      "episode: 2280   score: 2.0   memory length: 419600   epsilon: 0.26819002000938785    steps: 197     evaluation reward: 1.13\n",
      "episode: 2281   score: 2.0   memory length: 419820   epsilon: 0.2677544200093851    steps: 220     evaluation reward: 1.14\n",
      "episode: 2282   score: 0.0   memory length: 419942   epsilon: 0.26751286000938357    steps: 122     evaluation reward: 1.12\n",
      "episode: 2283   score: 0.0   memory length: 420065   epsilon: 0.267269320009382    steps: 123     evaluation reward: 1.12\n",
      "episode: 2284   score: 1.0   memory length: 420218   epsilon: 0.2669663800093801    steps: 153     evaluation reward: 1.13\n",
      "episode: 2285   score: 0.0   memory length: 420340   epsilon: 0.2667248200093786    steps: 122     evaluation reward: 1.11\n",
      "episode: 2286   score: 0.0   memory length: 420462   epsilon: 0.26648326000937705    steps: 122     evaluation reward: 1.11\n",
      "episode: 2287   score: 0.0   memory length: 420584   epsilon: 0.2662417000093755    steps: 122     evaluation reward: 1.11\n",
      "episode: 2288   score: 1.0   memory length: 420752   epsilon: 0.2659090600093734    steps: 168     evaluation reward: 1.11\n",
      "episode: 2289   score: 1.0   memory length: 420904   epsilon: 0.2656081000093715    steps: 152     evaluation reward: 1.1\n",
      "episode: 2290   score: 0.0   memory length: 421026   epsilon: 0.26536654000937    steps: 122     evaluation reward: 1.1\n",
      "episode: 2291   score: 1.0   memory length: 421176   epsilon: 0.2650695400093681    steps: 150     evaluation reward: 1.09\n",
      "episode: 2292   score: 2.0   memory length: 421375   epsilon: 0.2646755200093656    steps: 199     evaluation reward: 1.09\n",
      "episode: 2293   score: 1.0   memory length: 421526   epsilon: 0.2643765400093637    steps: 151     evaluation reward: 1.09\n",
      "episode: 2294   score: 1.0   memory length: 421676   epsilon: 0.26407954000936185    steps: 150     evaluation reward: 1.1\n",
      "episode: 2295   score: 2.0   memory length: 421894   epsilon: 0.2636479000093591    steps: 218     evaluation reward: 1.08\n",
      "episode: 2296   score: 1.0   memory length: 422046   epsilon: 0.2633469400093572    steps: 152     evaluation reward: 1.09\n",
      "episode: 2297   score: 0.0   memory length: 422168   epsilon: 0.2631053800093557    steps: 122     evaluation reward: 1.07\n",
      "episode: 2298   score: 0.0   memory length: 422290   epsilon: 0.26286382000935415    steps: 122     evaluation reward: 1.06\n",
      "episode: 2299   score: 3.0   memory length: 422518   epsilon: 0.2624123800093513    steps: 228     evaluation reward: 1.06\n",
      "episode: 2300   score: 0.0   memory length: 422640   epsilon: 0.26217082000934977    steps: 122     evaluation reward: 1.04\n",
      "episode: 2301   score: 1.0   memory length: 422790   epsilon: 0.2618738200093479    steps: 150     evaluation reward: 1.05\n",
      "episode: 2302   score: 2.0   memory length: 422987   epsilon: 0.2614837600093454    steps: 197     evaluation reward: 1.07\n",
      "episode: 2303   score: 3.0   memory length: 423215   epsilon: 0.26103232000934257    steps: 228     evaluation reward: 1.1\n",
      "episode: 2304   score: 1.0   memory length: 423366   epsilon: 0.2607333400093407    steps: 151     evaluation reward: 1.11\n",
      "episode: 2305   score: 1.0   memory length: 423516   epsilon: 0.2604363400093388    steps: 150     evaluation reward: 1.12\n",
      "episode: 2306   score: 0.0   memory length: 423639   epsilon: 0.26019280000933726    steps: 123     evaluation reward: 1.12\n",
      "episode: 2307   score: 1.0   memory length: 423790   epsilon: 0.25989382000933536    steps: 151     evaluation reward: 1.11\n",
      "episode: 2308   score: 0.0   memory length: 423913   epsilon: 0.2596502800093338    steps: 123     evaluation reward: 1.1\n",
      "episode: 2309   score: 2.0   memory length: 424092   epsilon: 0.2592958600093316    steps: 179     evaluation reward: 1.1\n",
      "episode: 2310   score: 2.0   memory length: 424290   epsilon: 0.2589038200093291    steps: 198     evaluation reward: 1.12\n",
      "episode: 2311   score: 1.0   memory length: 424443   epsilon: 0.2586008800093272    steps: 153     evaluation reward: 1.12\n",
      "episode: 2312   score: 1.0   memory length: 424594   epsilon: 0.2583019000093253    steps: 151     evaluation reward: 1.11\n",
      "episode: 2313   score: 1.0   memory length: 424762   epsilon: 0.2579692600093232    steps: 168     evaluation reward: 1.12\n",
      "episode: 2314   score: 1.0   memory length: 424931   epsilon: 0.25763464000932107    steps: 169     evaluation reward: 1.11\n",
      "episode: 2315   score: 3.0   memory length: 425176   epsilon: 0.257149540009318    steps: 245     evaluation reward: 1.12\n",
      "episode: 2316   score: 0.0   memory length: 425299   epsilon: 0.25690600000931646    steps: 123     evaluation reward: 1.12\n",
      "episode: 2317   score: 1.0   memory length: 425468   epsilon: 0.25657138000931434    steps: 169     evaluation reward: 1.08\n",
      "episode: 2318   score: 1.0   memory length: 425638   epsilon: 0.2562347800093122    steps: 170     evaluation reward: 1.08\n",
      "episode: 2319   score: 0.0   memory length: 425760   epsilon: 0.2559932200093107    steps: 122     evaluation reward: 1.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2320   score: 0.0   memory length: 425882   epsilon: 0.25575166000930916    steps: 122     evaluation reward: 1.03\n",
      "episode: 2321   score: 2.0   memory length: 426079   epsilon: 0.2553616000093067    steps: 197     evaluation reward: 1.04\n",
      "episode: 2322   score: 1.0   memory length: 426229   epsilon: 0.2550646000093048    steps: 150     evaluation reward: 1.05\n",
      "episode: 2323   score: 0.0   memory length: 426351   epsilon: 0.2548230400093033    steps: 122     evaluation reward: 1.05\n",
      "episode: 2324   score: 0.0   memory length: 426474   epsilon: 0.25457950000930174    steps: 123     evaluation reward: 1.04\n",
      "episode: 2325   score: 2.0   memory length: 426671   epsilon: 0.2541894400092993    steps: 197     evaluation reward: 1.04\n",
      "episode: 2326   score: 2.0   memory length: 426868   epsilon: 0.2537993800092968    steps: 197     evaluation reward: 1.06\n",
      "episode: 2327   score: 0.0   memory length: 426990   epsilon: 0.2535578200092953    steps: 122     evaluation reward: 1.04\n",
      "episode: 2328   score: 0.0   memory length: 427112   epsilon: 0.25331626000929375    steps: 122     evaluation reward: 1.02\n",
      "episode: 2329   score: 2.0   memory length: 427309   epsilon: 0.2529262000092913    steps: 197     evaluation reward: 1.04\n",
      "episode: 2330   score: 0.0   memory length: 427431   epsilon: 0.25268464000928975    steps: 122     evaluation reward: 1.02\n",
      "episode: 2331   score: 2.0   memory length: 427648   epsilon: 0.25225498000928703    steps: 217     evaluation reward: 1.01\n",
      "episode: 2332   score: 3.0   memory length: 427873   epsilon: 0.2518094800092842    steps: 225     evaluation reward: 1.04\n",
      "episode: 2333   score: 0.0   memory length: 427995   epsilon: 0.2515679200092827    steps: 122     evaluation reward: 0.99\n",
      "episode: 2334   score: 0.0   memory length: 428121   epsilon: 0.2513184400092811    steps: 126     evaluation reward: 0.98\n",
      "episode: 2335   score: 3.0   memory length: 428346   epsilon: 0.2508729400092783    steps: 225     evaluation reward: 1.01\n",
      "episode: 2336   score: 2.0   memory length: 428544   epsilon: 0.2504809000092758    steps: 198     evaluation reward: 1.01\n",
      "episode: 2337   score: 2.0   memory length: 428743   epsilon: 0.2500868800092733    steps: 199     evaluation reward: 1.02\n",
      "episode: 2338   score: 0.0   memory length: 428866   epsilon: 0.24984334000927177    steps: 123     evaluation reward: 0.99\n",
      "episode: 2339   score: 0.0   memory length: 428989   epsilon: 0.24959980000927023    steps: 123     evaluation reward: 0.99\n",
      "episode: 2340   score: 0.0   memory length: 429111   epsilon: 0.2493582400092687    steps: 122     evaluation reward: 0.99\n",
      "episode: 2341   score: 0.0   memory length: 429233   epsilon: 0.24911668000926718    steps: 122     evaluation reward: 0.97\n",
      "episode: 2342   score: 0.0   memory length: 429355   epsilon: 0.24887512000926565    steps: 122     evaluation reward: 0.97\n",
      "episode: 2343   score: 0.0   memory length: 429478   epsilon: 0.2486315800092641    steps: 123     evaluation reward: 0.96\n",
      "episode: 2344   score: 0.0   memory length: 429600   epsilon: 0.24839002000926258    steps: 122     evaluation reward: 0.95\n",
      "episode: 2345   score: 2.0   memory length: 429782   epsilon: 0.2480296600092603    steps: 182     evaluation reward: 0.96\n",
      "episode: 2346   score: 0.0   memory length: 429904   epsilon: 0.24778810000925877    steps: 122     evaluation reward: 0.95\n",
      "episode: 2347   score: 1.0   memory length: 430073   epsilon: 0.24745348000925665    steps: 169     evaluation reward: 0.96\n",
      "episode: 2348   score: 0.0   memory length: 430198   epsilon: 0.2472059800092551    steps: 125     evaluation reward: 0.96\n",
      "episode: 2349   score: 1.0   memory length: 430366   epsilon: 0.24687334000925298    steps: 168     evaluation reward: 0.94\n",
      "episode: 2350   score: 0.0   memory length: 430488   epsilon: 0.24663178000925146    steps: 122     evaluation reward: 0.93\n",
      "episode: 2351   score: 2.0   memory length: 430685   epsilon: 0.246241720009249    steps: 197     evaluation reward: 0.95\n",
      "episode: 2352   score: 1.0   memory length: 430837   epsilon: 0.24594076000924708    steps: 152     evaluation reward: 0.95\n",
      "episode: 2353   score: 2.0   memory length: 431034   epsilon: 0.24555070000924462    steps: 197     evaluation reward: 0.95\n",
      "episode: 2354   score: 0.0   memory length: 431158   epsilon: 0.24530518000924306    steps: 124     evaluation reward: 0.95\n",
      "episode: 2355   score: 1.0   memory length: 431308   epsilon: 0.24500818000924118    steps: 150     evaluation reward: 0.95\n",
      "episode: 2356   score: 0.0   memory length: 431431   epsilon: 0.24476464000923964    steps: 123     evaluation reward: 0.91\n",
      "episode: 2357   score: 0.0   memory length: 431553   epsilon: 0.24452308000923811    steps: 122     evaluation reward: 0.9\n",
      "episode: 2358   score: 0.0   memory length: 431675   epsilon: 0.24428152000923659    steps: 122     evaluation reward: 0.9\n",
      "episode: 2359   score: 2.0   memory length: 431893   epsilon: 0.24384988000923385    steps: 218     evaluation reward: 0.92\n",
      "episode: 2360   score: 1.0   memory length: 432066   epsilon: 0.2435073400092317    steps: 173     evaluation reward: 0.93\n",
      "episode: 2361   score: 0.0   memory length: 432188   epsilon: 0.24326578000923016    steps: 122     evaluation reward: 0.91\n",
      "episode: 2362   score: 1.0   memory length: 432341   epsilon: 0.24296284000922824    steps: 153     evaluation reward: 0.9\n",
      "episode: 2363   score: 0.0   memory length: 432464   epsilon: 0.2427193000092267    steps: 123     evaluation reward: 0.88\n",
      "episode: 2364   score: 0.0   memory length: 432586   epsilon: 0.24247774000922517    steps: 122     evaluation reward: 0.86\n",
      "episode: 2365   score: 0.0   memory length: 432708   epsilon: 0.24223618000922365    steps: 122     evaluation reward: 0.85\n",
      "episode: 2366   score: 1.0   memory length: 432858   epsilon: 0.24193918000922177    steps: 150     evaluation reward: 0.86\n",
      "episode: 2367   score: 0.0   memory length: 432981   epsilon: 0.24169564000922023    steps: 123     evaluation reward: 0.86\n",
      "episode: 2368   score: 0.0   memory length: 433104   epsilon: 0.24145210000921868    steps: 123     evaluation reward: 0.85\n",
      "episode: 2369   score: 3.0   memory length: 433349   epsilon: 0.24096700000921562    steps: 245     evaluation reward: 0.88\n",
      "episode: 2370   score: 1.0   memory length: 433519   epsilon: 0.24063040000921349    steps: 170     evaluation reward: 0.88\n",
      "episode: 2371   score: 2.0   memory length: 433703   epsilon: 0.24026608000921118    steps: 184     evaluation reward: 0.89\n",
      "episode: 2372   score: 3.0   memory length: 433950   epsilon: 0.23977702000920809    steps: 247     evaluation reward: 0.91\n",
      "episode: 2373   score: 1.0   memory length: 434100   epsilon: 0.2394800200092062    steps: 150     evaluation reward: 0.92\n",
      "episode: 2374   score: 1.0   memory length: 434269   epsilon: 0.2391454000092041    steps: 169     evaluation reward: 0.93\n",
      "episode: 2375   score: 1.0   memory length: 434441   epsilon: 0.23880484000920194    steps: 172     evaluation reward: 0.92\n",
      "episode: 2376   score: 1.0   memory length: 434591   epsilon: 0.23850784000920006    steps: 150     evaluation reward: 0.92\n",
      "episode: 2377   score: 0.0   memory length: 434713   epsilon: 0.23826628000919853    steps: 122     evaluation reward: 0.9\n",
      "episode: 2378   score: 0.0   memory length: 434835   epsilon: 0.238024720009197    steps: 122     evaluation reward: 0.89\n",
      "episode: 2379   score: 2.0   memory length: 435055   epsilon: 0.23758912000919424    steps: 220     evaluation reward: 0.91\n",
      "episode: 2380   score: 2.0   memory length: 435252   epsilon: 0.23719906000919178    steps: 197     evaluation reward: 0.91\n",
      "episode: 2381   score: 0.0   memory length: 435375   epsilon: 0.23695552000919023    steps: 123     evaluation reward: 0.89\n",
      "episode: 2382   score: 0.0   memory length: 435497   epsilon: 0.2367139600091887    steps: 122     evaluation reward: 0.89\n",
      "episode: 2383   score: 2.0   memory length: 435694   epsilon: 0.23632390000918624    steps: 197     evaluation reward: 0.91\n",
      "episode: 2384   score: 0.0   memory length: 435817   epsilon: 0.2360803600091847    steps: 123     evaluation reward: 0.9\n",
      "episode: 2385   score: 0.0   memory length: 435939   epsilon: 0.23583880000918317    steps: 122     evaluation reward: 0.9\n",
      "episode: 2386   score: 0.0   memory length: 436062   epsilon: 0.23559526000918163    steps: 123     evaluation reward: 0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2387   score: 0.0   memory length: 436185   epsilon: 0.2353517200091801    steps: 123     evaluation reward: 0.9\n",
      "episode: 2388   score: 1.0   memory length: 436353   epsilon: 0.23501908000917798    steps: 168     evaluation reward: 0.9\n",
      "episode: 2389   score: 1.0   memory length: 436504   epsilon: 0.2347201000091761    steps: 151     evaluation reward: 0.9\n",
      "episode: 2390   score: 4.0   memory length: 436801   epsilon: 0.23413204000917237    steps: 297     evaluation reward: 0.94\n",
      "episode: 2391   score: 2.0   memory length: 437019   epsilon: 0.23370040000916964    steps: 218     evaluation reward: 0.95\n",
      "episode: 2392   score: 2.0   memory length: 437219   epsilon: 0.23330440000916713    steps: 200     evaluation reward: 0.95\n",
      "episode: 2393   score: 3.0   memory length: 437466   epsilon: 0.23281534000916404    steps: 247     evaluation reward: 0.97\n",
      "episode: 2394   score: 1.0   memory length: 437618   epsilon: 0.23251438000916214    steps: 152     evaluation reward: 0.97\n",
      "episode: 2395   score: 3.0   memory length: 437864   epsilon: 0.23202730000915905    steps: 246     evaluation reward: 0.98\n",
      "episode: 2396   score: 3.0   memory length: 438092   epsilon: 0.2315758600091562    steps: 228     evaluation reward: 1.0\n",
      "episode: 2397   score: 1.0   memory length: 438263   epsilon: 0.23123728000915406    steps: 171     evaluation reward: 1.01\n",
      "episode: 2398   score: 1.0   memory length: 438432   epsilon: 0.23090266000915194    steps: 169     evaluation reward: 1.02\n",
      "episode: 2399   score: 0.0   memory length: 438555   epsilon: 0.2306591200091504    steps: 123     evaluation reward: 0.99\n",
      "episode: 2400   score: 0.0   memory length: 438678   epsilon: 0.23041558000914886    steps: 123     evaluation reward: 0.99\n",
      "episode: 2401   score: 0.0   memory length: 438801   epsilon: 0.23017204000914732    steps: 123     evaluation reward: 0.98\n",
      "episode: 2402   score: 2.0   memory length: 438998   epsilon: 0.22978198000914485    steps: 197     evaluation reward: 0.98\n",
      "episode: 2403   score: 2.0   memory length: 439196   epsilon: 0.22938994000914237    steps: 198     evaluation reward: 0.97\n",
      "episode: 2404   score: 2.0   memory length: 439393   epsilon: 0.2289998800091399    steps: 197     evaluation reward: 0.98\n",
      "episode: 2405   score: 1.0   memory length: 439543   epsilon: 0.22870288000913802    steps: 150     evaluation reward: 0.98\n",
      "episode: 2406   score: 0.0   memory length: 439665   epsilon: 0.2284613200091365    steps: 122     evaluation reward: 0.98\n",
      "episode: 2407   score: 2.0   memory length: 439863   epsilon: 0.228069280009134    steps: 198     evaluation reward: 0.99\n",
      "episode: 2408   score: 0.0   memory length: 439986   epsilon: 0.22782574000913247    steps: 123     evaluation reward: 0.99\n",
      "episode: 2409   score: 3.0   memory length: 440211   epsilon: 0.22738024000912965    steps: 225     evaluation reward: 1.0\n",
      "episode: 2410   score: 0.0   memory length: 440333   epsilon: 0.22713868000912812    steps: 122     evaluation reward: 0.98\n",
      "episode: 2411   score: 1.0   memory length: 440502   epsilon: 0.226804060009126    steps: 169     evaluation reward: 0.98\n",
      "episode: 2412   score: 0.0   memory length: 440624   epsilon: 0.22656250000912448    steps: 122     evaluation reward: 0.97\n",
      "episode: 2413   score: 0.0   memory length: 440746   epsilon: 0.22632094000912295    steps: 122     evaluation reward: 0.96\n",
      "episode: 2414   score: 2.0   memory length: 440967   epsilon: 0.22588336000912018    steps: 221     evaluation reward: 0.97\n",
      "episode: 2415   score: 2.0   memory length: 441164   epsilon: 0.22549330000911771    steps: 197     evaluation reward: 0.96\n",
      "episode: 2416   score: 0.0   memory length: 441286   epsilon: 0.22525174000911619    steps: 122     evaluation reward: 0.96\n",
      "episode: 2417   score: 0.0   memory length: 441408   epsilon: 0.22501018000911466    steps: 122     evaluation reward: 0.95\n",
      "episode: 2418   score: 0.0   memory length: 441532   epsilon: 0.2247646600091131    steps: 124     evaluation reward: 0.94\n",
      "episode: 2419   score: 0.0   memory length: 441654   epsilon: 0.22452310000911158    steps: 122     evaluation reward: 0.94\n",
      "episode: 2420   score: 2.0   memory length: 441833   epsilon: 0.22416868000910933    steps: 179     evaluation reward: 0.96\n",
      "episode: 2421   score: 0.0   memory length: 441955   epsilon: 0.2239271200091078    steps: 122     evaluation reward: 0.94\n",
      "episode: 2422   score: 1.0   memory length: 442105   epsilon: 0.22363012000910593    steps: 150     evaluation reward: 0.94\n",
      "episode: 2423   score: 0.0   memory length: 442229   epsilon: 0.22338460000910437    steps: 124     evaluation reward: 0.94\n",
      "episode: 2424   score: 1.0   memory length: 442398   epsilon: 0.22304998000910226    steps: 169     evaluation reward: 0.95\n",
      "episode: 2425   score: 2.0   memory length: 442598   epsilon: 0.22265398000909975    steps: 200     evaluation reward: 0.95\n",
      "episode: 2426   score: 0.0   memory length: 442720   epsilon: 0.22241242000909822    steps: 122     evaluation reward: 0.93\n",
      "episode: 2427   score: 2.0   memory length: 442899   epsilon: 0.22205800000909598    steps: 179     evaluation reward: 0.95\n",
      "episode: 2428   score: 0.0   memory length: 443022   epsilon: 0.22181446000909444    steps: 123     evaluation reward: 0.95\n",
      "episode: 2429   score: 4.0   memory length: 443294   epsilon: 0.22127590000909103    steps: 272     evaluation reward: 0.97\n",
      "episode: 2430   score: 0.0   memory length: 443416   epsilon: 0.2210343400090895    steps: 122     evaluation reward: 0.97\n",
      "episode: 2431   score: 0.0   memory length: 443538   epsilon: 0.22079278000908797    steps: 122     evaluation reward: 0.95\n",
      "episode: 2432   score: 0.0   memory length: 443660   epsilon: 0.22055122000908645    steps: 122     evaluation reward: 0.92\n",
      "episode: 2433   score: 2.0   memory length: 443878   epsilon: 0.22011958000908372    steps: 218     evaluation reward: 0.94\n",
      "episode: 2434   score: 0.0   memory length: 444002   epsilon: 0.21987406000908216    steps: 124     evaluation reward: 0.94\n",
      "episode: 2435   score: 0.0   memory length: 444124   epsilon: 0.21963250000908063    steps: 122     evaluation reward: 0.91\n",
      "episode: 2436   score: 2.0   memory length: 444322   epsilon: 0.21924046000907815    steps: 198     evaluation reward: 0.91\n",
      "episode: 2437   score: 2.0   memory length: 444519   epsilon: 0.21885040000907569    steps: 197     evaluation reward: 0.91\n",
      "episode: 2438   score: 1.0   memory length: 444687   epsilon: 0.21851776000907358    steps: 168     evaluation reward: 0.92\n",
      "episode: 2439   score: 1.0   memory length: 444855   epsilon: 0.21818512000907148    steps: 168     evaluation reward: 0.93\n",
      "episode: 2440   score: 2.0   memory length: 445052   epsilon: 0.217795060009069    steps: 197     evaluation reward: 0.95\n",
      "episode: 2441   score: 0.0   memory length: 445175   epsilon: 0.21755152000906747    steps: 123     evaluation reward: 0.95\n",
      "episode: 2442   score: 0.0   memory length: 445297   epsilon: 0.21730996000906594    steps: 122     evaluation reward: 0.95\n",
      "episode: 2443   score: 0.0   memory length: 445419   epsilon: 0.2170684000090644    steps: 122     evaluation reward: 0.95\n",
      "episode: 2444   score: 1.0   memory length: 445587   epsilon: 0.2167357600090623    steps: 168     evaluation reward: 0.96\n",
      "episode: 2445   score: 2.0   memory length: 445804   epsilon: 0.2163061000090596    steps: 217     evaluation reward: 0.96\n",
      "episode: 2446   score: 1.0   memory length: 445956   epsilon: 0.21600514000905768    steps: 152     evaluation reward: 0.97\n",
      "episode: 2447   score: 2.0   memory length: 446154   epsilon: 0.2156131000090552    steps: 198     evaluation reward: 0.98\n",
      "episode: 2448   score: 0.0   memory length: 446276   epsilon: 0.21537154000905367    steps: 122     evaluation reward: 0.98\n",
      "episode: 2449   score: 1.0   memory length: 446445   epsilon: 0.21503692000905156    steps: 169     evaluation reward: 0.98\n",
      "episode: 2450   score: 1.0   memory length: 446595   epsilon: 0.21473992000904968    steps: 150     evaluation reward: 0.99\n",
      "episode: 2451   score: 1.0   memory length: 446763   epsilon: 0.21440728000904757    steps: 168     evaluation reward: 0.98\n",
      "episode: 2452   score: 1.0   memory length: 446913   epsilon: 0.2141102800090457    steps: 150     evaluation reward: 0.98\n",
      "episode: 2453   score: 2.0   memory length: 447113   epsilon: 0.2137142800090432    steps: 200     evaluation reward: 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2454   score: 2.0   memory length: 447310   epsilon: 0.21332422000904072    steps: 197     evaluation reward: 1.0\n",
      "episode: 2455   score: 0.0   memory length: 447433   epsilon: 0.21308068000903918    steps: 123     evaluation reward: 0.99\n",
      "episode: 2456   score: 1.0   memory length: 447583   epsilon: 0.2127836800090373    steps: 150     evaluation reward: 1.0\n",
      "episode: 2457   score: 1.0   memory length: 447736   epsilon: 0.21248074000903538    steps: 153     evaluation reward: 1.01\n",
      "episode: 2458   score: 0.0   memory length: 447858   epsilon: 0.21223918000903386    steps: 122     evaluation reward: 1.01\n",
      "episode: 2459   score: 1.0   memory length: 448008   epsilon: 0.21194218000903198    steps: 150     evaluation reward: 1.0\n",
      "episode: 2460   score: 2.0   memory length: 448205   epsilon: 0.2115521200090295    steps: 197     evaluation reward: 1.01\n",
      "episode: 2461   score: 0.0   memory length: 448327   epsilon: 0.21131056000902798    steps: 122     evaluation reward: 1.01\n",
      "episode: 2462   score: 2.0   memory length: 448525   epsilon: 0.2109185200090255    steps: 198     evaluation reward: 1.02\n",
      "episode: 2463   score: 2.0   memory length: 448722   epsilon: 0.21052846000902303    steps: 197     evaluation reward: 1.04\n",
      "episode: 2464   score: 0.0   memory length: 448844   epsilon: 0.2102869000090215    steps: 122     evaluation reward: 1.04\n",
      "episode: 2465   score: 2.0   memory length: 449041   epsilon: 0.20989684000901904    steps: 197     evaluation reward: 1.06\n",
      "episode: 2466   score: 2.0   memory length: 449240   epsilon: 0.20950282000901654    steps: 199     evaluation reward: 1.07\n",
      "episode: 2467   score: 1.0   memory length: 449390   epsilon: 0.20920582000901466    steps: 150     evaluation reward: 1.08\n",
      "episode: 2468   score: 2.0   memory length: 449587   epsilon: 0.2088157600090122    steps: 197     evaluation reward: 1.1\n",
      "episode: 2469   score: 2.0   memory length: 449784   epsilon: 0.20842570000900973    steps: 197     evaluation reward: 1.09\n",
      "now time :  2019-09-25 18:02:20.312179\n",
      "episode: 2470   score: 3.0   memory length: 450050   epsilon: 0.2078990200090064    steps: 266     evaluation reward: 1.11\n",
      "episode: 2471   score: 1.0   memory length: 450219   epsilon: 0.20756440000900428    steps: 169     evaluation reward: 1.1\n",
      "episode: 2472   score: 0.0   memory length: 450341   epsilon: 0.20732284000900275    steps: 122     evaluation reward: 1.07\n",
      "episode: 2473   score: 2.0   memory length: 450538   epsilon: 0.20693278000900028    steps: 197     evaluation reward: 1.08\n",
      "episode: 2474   score: 1.0   memory length: 450691   epsilon: 0.20662984000899837    steps: 153     evaluation reward: 1.08\n",
      "episode: 2475   score: 0.0   memory length: 450813   epsilon: 0.20638828000899684    steps: 122     evaluation reward: 1.07\n",
      "episode: 2476   score: 0.0   memory length: 450935   epsilon: 0.2061467200089953    steps: 122     evaluation reward: 1.06\n",
      "episode: 2477   score: 1.0   memory length: 451087   epsilon: 0.2058457600089934    steps: 152     evaluation reward: 1.07\n",
      "episode: 2478   score: 0.0   memory length: 451210   epsilon: 0.20560222000899187    steps: 123     evaluation reward: 1.07\n",
      "episode: 2479   score: 2.0   memory length: 451408   epsilon: 0.20521018000898938    steps: 198     evaluation reward: 1.07\n",
      "episode: 2480   score: 3.0   memory length: 451657   epsilon: 0.20471716000898627    steps: 249     evaluation reward: 1.08\n",
      "episode: 2481   score: 0.0   memory length: 451779   epsilon: 0.20447560000898474    steps: 122     evaluation reward: 1.08\n",
      "episode: 2482   score: 1.0   memory length: 451930   epsilon: 0.20417662000898285    steps: 151     evaluation reward: 1.09\n",
      "episode: 2483   score: 2.0   memory length: 452147   epsilon: 0.20374696000898013    steps: 217     evaluation reward: 1.09\n",
      "episode: 2484   score: 2.0   memory length: 452344   epsilon: 0.20335690000897766    steps: 197     evaluation reward: 1.11\n",
      "episode: 2485   score: 1.0   memory length: 452494   epsilon: 0.20305990000897578    steps: 150     evaluation reward: 1.12\n",
      "episode: 2486   score: 2.0   memory length: 452693   epsilon: 0.2026658800089733    steps: 199     evaluation reward: 1.14\n",
      "episode: 2487   score: 1.0   memory length: 452845   epsilon: 0.20236492000897138    steps: 152     evaluation reward: 1.15\n",
      "episode: 2488   score: 0.0   memory length: 452967   epsilon: 0.20212336000896985    steps: 122     evaluation reward: 1.14\n",
      "episode: 2489   score: 2.0   memory length: 453164   epsilon: 0.2017333000089674    steps: 197     evaluation reward: 1.15\n",
      "episode: 2490   score: 2.0   memory length: 453381   epsilon: 0.20130364000896467    steps: 217     evaluation reward: 1.13\n",
      "episode: 2491   score: 0.0   memory length: 453503   epsilon: 0.20106208000896314    steps: 122     evaluation reward: 1.11\n",
      "episode: 2492   score: 0.0   memory length: 453625   epsilon: 0.2008205200089616    steps: 122     evaluation reward: 1.09\n",
      "episode: 2493   score: 2.0   memory length: 453824   epsilon: 0.20042650000895912    steps: 199     evaluation reward: 1.08\n",
      "episode: 2494   score: 3.0   memory length: 454071   epsilon: 0.19993744000895602    steps: 247     evaluation reward: 1.1\n",
      "episode: 2495   score: 1.0   memory length: 454221   epsilon: 0.19964044000895415    steps: 150     evaluation reward: 1.08\n",
      "episode: 2496   score: 2.0   memory length: 454418   epsilon: 0.19925038000895168    steps: 197     evaluation reward: 1.07\n",
      "episode: 2497   score: 1.0   memory length: 454568   epsilon: 0.1989533800089498    steps: 150     evaluation reward: 1.07\n",
      "episode: 2498   score: 2.0   memory length: 454765   epsilon: 0.19856332000894733    steps: 197     evaluation reward: 1.08\n",
      "episode: 2499   score: 0.0   memory length: 454888   epsilon: 0.1983197800089458    steps: 123     evaluation reward: 1.08\n",
      "episode: 2500   score: 0.0   memory length: 455010   epsilon: 0.19807822000894426    steps: 122     evaluation reward: 1.08\n",
      "episode: 2501   score: 1.0   memory length: 455179   epsilon: 0.19774360000894214    steps: 169     evaluation reward: 1.09\n",
      "episode: 2502   score: 0.0   memory length: 455301   epsilon: 0.19750204000894062    steps: 122     evaluation reward: 1.07\n",
      "episode: 2503   score: 2.0   memory length: 455498   epsilon: 0.19711198000893815    steps: 197     evaluation reward: 1.07\n",
      "episode: 2504   score: 1.0   memory length: 455650   epsilon: 0.19681102000893624    steps: 152     evaluation reward: 1.06\n",
      "episode: 2505   score: 0.0   memory length: 455773   epsilon: 0.1965674800089347    steps: 123     evaluation reward: 1.05\n",
      "episode: 2506   score: 1.0   memory length: 455944   epsilon: 0.19622890000893256    steps: 171     evaluation reward: 1.06\n",
      "episode: 2507   score: 0.0   memory length: 456067   epsilon: 0.19598536000893102    steps: 123     evaluation reward: 1.04\n",
      "episode: 2508   score: 1.0   memory length: 456218   epsilon: 0.19568638000892913    steps: 151     evaluation reward: 1.05\n",
      "episode: 2509   score: 1.0   memory length: 456388   epsilon: 0.195349780008927    steps: 170     evaluation reward: 1.03\n",
      "episode: 2510   score: 1.0   memory length: 456538   epsilon: 0.19505278000892512    steps: 150     evaluation reward: 1.04\n",
      "episode: 2511   score: 0.0   memory length: 456660   epsilon: 0.1948112200089236    steps: 122     evaluation reward: 1.03\n",
      "episode: 2512   score: 0.0   memory length: 456782   epsilon: 0.19456966000892206    steps: 122     evaluation reward: 1.03\n",
      "episode: 2513   score: 0.0   memory length: 456904   epsilon: 0.19432810000892053    steps: 122     evaluation reward: 1.03\n",
      "episode: 2514   score: 2.0   memory length: 457083   epsilon: 0.1939736800089183    steps: 179     evaluation reward: 1.03\n",
      "episode: 2515   score: 2.0   memory length: 457300   epsilon: 0.19354402000891557    steps: 217     evaluation reward: 1.03\n",
      "episode: 2516   score: 1.0   memory length: 457451   epsilon: 0.19324504000891368    steps: 151     evaluation reward: 1.04\n",
      "episode: 2517   score: 4.0   memory length: 457725   epsilon: 0.19270252000891025    steps: 274     evaluation reward: 1.08\n",
      "episode: 2518   score: 1.0   memory length: 457876   epsilon: 0.19240354000890836    steps: 151     evaluation reward: 1.09\n",
      "episode: 2519   score: 2.0   memory length: 458073   epsilon: 0.1920134800089059    steps: 197     evaluation reward: 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2520   score: 0.0   memory length: 458195   epsilon: 0.19177192000890436    steps: 122     evaluation reward: 1.09\n",
      "episode: 2521   score: 3.0   memory length: 458420   epsilon: 0.19132642000890154    steps: 225     evaluation reward: 1.12\n",
      "episode: 2522   score: 0.0   memory length: 458542   epsilon: 0.19108486000890001    steps: 122     evaluation reward: 1.11\n",
      "episode: 2523   score: 0.0   memory length: 458664   epsilon: 0.1908433000088985    steps: 122     evaluation reward: 1.11\n",
      "episode: 2524   score: 2.0   memory length: 458862   epsilon: 0.190451260008896    steps: 198     evaluation reward: 1.12\n",
      "episode: 2525   score: 2.0   memory length: 459059   epsilon: 0.19006120000889354    steps: 197     evaluation reward: 1.12\n",
      "episode: 2526   score: 3.0   memory length: 459284   epsilon: 0.18961570000889072    steps: 225     evaluation reward: 1.15\n",
      "episode: 2527   score: 3.0   memory length: 459529   epsilon: 0.18913060000888765    steps: 245     evaluation reward: 1.16\n",
      "episode: 2528   score: 0.0   memory length: 459651   epsilon: 0.18888904000888612    steps: 122     evaluation reward: 1.16\n",
      "episode: 2529   score: 1.0   memory length: 459802   epsilon: 0.18859006000888423    steps: 151     evaluation reward: 1.13\n",
      "episode: 2530   score: 0.0   memory length: 459924   epsilon: 0.1883485000088827    steps: 122     evaluation reward: 1.13\n",
      "episode: 2531   score: 0.0   memory length: 460047   epsilon: 0.18810496000888116    steps: 123     evaluation reward: 1.13\n",
      "episode: 2532   score: 2.0   memory length: 460226   epsilon: 0.18775054000887892    steps: 179     evaluation reward: 1.15\n",
      "episode: 2533   score: 1.0   memory length: 460376   epsilon: 0.18745354000887704    steps: 150     evaluation reward: 1.14\n",
      "episode: 2534   score: 1.0   memory length: 460548   epsilon: 0.18711298000887489    steps: 172     evaluation reward: 1.15\n",
      "episode: 2535   score: 2.0   memory length: 460746   epsilon: 0.1867209400088724    steps: 198     evaluation reward: 1.17\n",
      "episode: 2536   score: 1.0   memory length: 460916   epsilon: 0.18638434000887028    steps: 170     evaluation reward: 1.16\n",
      "episode: 2537   score: 5.0   memory length: 461262   epsilon: 0.18569926000886594    steps: 346     evaluation reward: 1.19\n",
      "episode: 2538   score: 1.0   memory length: 461412   epsilon: 0.18540226000886406    steps: 150     evaluation reward: 1.19\n",
      "episode: 2539   score: 2.0   memory length: 461610   epsilon: 0.18501022000886158    steps: 198     evaluation reward: 1.2\n",
      "episode: 2540   score: 0.0   memory length: 461732   epsilon: 0.18476866000886005    steps: 122     evaluation reward: 1.18\n",
      "episode: 2541   score: 0.0   memory length: 461854   epsilon: 0.18452710000885852    steps: 122     evaluation reward: 1.18\n",
      "episode: 2542   score: 1.0   memory length: 462004   epsilon: 0.18423010000885665    steps: 150     evaluation reward: 1.19\n",
      "episode: 2543   score: 2.0   memory length: 462201   epsilon: 0.18384004000885418    steps: 197     evaluation reward: 1.21\n",
      "episode: 2544   score: 1.0   memory length: 462351   epsilon: 0.1835430400088523    steps: 150     evaluation reward: 1.21\n",
      "episode: 2545   score: 1.0   memory length: 462501   epsilon: 0.18324604000885042    steps: 150     evaluation reward: 1.2\n",
      "episode: 2546   score: 1.0   memory length: 462653   epsilon: 0.18294508000884852    steps: 152     evaluation reward: 1.2\n",
      "episode: 2547   score: 0.0   memory length: 462777   epsilon: 0.18269956000884696    steps: 124     evaluation reward: 1.18\n",
      "episode: 2548   score: 0.0   memory length: 462899   epsilon: 0.18245800000884543    steps: 122     evaluation reward: 1.18\n",
      "episode: 2549   score: 0.0   memory length: 463021   epsilon: 0.1822164400088439    steps: 122     evaluation reward: 1.17\n",
      "episode: 2550   score: 0.0   memory length: 463143   epsilon: 0.18197488000884238    steps: 122     evaluation reward: 1.16\n",
      "episode: 2551   score: 0.0   memory length: 463266   epsilon: 0.18173134000884084    steps: 123     evaluation reward: 1.15\n",
      "episode: 2552   score: 2.0   memory length: 463464   epsilon: 0.18133930000883836    steps: 198     evaluation reward: 1.16\n",
      "episode: 2553   score: 0.0   memory length: 463586   epsilon: 0.18109774000883683    steps: 122     evaluation reward: 1.14\n",
      "episode: 2554   score: 0.0   memory length: 463708   epsilon: 0.1808561800088353    steps: 122     evaluation reward: 1.12\n",
      "episode: 2555   score: 0.0   memory length: 463830   epsilon: 0.18061462000883377    steps: 122     evaluation reward: 1.12\n",
      "episode: 2556   score: 2.0   memory length: 464027   epsilon: 0.1802245600088313    steps: 197     evaluation reward: 1.13\n",
      "episode: 2557   score: 0.0   memory length: 464149   epsilon: 0.17998300000882977    steps: 122     evaluation reward: 1.12\n",
      "episode: 2558   score: 3.0   memory length: 464374   epsilon: 0.17953750000882696    steps: 225     evaluation reward: 1.15\n",
      "episode: 2559   score: 1.0   memory length: 464524   epsilon: 0.17924050000882508    steps: 150     evaluation reward: 1.15\n",
      "episode: 2560   score: 1.0   memory length: 464678   epsilon: 0.17893558000882315    steps: 154     evaluation reward: 1.14\n",
      "episode: 2561   score: 0.0   memory length: 464800   epsilon: 0.17869402000882162    steps: 122     evaluation reward: 1.14\n",
      "episode: 2562   score: 0.0   memory length: 464922   epsilon: 0.1784524600088201    steps: 122     evaluation reward: 1.12\n",
      "episode: 2563   score: 0.0   memory length: 465044   epsilon: 0.17821090000881856    steps: 122     evaluation reward: 1.1\n",
      "episode: 2564   score: 0.0   memory length: 465167   epsilon: 0.17796736000881702    steps: 123     evaluation reward: 1.1\n",
      "episode: 2565   score: 3.0   memory length: 465399   epsilon: 0.17750800000881412    steps: 232     evaluation reward: 1.11\n",
      "episode: 2566   score: 1.0   memory length: 465549   epsilon: 0.17721100000881224    steps: 150     evaluation reward: 1.1\n",
      "episode: 2567   score: 1.0   memory length: 465699   epsilon: 0.17691400000881036    steps: 150     evaluation reward: 1.1\n",
      "episode: 2568   score: 1.0   memory length: 465868   epsilon: 0.17657938000880824    steps: 169     evaluation reward: 1.09\n",
      "episode: 2569   score: 0.0   memory length: 465990   epsilon: 0.1763378200088067    steps: 122     evaluation reward: 1.07\n",
      "episode: 2570   score: 0.0   memory length: 466112   epsilon: 0.17609626000880518    steps: 122     evaluation reward: 1.04\n",
      "episode: 2571   score: 1.0   memory length: 466281   epsilon: 0.17576164000880307    steps: 169     evaluation reward: 1.04\n",
      "episode: 2572   score: 2.0   memory length: 466498   epsilon: 0.17533198000880035    steps: 217     evaluation reward: 1.06\n",
      "episode: 2573   score: 0.0   memory length: 466620   epsilon: 0.17509042000879882    steps: 122     evaluation reward: 1.04\n",
      "episode: 2574   score: 0.0   memory length: 466742   epsilon: 0.1748488600087973    steps: 122     evaluation reward: 1.03\n",
      "episode: 2575   score: 1.0   memory length: 466892   epsilon: 0.1745518600087954    steps: 150     evaluation reward: 1.04\n",
      "episode: 2576   score: 0.0   memory length: 467015   epsilon: 0.17430832000879387    steps: 123     evaluation reward: 1.04\n",
      "episode: 2577   score: 2.0   memory length: 467212   epsilon: 0.1739182600087914    steps: 197     evaluation reward: 1.05\n",
      "episode: 2578   score: 1.0   memory length: 467381   epsilon: 0.1735836400087893    steps: 169     evaluation reward: 1.06\n",
      "episode: 2579   score: 0.0   memory length: 467503   epsilon: 0.17334208000878776    steps: 122     evaluation reward: 1.04\n",
      "episode: 2580   score: 0.0   memory length: 467628   epsilon: 0.1730945800087862    steps: 125     evaluation reward: 1.01\n",
      "episode: 2581   score: 1.0   memory length: 467797   epsilon: 0.17275996000878407    steps: 169     evaluation reward: 1.02\n",
      "episode: 2582   score: 6.0   memory length: 468124   epsilon: 0.17211250000877998    steps: 327     evaluation reward: 1.07\n",
      "episode: 2583   score: 2.0   memory length: 468303   epsilon: 0.17175808000877774    steps: 179     evaluation reward: 1.07\n",
      "episode: 2584   score: 1.0   memory length: 468454   epsilon: 0.17145910000877584    steps: 151     evaluation reward: 1.06\n",
      "episode: 2585   score: 1.0   memory length: 468624   epsilon: 0.17112250000877371    steps: 170     evaluation reward: 1.06\n",
      "episode: 2586   score: 3.0   memory length: 468834   epsilon: 0.17070670000877108    steps: 210     evaluation reward: 1.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2587   score: 0.0   memory length: 468956   epsilon: 0.17046514000876956    steps: 122     evaluation reward: 1.06\n",
      "episode: 2588   score: 2.0   memory length: 469141   epsilon: 0.17009884000876724    steps: 185     evaluation reward: 1.08\n",
      "episode: 2589   score: 1.0   memory length: 469310   epsilon: 0.16976422000876512    steps: 169     evaluation reward: 1.07\n",
      "episode: 2590   score: 0.0   memory length: 469436   epsilon: 0.16951474000876354    steps: 126     evaluation reward: 1.05\n",
      "episode: 2591   score: 0.0   memory length: 469559   epsilon: 0.169271200008762    steps: 123     evaluation reward: 1.05\n",
      "episode: 2592   score: 2.0   memory length: 469740   epsilon: 0.16891282000875973    steps: 181     evaluation reward: 1.07\n",
      "episode: 2593   score: 2.0   memory length: 469938   epsilon: 0.16852078000875725    steps: 198     evaluation reward: 1.07\n",
      "episode: 2594   score: 2.0   memory length: 470135   epsilon: 0.16813072000875479    steps: 197     evaluation reward: 1.06\n",
      "episode: 2595   score: 1.0   memory length: 470285   epsilon: 0.1678337200087529    steps: 150     evaluation reward: 1.06\n",
      "episode: 2596   score: 0.0   memory length: 470408   epsilon: 0.16759018000875137    steps: 123     evaluation reward: 1.04\n",
      "episode: 2597   score: 0.0   memory length: 470531   epsilon: 0.16734664000874983    steps: 123     evaluation reward: 1.03\n",
      "episode: 2598   score: 2.0   memory length: 470728   epsilon: 0.16695658000874736    steps: 197     evaluation reward: 1.03\n",
      "episode: 2599   score: 2.0   memory length: 470925   epsilon: 0.1665665200087449    steps: 197     evaluation reward: 1.05\n",
      "episode: 2600   score: 0.0   memory length: 471049   epsilon: 0.16632100000874334    steps: 124     evaluation reward: 1.05\n",
      "episode: 2601   score: 0.0   memory length: 471171   epsilon: 0.1660794400087418    steps: 122     evaluation reward: 1.04\n",
      "episode: 2602   score: 2.0   memory length: 471369   epsilon: 0.16568740000873933    steps: 198     evaluation reward: 1.06\n",
      "episode: 2603   score: 3.0   memory length: 471616   epsilon: 0.16519834000873623    steps: 247     evaluation reward: 1.07\n",
      "episode: 2604   score: 3.0   memory length: 471843   epsilon: 0.1647488800087334    steps: 227     evaluation reward: 1.09\n",
      "episode: 2605   score: 1.0   memory length: 471994   epsilon: 0.1644499000087315    steps: 151     evaluation reward: 1.1\n",
      "episode: 2606   score: 1.0   memory length: 472144   epsilon: 0.16415290000872962    steps: 150     evaluation reward: 1.1\n",
      "episode: 2607   score: 0.0   memory length: 472266   epsilon: 0.1639113400087281    steps: 122     evaluation reward: 1.1\n",
      "episode: 2608   score: 0.0   memory length: 472388   epsilon: 0.16366978000872656    steps: 122     evaluation reward: 1.09\n",
      "episode: 2609   score: 2.0   memory length: 472586   epsilon: 0.16327774000872408    steps: 198     evaluation reward: 1.1\n",
      "episode: 2610   score: 0.0   memory length: 472709   epsilon: 0.16303420000872254    steps: 123     evaluation reward: 1.09\n",
      "episode: 2611   score: 2.0   memory length: 472926   epsilon: 0.16260454000871982    steps: 217     evaluation reward: 1.11\n",
      "episode: 2612   score: 0.0   memory length: 473049   epsilon: 0.16236100000871828    steps: 123     evaluation reward: 1.11\n",
      "episode: 2613   score: 0.0   memory length: 473172   epsilon: 0.16211746000871674    steps: 123     evaluation reward: 1.11\n",
      "episode: 2614   score: 0.0   memory length: 473294   epsilon: 0.1618759000087152    steps: 122     evaluation reward: 1.09\n",
      "episode: 2615   score: 1.0   memory length: 473444   epsilon: 0.16157890000871333    steps: 150     evaluation reward: 1.08\n",
      "episode: 2616   score: 2.0   memory length: 473641   epsilon: 0.16118884000871087    steps: 197     evaluation reward: 1.09\n",
      "episode: 2617   score: 0.0   memory length: 473763   epsilon: 0.16094728000870934    steps: 122     evaluation reward: 1.05\n",
      "episode: 2618   score: 3.0   memory length: 473989   epsilon: 0.1604998000087065    steps: 226     evaluation reward: 1.07\n",
      "episode: 2619   score: 0.0   memory length: 474112   epsilon: 0.16025626000870496    steps: 123     evaluation reward: 1.05\n",
      "episode: 2620   score: 0.0   memory length: 474235   epsilon: 0.16001272000870342    steps: 123     evaluation reward: 1.05\n",
      "episode: 2621   score: 1.0   memory length: 474386   epsilon: 0.15971374000870153    steps: 151     evaluation reward: 1.03\n",
      "episode: 2622   score: 0.0   memory length: 474510   epsilon: 0.15946822000869998    steps: 124     evaluation reward: 1.03\n",
      "episode: 2623   score: 2.0   memory length: 474707   epsilon: 0.1590781600086975    steps: 197     evaluation reward: 1.05\n",
      "episode: 2624   score: 0.0   memory length: 474829   epsilon: 0.15883660000869598    steps: 122     evaluation reward: 1.03\n",
      "episode: 2625   score: 2.0   memory length: 475010   epsilon: 0.15847822000869372    steps: 181     evaluation reward: 1.03\n",
      "episode: 2626   score: 0.0   memory length: 475132   epsilon: 0.1582366600086922    steps: 122     evaluation reward: 1.0\n",
      "episode: 2627   score: 0.0   memory length: 475254   epsilon: 0.15799510000869066    steps: 122     evaluation reward: 0.97\n",
      "episode: 2628   score: 0.0   memory length: 475378   epsilon: 0.1577495800086891    steps: 124     evaluation reward: 0.97\n",
      "episode: 2629   score: 0.0   memory length: 475501   epsilon: 0.15750604000868756    steps: 123     evaluation reward: 0.96\n",
      "episode: 2630   score: 0.0   memory length: 475624   epsilon: 0.15726250000868602    steps: 123     evaluation reward: 0.96\n",
      "episode: 2631   score: 3.0   memory length: 475871   epsilon: 0.15677344000868293    steps: 247     evaluation reward: 0.99\n",
      "episode: 2632   score: 3.0   memory length: 476137   epsilon: 0.1562467600086796    steps: 266     evaluation reward: 1.0\n",
      "episode: 2633   score: 2.0   memory length: 476334   epsilon: 0.15585670000867713    steps: 197     evaluation reward: 1.01\n",
      "episode: 2634   score: 0.0   memory length: 476456   epsilon: 0.1556151400086756    steps: 122     evaluation reward: 1.0\n",
      "episode: 2635   score: 2.0   memory length: 476654   epsilon: 0.15522310000867312    steps: 198     evaluation reward: 1.0\n",
      "episode: 2636   score: 0.0   memory length: 476776   epsilon: 0.1549815400086716    steps: 122     evaluation reward: 0.99\n",
      "episode: 2637   score: 1.0   memory length: 476926   epsilon: 0.1546845400086697    steps: 150     evaluation reward: 0.95\n",
      "episode: 2638   score: 0.0   memory length: 477048   epsilon: 0.15444298000866818    steps: 122     evaluation reward: 0.94\n",
      "episode: 2639   score: 2.0   memory length: 477232   epsilon: 0.15407866000866588    steps: 184     evaluation reward: 0.94\n",
      "episode: 2640   score: 1.0   memory length: 477383   epsilon: 0.153779680008664    steps: 151     evaluation reward: 0.95\n",
      "episode: 2641   score: 2.0   memory length: 477580   epsilon: 0.15338962000866152    steps: 197     evaluation reward: 0.97\n",
      "episode: 2642   score: 1.0   memory length: 477732   epsilon: 0.15308866000865962    steps: 152     evaluation reward: 0.97\n",
      "episode: 2643   score: 0.0   memory length: 477854   epsilon: 0.1528471000086581    steps: 122     evaluation reward: 0.95\n",
      "episode: 2644   score: 2.0   memory length: 478051   epsilon: 0.15245704000865562    steps: 197     evaluation reward: 0.96\n",
      "episode: 2645   score: 0.0   memory length: 478173   epsilon: 0.1522154800086541    steps: 122     evaluation reward: 0.95\n",
      "episode: 2646   score: 2.0   memory length: 478390   epsilon: 0.15178582000865137    steps: 217     evaluation reward: 0.96\n",
      "episode: 2647   score: 0.0   memory length: 478512   epsilon: 0.15154426000864984    steps: 122     evaluation reward: 0.96\n",
      "episode: 2648   score: 4.0   memory length: 478770   epsilon: 0.1510334200086466    steps: 258     evaluation reward: 1.0\n",
      "episode: 2649   score: 3.0   memory length: 478997   epsilon: 0.15058396000864377    steps: 227     evaluation reward: 1.03\n",
      "episode: 2650   score: 0.0   memory length: 479119   epsilon: 0.15034240000864224    steps: 122     evaluation reward: 1.03\n",
      "episode: 2651   score: 0.0   memory length: 479241   epsilon: 0.1501008400086407    steps: 122     evaluation reward: 1.03\n",
      "episode: 2652   score: 1.0   memory length: 479410   epsilon: 0.1497662200086386    steps: 169     evaluation reward: 1.02\n",
      "episode: 2653   score: 0.0   memory length: 479533   epsilon: 0.14952268000863705    steps: 123     evaluation reward: 1.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2654   score: 1.0   memory length: 479684   epsilon: 0.14922370000863516    steps: 151     evaluation reward: 1.03\n",
      "episode: 2655   score: 0.0   memory length: 479807   epsilon: 0.14898016000863362    steps: 123     evaluation reward: 1.03\n",
      "episode: 2656   score: 0.0   memory length: 479929   epsilon: 0.1487386000086321    steps: 122     evaluation reward: 1.01\n",
      "episode: 2657   score: 0.0   memory length: 480051   epsilon: 0.14849704000863057    steps: 122     evaluation reward: 1.01\n",
      "episode: 2658   score: 2.0   memory length: 480248   epsilon: 0.1481069800086281    steps: 197     evaluation reward: 1.0\n",
      "episode: 2659   score: 0.0   memory length: 480370   epsilon: 0.14786542000862657    steps: 122     evaluation reward: 0.99\n",
      "episode: 2660   score: 0.0   memory length: 480492   epsilon: 0.14762386000862504    steps: 122     evaluation reward: 0.98\n",
      "episode: 2661   score: 1.0   memory length: 480643   epsilon: 0.14732488000862315    steps: 151     evaluation reward: 0.99\n",
      "episode: 2662   score: 1.0   memory length: 480794   epsilon: 0.14702590000862126    steps: 151     evaluation reward: 1.0\n",
      "episode: 2663   score: 0.0   memory length: 480916   epsilon: 0.14678434000861973    steps: 122     evaluation reward: 1.0\n",
      "episode: 2664   score: 0.0   memory length: 481038   epsilon: 0.1465427800086182    steps: 122     evaluation reward: 1.0\n",
      "episode: 2665   score: 2.0   memory length: 481220   epsilon: 0.14618242000861592    steps: 182     evaluation reward: 0.99\n",
      "episode: 2666   score: 1.0   memory length: 481370   epsilon: 0.14588542000861404    steps: 150     evaluation reward: 0.99\n",
      "episode: 2667   score: 0.0   memory length: 481492   epsilon: 0.1456438600086125    steps: 122     evaluation reward: 0.98\n",
      "episode: 2668   score: 0.0   memory length: 481614   epsilon: 0.14540230000861099    steps: 122     evaluation reward: 0.97\n",
      "episode: 2669   score: 3.0   memory length: 481861   epsilon: 0.1449132400086079    steps: 247     evaluation reward: 1.0\n",
      "episode: 2670   score: 0.0   memory length: 481984   epsilon: 0.14466970000860635    steps: 123     evaluation reward: 1.0\n",
      "episode: 2671   score: 0.0   memory length: 482106   epsilon: 0.14442814000860482    steps: 122     evaluation reward: 0.99\n",
      "episode: 2672   score: 0.0   memory length: 482229   epsilon: 0.14418460000860328    steps: 123     evaluation reward: 0.97\n",
      "episode: 2673   score: 0.0   memory length: 482353   epsilon: 0.14393908000860173    steps: 124     evaluation reward: 0.97\n",
      "episode: 2674   score: 0.0   memory length: 482475   epsilon: 0.1436975200086002    steps: 122     evaluation reward: 0.97\n",
      "episode: 2675   score: 0.0   memory length: 482598   epsilon: 0.14345398000859866    steps: 123     evaluation reward: 0.96\n",
      "episode: 2676   score: 1.0   memory length: 482748   epsilon: 0.14315698000859678    steps: 150     evaluation reward: 0.97\n",
      "episode: 2677   score: 0.0   memory length: 482870   epsilon: 0.14291542000859525    steps: 122     evaluation reward: 0.95\n",
      "episode: 2678   score: 0.0   memory length: 482992   epsilon: 0.14267386000859372    steps: 122     evaluation reward: 0.94\n",
      "episode: 2679   score: 3.0   memory length: 483239   epsilon: 0.14218480000859063    steps: 247     evaluation reward: 0.97\n",
      "episode: 2680   score: 0.0   memory length: 483361   epsilon: 0.1419432400085891    steps: 122     evaluation reward: 0.97\n",
      "episode: 2681   score: 2.0   memory length: 483559   epsilon: 0.14155120000858662    steps: 198     evaluation reward: 0.98\n",
      "episode: 2682   score: 0.0   memory length: 483682   epsilon: 0.14130766000858508    steps: 123     evaluation reward: 0.92\n",
      "episode: 2683   score: 0.0   memory length: 483805   epsilon: 0.14106412000858354    steps: 123     evaluation reward: 0.9\n",
      "episode: 2684   score: 1.0   memory length: 483955   epsilon: 0.14076712000858166    steps: 150     evaluation reward: 0.9\n",
      "episode: 2685   score: 0.0   memory length: 484078   epsilon: 0.14052358000858012    steps: 123     evaluation reward: 0.89\n",
      "episode: 2686   score: 0.0   memory length: 484200   epsilon: 0.1402820200085786    steps: 122     evaluation reward: 0.86\n",
      "episode: 2687   score: 0.0   memory length: 484323   epsilon: 0.14003848000857705    steps: 123     evaluation reward: 0.86\n",
      "episode: 2688   score: 3.0   memory length: 484548   epsilon: 0.13959298000857423    steps: 225     evaluation reward: 0.87\n",
      "episode: 2689   score: 1.0   memory length: 484699   epsilon: 0.13929400000857234    steps: 151     evaluation reward: 0.87\n",
      "episode: 2690   score: 0.0   memory length: 484821   epsilon: 0.1390524400085708    steps: 122     evaluation reward: 0.87\n",
      "episode: 2691   score: 2.0   memory length: 485020   epsilon: 0.13865842000856832    steps: 199     evaluation reward: 0.89\n",
      "episode: 2692   score: 2.0   memory length: 485222   epsilon: 0.1382584600085658    steps: 202     evaluation reward: 0.89\n",
      "episode: 2693   score: 1.0   memory length: 485372   epsilon: 0.1379614600085639    steps: 150     evaluation reward: 0.88\n",
      "episode: 2694   score: 1.0   memory length: 485522   epsilon: 0.13766446000856203    steps: 150     evaluation reward: 0.87\n",
      "episode: 2695   score: 2.0   memory length: 485719   epsilon: 0.13727440000855956    steps: 197     evaluation reward: 0.88\n",
      "episode: 2696   score: 1.0   memory length: 485870   epsilon: 0.13697542000855767    steps: 151     evaluation reward: 0.89\n",
      "episode: 2697   score: 4.0   memory length: 486126   epsilon: 0.13646854000855446    steps: 256     evaluation reward: 0.93\n",
      "episode: 2698   score: 2.0   memory length: 486323   epsilon: 0.136078480008552    steps: 197     evaluation reward: 0.93\n",
      "episode: 2699   score: 2.0   memory length: 486539   epsilon: 0.1356508000085493    steps: 216     evaluation reward: 0.93\n",
      "episode: 2700   score: 1.0   memory length: 486689   epsilon: 0.1353538000085474    steps: 150     evaluation reward: 0.94\n",
      "episode: 2701   score: 0.0   memory length: 486811   epsilon: 0.13511224000854588    steps: 122     evaluation reward: 0.94\n",
      "episode: 2702   score: 0.0   memory length: 486933   epsilon: 0.13487068000854435    steps: 122     evaluation reward: 0.92\n",
      "episode: 2703   score: 0.0   memory length: 487055   epsilon: 0.13462912000854282    steps: 122     evaluation reward: 0.89\n",
      "episode: 2704   score: 1.0   memory length: 487223   epsilon: 0.13429648000854072    steps: 168     evaluation reward: 0.87\n",
      "episode: 2705   score: 2.0   memory length: 487421   epsilon: 0.13390444000853824    steps: 198     evaluation reward: 0.88\n",
      "episode: 2706   score: 2.0   memory length: 487618   epsilon: 0.13351438000853577    steps: 197     evaluation reward: 0.89\n",
      "episode: 2707   score: 1.0   memory length: 487768   epsilon: 0.1332173800085339    steps: 150     evaluation reward: 0.9\n",
      "episode: 2708   score: 1.0   memory length: 487936   epsilon: 0.1328847400085318    steps: 168     evaluation reward: 0.91\n",
      "episode: 2709   score: 2.0   memory length: 488133   epsilon: 0.13249468000852932    steps: 197     evaluation reward: 0.91\n",
      "episode: 2710   score: 0.0   memory length: 488255   epsilon: 0.1322531200085278    steps: 122     evaluation reward: 0.91\n",
      "episode: 2711   score: 2.0   memory length: 488452   epsilon: 0.13186306000852532    steps: 197     evaluation reward: 0.91\n",
      "episode: 2712   score: 1.0   memory length: 488603   epsilon: 0.13156408000852343    steps: 151     evaluation reward: 0.92\n",
      "episode: 2713   score: 1.0   memory length: 488771   epsilon: 0.13123144000852133    steps: 168     evaluation reward: 0.93\n",
      "episode: 2714   score: 2.0   memory length: 488968   epsilon: 0.13084138000851886    steps: 197     evaluation reward: 0.95\n",
      "episode: 2715   score: 0.0   memory length: 489090   epsilon: 0.13059982000851733    steps: 122     evaluation reward: 0.94\n",
      "episode: 2716   score: 2.0   memory length: 489269   epsilon: 0.1302454000085151    steps: 179     evaluation reward: 0.94\n",
      "episode: 2717   score: 1.0   memory length: 489419   epsilon: 0.1299484000085132    steps: 150     evaluation reward: 0.95\n",
      "episode: 2718   score: 3.0   memory length: 489644   epsilon: 0.1295029000085104    steps: 225     evaluation reward: 0.95\n",
      "episode: 2719   score: 2.0   memory length: 489827   epsilon: 0.1291405600085081    steps: 183     evaluation reward: 0.97\n",
      "episode: 2720   score: 0.0   memory length: 489949   epsilon: 0.12889900000850657    steps: 122     evaluation reward: 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2721   score: 0.0   memory length: 490071   epsilon: 0.12865744000850504    steps: 122     evaluation reward: 0.96\n",
      "episode: 2722   score: 0.0   memory length: 490193   epsilon: 0.1284158800085035    steps: 122     evaluation reward: 0.96\n",
      "episode: 2723   score: 2.0   memory length: 490390   epsilon: 0.12802582000850105    steps: 197     evaluation reward: 0.96\n",
      "episode: 2724   score: 0.0   memory length: 490512   epsilon: 0.12778426000849952    steps: 122     evaluation reward: 0.96\n",
      "episode: 2725   score: 1.0   memory length: 490662   epsilon: 0.12748726000849764    steps: 150     evaluation reward: 0.95\n",
      "episode: 2726   score: 0.0   memory length: 490784   epsilon: 0.1272457000084961    steps: 122     evaluation reward: 0.95\n",
      "episode: 2727   score: 2.0   memory length: 490981   epsilon: 0.12685564000849364    steps: 197     evaluation reward: 0.97\n",
      "episode: 2728   score: 0.0   memory length: 491103   epsilon: 0.1266140800084921    steps: 122     evaluation reward: 0.97\n",
      "episode: 2729   score: 2.0   memory length: 491300   epsilon: 0.12622402000848965    steps: 197     evaluation reward: 0.99\n",
      "episode: 2730   score: 0.0   memory length: 491422   epsilon: 0.12598246000848812    steps: 122     evaluation reward: 0.99\n",
      "episode: 2731   score: 2.0   memory length: 491619   epsilon: 0.12559240000848565    steps: 197     evaluation reward: 0.98\n",
      "episode: 2732   score: 3.0   memory length: 491868   epsilon: 0.12509938000848253    steps: 249     evaluation reward: 0.98\n",
      "episode: 2733   score: 1.0   memory length: 492036   epsilon: 0.12476674000848206    steps: 168     evaluation reward: 0.97\n",
      "episode: 2734   score: 2.0   memory length: 492233   epsilon: 0.12437668000848233    steps: 197     evaluation reward: 0.99\n",
      "episode: 2735   score: 0.0   memory length: 492356   epsilon: 0.1241331400084825    steps: 123     evaluation reward: 0.97\n",
      "episode: 2736   score: 0.0   memory length: 492478   epsilon: 0.12389158000848266    steps: 122     evaluation reward: 0.97\n",
      "episode: 2737   score: 1.0   memory length: 492646   epsilon: 0.12355894000848289    steps: 168     evaluation reward: 0.97\n",
      "episode: 2738   score: 2.0   memory length: 492843   epsilon: 0.12316888000848315    steps: 197     evaluation reward: 0.99\n",
      "episode: 2739   score: 2.0   memory length: 493024   epsilon: 0.1228105000084834    steps: 181     evaluation reward: 0.99\n",
      "episode: 2740   score: 0.0   memory length: 493146   epsilon: 0.12256894000848356    steps: 122     evaluation reward: 0.98\n",
      "episode: 2741   score: 0.0   memory length: 493268   epsilon: 0.12232738000848373    steps: 122     evaluation reward: 0.96\n",
      "episode: 2742   score: 0.0   memory length: 493390   epsilon: 0.12208582000848389    steps: 122     evaluation reward: 0.95\n",
      "episode: 2743   score: 2.0   memory length: 493587   epsilon: 0.12169576000848416    steps: 197     evaluation reward: 0.97\n",
      "episode: 2744   score: 0.0   memory length: 493709   epsilon: 0.12145420000848432    steps: 122     evaluation reward: 0.95\n",
      "episode: 2745   score: 2.0   memory length: 493906   epsilon: 0.12106414000848459    steps: 197     evaluation reward: 0.97\n",
      "episode: 2746   score: 0.0   memory length: 494028   epsilon: 0.12082258000848475    steps: 122     evaluation reward: 0.95\n",
      "episode: 2747   score: 2.0   memory length: 494225   epsilon: 0.12043252000848502    steps: 197     evaluation reward: 0.97\n",
      "episode: 2748   score: 0.0   memory length: 494348   epsilon: 0.12018898000848519    steps: 123     evaluation reward: 0.93\n",
      "episode: 2749   score: 1.0   memory length: 494498   epsilon: 0.11989198000848539    steps: 150     evaluation reward: 0.91\n",
      "episode: 2750   score: 2.0   memory length: 494697   epsilon: 0.11949796000848566    steps: 199     evaluation reward: 0.93\n",
      "episode: 2751   score: 0.0   memory length: 494819   epsilon: 0.11925640000848582    steps: 122     evaluation reward: 0.93\n",
      "episode: 2752   score: 3.0   memory length: 495044   epsilon: 0.11881090000848613    steps: 225     evaluation reward: 0.95\n",
      "episode: 2753   score: 0.0   memory length: 495166   epsilon: 0.11856934000848629    steps: 122     evaluation reward: 0.95\n",
      "episode: 2754   score: 2.0   memory length: 495363   epsilon: 0.11817928000848656    steps: 197     evaluation reward: 0.96\n",
      "episode: 2755   score: 1.0   memory length: 495531   epsilon: 0.11784664000848678    steps: 168     evaluation reward: 0.97\n",
      "episode: 2756   score: 0.0   memory length: 495653   epsilon: 0.11760508000848695    steps: 122     evaluation reward: 0.97\n",
      "episode: 2757   score: 0.0   memory length: 495775   epsilon: 0.11736352000848711    steps: 122     evaluation reward: 0.97\n",
      "episode: 2758   score: 2.0   memory length: 495972   epsilon: 0.11697346000848738    steps: 197     evaluation reward: 0.97\n",
      "episode: 2759   score: 1.0   memory length: 496122   epsilon: 0.11667646000848758    steps: 150     evaluation reward: 0.98\n",
      "episode: 2760   score: 2.0   memory length: 496320   epsilon: 0.11628442000848785    steps: 198     evaluation reward: 1.0\n",
      "episode: 2761   score: 0.0   memory length: 496442   epsilon: 0.11604286000848801    steps: 122     evaluation reward: 0.99\n",
      "episode: 2762   score: 1.0   memory length: 496593   epsilon: 0.11574388000848822    steps: 151     evaluation reward: 0.99\n",
      "episode: 2763   score: 0.0   memory length: 496715   epsilon: 0.11550232000848838    steps: 122     evaluation reward: 0.99\n",
      "episode: 2764   score: 2.0   memory length: 496912   epsilon: 0.11511226000848865    steps: 197     evaluation reward: 1.01\n",
      "episode: 2765   score: 2.0   memory length: 497109   epsilon: 0.11472220000848891    steps: 197     evaluation reward: 1.01\n",
      "episode: 2766   score: 0.0   memory length: 497233   epsilon: 0.11447668000848908    steps: 124     evaluation reward: 1.0\n",
      "episode: 2767   score: 2.0   memory length: 497430   epsilon: 0.11408662000848935    steps: 197     evaluation reward: 1.02\n",
      "episode: 2768   score: 1.0   memory length: 497580   epsilon: 0.11378962000848955    steps: 150     evaluation reward: 1.03\n",
      "episode: 2769   score: 3.0   memory length: 497806   epsilon: 0.11334214000848986    steps: 226     evaluation reward: 1.03\n",
      "episode: 2770   score: 2.0   memory length: 498003   epsilon: 0.11295208000849012    steps: 197     evaluation reward: 1.05\n",
      "episode: 2771   score: 0.0   memory length: 498125   epsilon: 0.11271052000849029    steps: 122     evaluation reward: 1.05\n",
      "episode: 2772   score: 2.0   memory length: 498322   epsilon: 0.11232046000849055    steps: 197     evaluation reward: 1.07\n",
      "episode: 2773   score: 1.0   memory length: 498490   epsilon: 0.11198782000849078    steps: 168     evaluation reward: 1.08\n",
      "episode: 2774   score: 1.0   memory length: 498640   epsilon: 0.11169082000849098    steps: 150     evaluation reward: 1.09\n",
      "episode: 2775   score: 3.0   memory length: 498887   epsilon: 0.11120176000849132    steps: 247     evaluation reward: 1.12\n",
      "episode: 2776   score: 0.0   memory length: 499009   epsilon: 0.11096020000849148    steps: 122     evaluation reward: 1.11\n",
      "episode: 2777   score: 1.0   memory length: 499178   epsilon: 0.11062558000849171    steps: 169     evaluation reward: 1.12\n",
      "episode: 2778   score: 2.0   memory length: 499375   epsilon: 0.11023552000849197    steps: 197     evaluation reward: 1.14\n",
      "episode: 2779   score: 1.0   memory length: 499525   epsilon: 0.10993852000849218    steps: 150     evaluation reward: 1.12\n",
      "episode: 2780   score: 2.0   memory length: 499708   epsilon: 0.10957618000849242    steps: 183     evaluation reward: 1.14\n",
      "episode: 2781   score: 1.0   memory length: 499859   epsilon: 0.10927720000849263    steps: 151     evaluation reward: 1.13\n",
      "episode: 2782   score: 0.0   memory length: 499981   epsilon: 0.10903564000849279    steps: 122     evaluation reward: 1.13\n",
      "now time :  2019-09-25 18:20:24.509397\n",
      "episode: 2783   score: 2.0   memory length: 500160   epsilon: 0.10868122000849303    steps: 179     evaluation reward: 1.15\n",
      "episode: 2784   score: 0.0   memory length: 500282   epsilon: 0.1084396600084932    steps: 122     evaluation reward: 1.14\n",
      "episode: 2785   score: 0.0   memory length: 500404   epsilon: 0.10819810000849336    steps: 122     evaluation reward: 1.14\n",
      "episode: 2786   score: 0.0   memory length: 500526   epsilon: 0.10795654000849353    steps: 122     evaluation reward: 1.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2787   score: 1.0   memory length: 500676   epsilon: 0.10765954000849373    steps: 150     evaluation reward: 1.15\n",
      "episode: 2788   score: 2.0   memory length: 500873   epsilon: 0.107269480008494    steps: 197     evaluation reward: 1.14\n",
      "episode: 2789   score: 0.0   memory length: 500996   epsilon: 0.10702594000849416    steps: 123     evaluation reward: 1.13\n",
      "episode: 2790   score: 0.0   memory length: 501118   epsilon: 0.10678438000849433    steps: 122     evaluation reward: 1.13\n",
      "episode: 2791   score: 0.0   memory length: 501240   epsilon: 0.1065428200084945    steps: 122     evaluation reward: 1.11\n",
      "episode: 2792   score: 0.0   memory length: 501362   epsilon: 0.10630126000849466    steps: 122     evaluation reward: 1.09\n",
      "episode: 2793   score: 3.0   memory length: 501608   epsilon: 0.10581418000849499    steps: 246     evaluation reward: 1.11\n",
      "episode: 2794   score: 0.0   memory length: 501730   epsilon: 0.10557262000849515    steps: 122     evaluation reward: 1.1\n",
      "episode: 2795   score: 1.0   memory length: 501880   epsilon: 0.10527562000849536    steps: 150     evaluation reward: 1.09\n",
      "episode: 2796   score: 2.0   memory length: 502078   epsilon: 0.10488358000849562    steps: 198     evaluation reward: 1.1\n",
      "episode: 2797   score: 0.0   memory length: 502200   epsilon: 0.10464202000849579    steps: 122     evaluation reward: 1.06\n",
      "episode: 2798   score: 2.0   memory length: 502397   epsilon: 0.10425196000849606    steps: 197     evaluation reward: 1.06\n",
      "episode: 2799   score: 3.0   memory length: 502612   epsilon: 0.10382626000849635    steps: 215     evaluation reward: 1.07\n",
      "episode: 2800   score: 1.0   memory length: 502763   epsilon: 0.10352728000849655    steps: 151     evaluation reward: 1.07\n",
      "episode: 2801   score: 1.0   memory length: 502913   epsilon: 0.10323028000849675    steps: 150     evaluation reward: 1.08\n",
      "episode: 2802   score: 3.0   memory length: 503147   epsilon: 0.10276696000849707    steps: 234     evaluation reward: 1.11\n",
      "episode: 2803   score: 3.0   memory length: 503358   epsilon: 0.10234918000849735    steps: 211     evaluation reward: 1.14\n",
      "episode: 2804   score: 0.0   memory length: 503480   epsilon: 0.10210762000849752    steps: 122     evaluation reward: 1.13\n",
      "episode: 2805   score: 1.0   memory length: 503630   epsilon: 0.10181062000849772    steps: 150     evaluation reward: 1.12\n",
      "episode: 2806   score: 1.0   memory length: 503780   epsilon: 0.10151362000849792    steps: 150     evaluation reward: 1.11\n",
      "episode: 2807   score: 0.0   memory length: 503902   epsilon: 0.10127206000849809    steps: 122     evaluation reward: 1.1\n",
      "episode: 2808   score: 1.0   memory length: 504053   epsilon: 0.10097308000849829    steps: 151     evaluation reward: 1.1\n",
      "episode: 2809   score: 2.0   memory length: 504232   epsilon: 0.10061866000849853    steps: 179     evaluation reward: 1.1\n",
      "episode: 2810   score: 0.0   memory length: 504354   epsilon: 0.1003771000084987    steps: 122     evaluation reward: 1.1\n",
      "episode: 2811   score: 2.0   memory length: 504537   epsilon: 0.10001476000849895    steps: 183     evaluation reward: 1.1\n",
      "episode: 2812   score: 1.0   memory length: 504687   epsilon: 0.09971776000849915    steps: 150     evaluation reward: 1.1\n",
      "episode: 2813   score: 2.0   memory length: 504884   epsilon: 0.09932770000849941    steps: 197     evaluation reward: 1.11\n",
      "episode: 2814   score: 0.0   memory length: 505006   epsilon: 0.09908614000849958    steps: 122     evaluation reward: 1.09\n",
      "episode: 2815   score: 3.0   memory length: 505253   epsilon: 0.09859708000849991    steps: 247     evaluation reward: 1.12\n",
      "episode: 2816   score: 0.0   memory length: 505375   epsilon: 0.09835552000850008    steps: 122     evaluation reward: 1.1\n",
      "episode: 2817   score: 0.0   memory length: 505497   epsilon: 0.09811396000850024    steps: 122     evaluation reward: 1.09\n",
      "episode: 2818   score: 1.0   memory length: 505647   epsilon: 0.09781696000850044    steps: 150     evaluation reward: 1.07\n",
      "episode: 2819   score: 0.0   memory length: 505769   epsilon: 0.09757540000850061    steps: 122     evaluation reward: 1.05\n",
      "episode: 2820   score: 2.0   memory length: 505966   epsilon: 0.09718534000850088    steps: 197     evaluation reward: 1.07\n",
      "episode: 2821   score: 2.0   memory length: 506145   epsilon: 0.09683092000850112    steps: 179     evaluation reward: 1.09\n",
      "episode: 2822   score: 3.0   memory length: 506372   epsilon: 0.09638146000850142    steps: 227     evaluation reward: 1.12\n",
      "episode: 2823   score: 0.0   memory length: 506494   epsilon: 0.09613990000850159    steps: 122     evaluation reward: 1.1\n",
      "episode: 2824   score: 0.0   memory length: 506616   epsilon: 0.09589834000850175    steps: 122     evaluation reward: 1.1\n",
      "episode: 2825   score: 2.0   memory length: 506799   epsilon: 0.095536000008502    steps: 183     evaluation reward: 1.11\n",
      "episode: 2826   score: 0.0   memory length: 506921   epsilon: 0.09529444000850217    steps: 122     evaluation reward: 1.11\n",
      "episode: 2827   score: 0.0   memory length: 507043   epsilon: 0.09505288000850233    steps: 122     evaluation reward: 1.09\n",
      "episode: 2828   score: 0.0   memory length: 507165   epsilon: 0.0948113200085025    steps: 122     evaluation reward: 1.09\n",
      "episode: 2829   score: 1.0   memory length: 507333   epsilon: 0.09447868000850272    steps: 168     evaluation reward: 1.08\n",
      "episode: 2830   score: 2.0   memory length: 507517   epsilon: 0.09411436000850297    steps: 184     evaluation reward: 1.1\n",
      "episode: 2831   score: 0.0   memory length: 507639   epsilon: 0.09387280000850313    steps: 122     evaluation reward: 1.08\n",
      "episode: 2832   score: 1.0   memory length: 507789   epsilon: 0.09357580000850334    steps: 150     evaluation reward: 1.06\n",
      "episode: 2833   score: 1.0   memory length: 507958   epsilon: 0.09324118000850357    steps: 169     evaluation reward: 1.06\n",
      "episode: 2834   score: 1.0   memory length: 508126   epsilon: 0.09290854000850379    steps: 168     evaluation reward: 1.05\n",
      "episode: 2835   score: 0.0   memory length: 508249   epsilon: 0.09266500000850396    steps: 123     evaluation reward: 1.05\n",
      "episode: 2836   score: 0.0   memory length: 508371   epsilon: 0.09242344000850412    steps: 122     evaluation reward: 1.05\n",
      "episode: 2837   score: 3.0   memory length: 508597   epsilon: 0.09197596000850443    steps: 226     evaluation reward: 1.07\n",
      "episode: 2838   score: 2.0   memory length: 508794   epsilon: 0.0915859000085047    steps: 197     evaluation reward: 1.07\n",
      "episode: 2839   score: 3.0   memory length: 509008   epsilon: 0.09116218000850498    steps: 214     evaluation reward: 1.08\n",
      "episode: 2840   score: 1.0   memory length: 509159   epsilon: 0.09086320000850519    steps: 151     evaluation reward: 1.09\n",
      "episode: 2841   score: 0.0   memory length: 509281   epsilon: 0.09062164000850535    steps: 122     evaluation reward: 1.09\n",
      "episode: 2842   score: 0.0   memory length: 509403   epsilon: 0.09038008000850552    steps: 122     evaluation reward: 1.09\n",
      "episode: 2843   score: 2.0   memory length: 509582   epsilon: 0.09002566000850576    steps: 179     evaluation reward: 1.09\n",
      "episode: 2844   score: 0.0   memory length: 509705   epsilon: 0.08978212000850593    steps: 123     evaluation reward: 1.09\n",
      "episode: 2845   score: 0.0   memory length: 509827   epsilon: 0.08954056000850609    steps: 122     evaluation reward: 1.07\n",
      "episode: 2846   score: 1.0   memory length: 509977   epsilon: 0.08924356000850629    steps: 150     evaluation reward: 1.08\n",
      "episode: 2847   score: 2.0   memory length: 510174   epsilon: 0.08885350000850656    steps: 197     evaluation reward: 1.08\n",
      "episode: 2848   score: 0.0   memory length: 510296   epsilon: 0.08861194000850672    steps: 122     evaluation reward: 1.08\n",
      "episode: 2849   score: 1.0   memory length: 510447   epsilon: 0.08831296000850693    steps: 151     evaluation reward: 1.08\n",
      "episode: 2850   score: 1.0   memory length: 510597   epsilon: 0.08801596000850713    steps: 150     evaluation reward: 1.07\n",
      "episode: 2851   score: 0.0   memory length: 510720   epsilon: 0.0877724200085073    steps: 123     evaluation reward: 1.07\n",
      "episode: 2852   score: 1.0   memory length: 510870   epsilon: 0.0874754200085075    steps: 150     evaluation reward: 1.05\n",
      "episode: 2853   score: 3.0   memory length: 511098   epsilon: 0.0870239800085078    steps: 228     evaluation reward: 1.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2854   score: 1.0   memory length: 511248   epsilon: 0.08672698000850801    steps: 150     evaluation reward: 1.07\n",
      "episode: 2855   score: 4.0   memory length: 511504   epsilon: 0.08622010000850835    steps: 256     evaluation reward: 1.1\n",
      "episode: 2856   score: 0.0   memory length: 511627   epsilon: 0.08597656000850852    steps: 123     evaluation reward: 1.1\n",
      "episode: 2857   score: 0.0   memory length: 511749   epsilon: 0.08573500000850869    steps: 122     evaluation reward: 1.1\n",
      "episode: 2858   score: 0.0   memory length: 511871   epsilon: 0.08549344000850885    steps: 122     evaluation reward: 1.08\n",
      "episode: 2859   score: 2.0   memory length: 512054   epsilon: 0.0851311000085091    steps: 183     evaluation reward: 1.09\n",
      "episode: 2860   score: 0.0   memory length: 512176   epsilon: 0.08488954000850926    steps: 122     evaluation reward: 1.07\n",
      "episode: 2861   score: 0.0   memory length: 512298   epsilon: 0.08464798000850943    steps: 122     evaluation reward: 1.07\n",
      "episode: 2862   score: 1.0   memory length: 512448   epsilon: 0.08435098000850963    steps: 150     evaluation reward: 1.07\n",
      "episode: 2863   score: 0.0   memory length: 512570   epsilon: 0.0841094200085098    steps: 122     evaluation reward: 1.07\n",
      "episode: 2864   score: 0.0   memory length: 512692   epsilon: 0.08386786000850996    steps: 122     evaluation reward: 1.05\n",
      "episode: 2865   score: 0.0   memory length: 512814   epsilon: 0.08362630000851012    steps: 122     evaluation reward: 1.03\n",
      "episode: 2866   score: 0.0   memory length: 512937   epsilon: 0.08338276000851029    steps: 123     evaluation reward: 1.03\n",
      "episode: 2867   score: 1.0   memory length: 513088   epsilon: 0.0830837800085105    steps: 151     evaluation reward: 1.02\n",
      "episode: 2868   score: 1.0   memory length: 513238   epsilon: 0.0827867800085107    steps: 150     evaluation reward: 1.02\n",
      "episode: 2869   score: 0.0   memory length: 513360   epsilon: 0.08254522000851086    steps: 122     evaluation reward: 0.99\n",
      "episode: 2870   score: 0.0   memory length: 513482   epsilon: 0.08230366000851103    steps: 122     evaluation reward: 0.97\n",
      "episode: 2871   score: 3.0   memory length: 513710   epsilon: 0.08185222000851133    steps: 228     evaluation reward: 1.0\n",
      "episode: 2872   score: 2.0   memory length: 513907   epsilon: 0.0814621600085116    steps: 197     evaluation reward: 1.0\n",
      "episode: 2873   score: 1.0   memory length: 514076   epsilon: 0.08112754000851183    steps: 169     evaluation reward: 1.0\n",
      "episode: 2874   score: 1.0   memory length: 514228   epsilon: 0.08082658000851203    steps: 152     evaluation reward: 1.0\n",
      "episode: 2875   score: 0.0   memory length: 514351   epsilon: 0.0805830400085122    steps: 123     evaluation reward: 0.97\n",
      "episode: 2876   score: 2.0   memory length: 514530   epsilon: 0.08022862000851244    steps: 179     evaluation reward: 0.99\n",
      "episode: 2877   score: 0.0   memory length: 514652   epsilon: 0.0799870600085126    steps: 122     evaluation reward: 0.98\n",
      "episode: 2878   score: 0.0   memory length: 514774   epsilon: 0.07974550000851277    steps: 122     evaluation reward: 0.96\n",
      "episode: 2879   score: 2.0   memory length: 514972   epsilon: 0.07935346000851304    steps: 198     evaluation reward: 0.97\n",
      "episode: 2880   score: 1.0   memory length: 515122   epsilon: 0.07905646000851324    steps: 150     evaluation reward: 0.96\n",
      "episode: 2881   score: 0.0   memory length: 515244   epsilon: 0.0788149000085134    steps: 122     evaluation reward: 0.95\n",
      "episode: 2882   score: 2.0   memory length: 515427   epsilon: 0.07845256000851365    steps: 183     evaluation reward: 0.97\n",
      "episode: 2883   score: 0.0   memory length: 515549   epsilon: 0.07821100000851382    steps: 122     evaluation reward: 0.95\n",
      "episode: 2884   score: 0.0   memory length: 515671   epsilon: 0.07796944000851398    steps: 122     evaluation reward: 0.95\n",
      "episode: 2885   score: 0.0   memory length: 515793   epsilon: 0.07772788000851415    steps: 122     evaluation reward: 0.95\n",
      "episode: 2886   score: 0.0   memory length: 515915   epsilon: 0.07748632000851431    steps: 122     evaluation reward: 0.95\n",
      "episode: 2887   score: 1.0   memory length: 516065   epsilon: 0.07718932000851451    steps: 150     evaluation reward: 0.95\n",
      "episode: 2888   score: 0.0   memory length: 516187   epsilon: 0.07694776000851468    steps: 122     evaluation reward: 0.93\n",
      "episode: 2889   score: 0.0   memory length: 516309   epsilon: 0.07670620000851484    steps: 122     evaluation reward: 0.93\n",
      "episode: 2890   score: 2.0   memory length: 516488   epsilon: 0.07635178000851509    steps: 179     evaluation reward: 0.95\n",
      "episode: 2891   score: 1.0   memory length: 516638   epsilon: 0.07605478000851529    steps: 150     evaluation reward: 0.96\n",
      "episode: 2892   score: 0.0   memory length: 516760   epsilon: 0.07581322000851545    steps: 122     evaluation reward: 0.96\n",
      "episode: 2893   score: 2.0   memory length: 516957   epsilon: 0.07542316000851572    steps: 197     evaluation reward: 0.95\n",
      "episode: 2894   score: 0.0   memory length: 517079   epsilon: 0.07518160000851588    steps: 122     evaluation reward: 0.95\n",
      "episode: 2895   score: 0.0   memory length: 517201   epsilon: 0.07494004000851605    steps: 122     evaluation reward: 0.94\n",
      "episode: 2896   score: 0.0   memory length: 517324   epsilon: 0.07469650000851621    steps: 123     evaluation reward: 0.92\n",
      "episode: 2897   score: 4.0   memory length: 517598   epsilon: 0.07415398000851658    steps: 274     evaluation reward: 0.96\n",
      "episode: 2898   score: 3.0   memory length: 517808   epsilon: 0.07373818000851687    steps: 210     evaluation reward: 0.97\n",
      "episode: 2899   score: 0.0   memory length: 517930   epsilon: 0.07349662000851703    steps: 122     evaluation reward: 0.94\n",
      "episode: 2900   score: 1.0   memory length: 518081   epsilon: 0.07319764000851724    steps: 151     evaluation reward: 0.94\n",
      "episode: 2901   score: 0.0   memory length: 518203   epsilon: 0.0729560800085174    steps: 122     evaluation reward: 0.93\n",
      "episode: 2902   score: 1.0   memory length: 518353   epsilon: 0.0726590800085176    steps: 150     evaluation reward: 0.91\n",
      "episode: 2903   score: 0.0   memory length: 518475   epsilon: 0.07241752000851777    steps: 122     evaluation reward: 0.88\n",
      "episode: 2904   score: 0.0   memory length: 518597   epsilon: 0.07217596000851793    steps: 122     evaluation reward: 0.88\n",
      "episode: 2905   score: 0.0   memory length: 518719   epsilon: 0.0719344000085181    steps: 122     evaluation reward: 0.87\n",
      "episode: 2906   score: 0.0   memory length: 518841   epsilon: 0.07169284000851826    steps: 122     evaluation reward: 0.86\n",
      "episode: 2907   score: 0.0   memory length: 518963   epsilon: 0.07145128000851843    steps: 122     evaluation reward: 0.86\n",
      "episode: 2908   score: 0.0   memory length: 519085   epsilon: 0.07120972000851859    steps: 122     evaluation reward: 0.85\n",
      "episode: 2909   score: 0.0   memory length: 519207   epsilon: 0.07096816000851876    steps: 122     evaluation reward: 0.83\n",
      "episode: 2910   score: 0.0   memory length: 519329   epsilon: 0.07072660000851892    steps: 122     evaluation reward: 0.83\n",
      "episode: 2911   score: 2.0   memory length: 519508   epsilon: 0.07037218000851916    steps: 179     evaluation reward: 0.83\n",
      "episode: 2912   score: 1.0   memory length: 519676   epsilon: 0.07003954000851939    steps: 168     evaluation reward: 0.83\n",
      "episode: 2913   score: 2.0   memory length: 519875   epsilon: 0.06964552000851966    steps: 199     evaluation reward: 0.83\n",
      "episode: 2914   score: 2.0   memory length: 520072   epsilon: 0.06925546000851993    steps: 197     evaluation reward: 0.85\n",
      "episode: 2915   score: 0.0   memory length: 520194   epsilon: 0.06901390000852009    steps: 122     evaluation reward: 0.82\n",
      "episode: 2916   score: 0.0   memory length: 520316   epsilon: 0.06877234000852026    steps: 122     evaluation reward: 0.82\n",
      "episode: 2917   score: 0.0   memory length: 520438   epsilon: 0.06853078000852042    steps: 122     evaluation reward: 0.82\n",
      "episode: 2918   score: 3.0   memory length: 520648   epsilon: 0.0681149800085207    steps: 210     evaluation reward: 0.84\n",
      "episode: 2919   score: 2.0   memory length: 520845   epsilon: 0.06772492000852097    steps: 197     evaluation reward: 0.86\n",
      "episode: 2920   score: 0.0   memory length: 520967   epsilon: 0.06748336000852113    steps: 122     evaluation reward: 0.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2921   score: 0.0   memory length: 521089   epsilon: 0.0672418000085213    steps: 122     evaluation reward: 0.82\n",
      "episode: 2922   score: 1.0   memory length: 521239   epsilon: 0.0669448000085215    steps: 150     evaluation reward: 0.8\n",
      "episode: 2923   score: 0.0   memory length: 521361   epsilon: 0.06670324000852167    steps: 122     evaluation reward: 0.8\n",
      "episode: 2924   score: 0.0   memory length: 521483   epsilon: 0.06646168000852183    steps: 122     evaluation reward: 0.8\n",
      "episode: 2925   score: 0.0   memory length: 521606   epsilon: 0.066218140008522    steps: 123     evaluation reward: 0.78\n",
      "episode: 2926   score: 0.0   memory length: 521728   epsilon: 0.06597658000852216    steps: 122     evaluation reward: 0.78\n",
      "episode: 2927   score: 0.0   memory length: 521851   epsilon: 0.06573304000852233    steps: 123     evaluation reward: 0.78\n",
      "episode: 2928   score: 2.0   memory length: 522034   epsilon: 0.06537070000852258    steps: 183     evaluation reward: 0.8\n",
      "episode: 2929   score: 0.0   memory length: 522156   epsilon: 0.06512914000852274    steps: 122     evaluation reward: 0.79\n",
      "episode: 2930   score: 0.0   memory length: 522278   epsilon: 0.0648875800085229    steps: 122     evaluation reward: 0.77\n",
      "episode: 2931   score: 2.0   memory length: 522475   epsilon: 0.06449752000852317    steps: 197     evaluation reward: 0.79\n",
      "episode: 2932   score: 2.0   memory length: 522672   epsilon: 0.06410746000852344    steps: 197     evaluation reward: 0.8\n",
      "episode: 2933   score: 0.0   memory length: 522794   epsilon: 0.0638659000085236    steps: 122     evaluation reward: 0.79\n",
      "episode: 2934   score: 1.0   memory length: 522944   epsilon: 0.0635689000085238    steps: 150     evaluation reward: 0.79\n",
      "episode: 2935   score: 4.0   memory length: 523197   epsilon: 0.06306796000852415    steps: 253     evaluation reward: 0.83\n",
      "episode: 2936   score: 3.0   memory length: 523410   epsilon: 0.06264622000852443    steps: 213     evaluation reward: 0.86\n",
      "episode: 2937   score: 0.0   memory length: 523532   epsilon: 0.0624046600085246    steps: 122     evaluation reward: 0.83\n",
      "episode: 2938   score: 2.0   memory length: 523711   epsilon: 0.06205024000852484    steps: 179     evaluation reward: 0.83\n",
      "episode: 2939   score: 0.0   memory length: 523833   epsilon: 0.061808680008525005    steps: 122     evaluation reward: 0.8\n",
      "episode: 2940   score: 1.0   memory length: 523983   epsilon: 0.06151168000852521    steps: 150     evaluation reward: 0.8\n",
      "episode: 2941   score: 0.0   memory length: 524105   epsilon: 0.06127012000852537    steps: 122     evaluation reward: 0.8\n",
      "episode: 2942   score: 0.0   memory length: 524227   epsilon: 0.06102856000852554    steps: 122     evaluation reward: 0.8\n",
      "episode: 2943   score: 1.0   memory length: 524378   epsilon: 0.06072958000852574    steps: 151     evaluation reward: 0.79\n",
      "episode: 2944   score: 0.0   memory length: 524500   epsilon: 0.060488020008525906    steps: 122     evaluation reward: 0.79\n",
      "episode: 2945   score: 0.0   memory length: 524622   epsilon: 0.06024646000852607    steps: 122     evaluation reward: 0.79\n",
      "episode: 2946   score: 0.0   memory length: 524744   epsilon: 0.060004900008526235    steps: 122     evaluation reward: 0.78\n",
      "episode: 2947   score: 2.0   memory length: 524941   epsilon: 0.0596148400085265    steps: 197     evaluation reward: 0.78\n",
      "episode: 2948   score: 0.0   memory length: 525063   epsilon: 0.059373280008526666    steps: 122     evaluation reward: 0.78\n",
      "episode: 2949   score: 2.0   memory length: 525260   epsilon: 0.05898322000852693    steps: 197     evaluation reward: 0.79\n",
      "episode: 2950   score: 2.0   memory length: 525457   epsilon: 0.0585931600085272    steps: 197     evaluation reward: 0.8\n",
      "episode: 2951   score: 0.0   memory length: 525579   epsilon: 0.05835160000852736    steps: 122     evaluation reward: 0.8\n",
      "episode: 2952   score: 1.0   memory length: 525731   epsilon: 0.05805064000852757    steps: 152     evaluation reward: 0.8\n",
      "episode: 2953   score: 0.0   memory length: 525853   epsilon: 0.05780908000852773    steps: 122     evaluation reward: 0.77\n",
      "episode: 2954   score: 0.0   memory length: 525975   epsilon: 0.0575675200085279    steps: 122     evaluation reward: 0.76\n",
      "episode: 2955   score: 1.0   memory length: 526126   epsilon: 0.0572685400085281    steps: 151     evaluation reward: 0.73\n",
      "episode: 2956   score: 0.0   memory length: 526248   epsilon: 0.057026980008528266    steps: 122     evaluation reward: 0.73\n",
      "episode: 2957   score: 0.0   memory length: 526370   epsilon: 0.05678542000852843    steps: 122     evaluation reward: 0.73\n",
      "episode: 2958   score: 0.0   memory length: 526492   epsilon: 0.056543860008528596    steps: 122     evaluation reward: 0.73\n",
      "episode: 2959   score: 1.0   memory length: 526643   epsilon: 0.0562448800085288    steps: 151     evaluation reward: 0.72\n",
      "episode: 2960   score: 0.0   memory length: 526765   epsilon: 0.056003320008528965    steps: 122     evaluation reward: 0.72\n",
      "episode: 2961   score: 0.0   memory length: 526888   epsilon: 0.05575978000852913    steps: 123     evaluation reward: 0.72\n",
      "episode: 2962   score: 0.0   memory length: 527010   epsilon: 0.055518220008529295    steps: 122     evaluation reward: 0.71\n",
      "episode: 2963   score: 0.0   memory length: 527132   epsilon: 0.05527666000852946    steps: 122     evaluation reward: 0.71\n",
      "episode: 2964   score: 3.0   memory length: 527360   epsilon: 0.05482522000852977    steps: 228     evaluation reward: 0.74\n",
      "episode: 2965   score: 3.0   memory length: 527570   epsilon: 0.05440942000853005    steps: 210     evaluation reward: 0.77\n",
      "episode: 2966   score: 0.0   memory length: 527692   epsilon: 0.054167860008530216    steps: 122     evaluation reward: 0.77\n",
      "episode: 2967   score: 0.0   memory length: 527814   epsilon: 0.05392630000853038    steps: 122     evaluation reward: 0.76\n",
      "episode: 2968   score: 0.0   memory length: 527936   epsilon: 0.053684740008530546    steps: 122     evaluation reward: 0.75\n",
      "episode: 2969   score: 2.0   memory length: 528153   epsilon: 0.05325508000853084    steps: 217     evaluation reward: 0.77\n",
      "episode: 2970   score: 0.0   memory length: 528275   epsilon: 0.053013520008531004    steps: 122     evaluation reward: 0.77\n",
      "episode: 2971   score: 0.0   memory length: 528397   epsilon: 0.05277196000853117    steps: 122     evaluation reward: 0.74\n",
      "episode: 2972   score: 3.0   memory length: 528608   epsilon: 0.052354180008531453    steps: 211     evaluation reward: 0.75\n",
      "episode: 2973   score: 0.0   memory length: 528730   epsilon: 0.05211262000853162    steps: 122     evaluation reward: 0.74\n",
      "episode: 2974   score: 0.0   memory length: 528852   epsilon: 0.05187106000853178    steps: 122     evaluation reward: 0.73\n",
      "episode: 2975   score: 0.0   memory length: 528974   epsilon: 0.05162950000853195    steps: 122     evaluation reward: 0.73\n",
      "episode: 2976   score: 0.0   memory length: 529096   epsilon: 0.05138794000853211    steps: 122     evaluation reward: 0.71\n",
      "episode: 2977   score: 0.0   memory length: 529218   epsilon: 0.05114638000853228    steps: 122     evaluation reward: 0.71\n",
      "episode: 2978   score: 1.0   memory length: 529368   epsilon: 0.05084938000853248    steps: 150     evaluation reward: 0.72\n",
      "episode: 2979   score: 0.0   memory length: 529490   epsilon: 0.050607820008532645    steps: 122     evaluation reward: 0.7\n",
      "episode: 2980   score: 2.0   memory length: 529687   epsilon: 0.05021776000853291    steps: 197     evaluation reward: 0.71\n",
      "episode: 2981   score: 1.0   memory length: 529837   epsilon: 0.04992076000853311    steps: 150     evaluation reward: 0.72\n",
      "episode: 2982   score: 0.0   memory length: 529959   epsilon: 0.04967920000853328    steps: 122     evaluation reward: 0.7\n",
      "episode: 2983   score: 0.0   memory length: 530081   epsilon: 0.04943764000853344    steps: 122     evaluation reward: 0.7\n",
      "episode: 2984   score: 1.0   memory length: 530231   epsilon: 0.049140640008533645    steps: 150     evaluation reward: 0.71\n",
      "episode: 2985   score: 1.0   memory length: 530381   epsilon: 0.04884364000853385    steps: 150     evaluation reward: 0.72\n",
      "episode: 2986   score: 1.0   memory length: 530531   epsilon: 0.04854664000853405    steps: 150     evaluation reward: 0.73\n",
      "episode: 2987   score: 0.0   memory length: 530653   epsilon: 0.048305080008534215    steps: 122     evaluation reward: 0.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2988   score: 0.0   memory length: 530775   epsilon: 0.04806352000853438    steps: 122     evaluation reward: 0.72\n",
      "episode: 2989   score: 0.0   memory length: 530897   epsilon: 0.047821960008534545    steps: 122     evaluation reward: 0.72\n",
      "episode: 2990   score: 1.0   memory length: 531066   epsilon: 0.04748734000853477    steps: 169     evaluation reward: 0.71\n",
      "episode: 2991   score: 0.0   memory length: 531188   epsilon: 0.04724578000853494    steps: 122     evaluation reward: 0.7\n",
      "episode: 2992   score: 0.0   memory length: 531311   epsilon: 0.047002240008535104    steps: 123     evaluation reward: 0.7\n",
      "episode: 2993   score: 0.0   memory length: 531433   epsilon: 0.04676068000853527    steps: 122     evaluation reward: 0.68\n",
      "episode: 2994   score: 0.0   memory length: 531555   epsilon: 0.04651912000853543    steps: 122     evaluation reward: 0.68\n",
      "episode: 2995   score: 1.0   memory length: 531705   epsilon: 0.046222120008535636    steps: 150     evaluation reward: 0.69\n",
      "episode: 2996   score: 2.0   memory length: 531922   epsilon: 0.04579246000853593    steps: 217     evaluation reward: 0.71\n",
      "episode: 2997   score: 1.0   memory length: 532072   epsilon: 0.04549546000853613    steps: 150     evaluation reward: 0.68\n",
      "episode: 2998   score: 0.0   memory length: 532194   epsilon: 0.045253900008536296    steps: 122     evaluation reward: 0.65\n",
      "episode: 2999   score: 1.0   memory length: 532344   epsilon: 0.0449569000085365    steps: 150     evaluation reward: 0.66\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "        if render_breakout:\n",
    "            env.render()\n",
    "\n",
    "        # Select and perform an action\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action + 1)\n",
    "\n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['ale.lives'])\n",
    "\n",
    "        life = info['ale.lives']\n",
    "        r = np.clip(reward, -1, 1)\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network\n",
    "            if(frame % Update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "\n",
    "        if frame % 50000 == 0:\n",
    "            print('now time : ', datetime.now())\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\")\n",
    "\n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 400\n",
    "            # stop training\n",
    "            if np.mean(evaluation_reward) > 400:\n",
    "                torch.save(agent.model, \"./save_model/breakout_dqn\")\n",
    "                sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
